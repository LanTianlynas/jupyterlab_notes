{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32436474-cb00-490b-8e7f-ef956d5b4c4d",
   "metadata": {},
   "source": [
    "https://newspaper.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e631bd4e-8827-469c-bd22-94dec541fa99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取新闻页面的url！！！\n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-09-29-9\n",
      "title= 机器之心A100数智中国榜发布：让中小企业看懂、选对、用好数智化服务方案（附研究报告）\n",
      "author= []\n",
      "publish_date= 2020-09-29 00:00:00\n",
      "text= 行业方案榜：TOP数智+交通运输\n",
      "\n",
      "方案名称 服务商名称 入选理由\n",
      "\n",
      "ET航空大脑 阿里巴巴 集团 阿里云 是 阿里巴巴 旗下全球领先的 云计算 及 人工智能 公司，在财务资本方面具有雄厚的实力，自从成立以来一直强调多远化发展，其云服务已基本涵盖各行各业。它推出的ET航空大脑，涵盖大数据、 运筹优化 、路基 规划 等多种创新性技术，并聚焦于停机位分配、班组排版、餐补 规划 以及客流态式监控等多个航空运输聚焦领域。它已与数十个不同数智化技术中小微企业构建合作关系，并引入其相关优势技术。目前，该ET航空大脑已在北京首都国际机场落地，让1700个停机位安排可在50秒内完成 规划 ，总计节省约5000小时服务时长。\n",
      "\n",
      "百度 地图智慧交通解决方案 北京 百度 网讯科技有限公司 百度 是全球领先的互联网、 云计算 与 人工智能 上市公司，其在财务资本方面具有雄厚实力，自从成立以来已参与或建立多个国家级与省市级行业标准。它推出的 百度 地图智慧交通涵盖 人工智能 、大数据、GIS等多项创新性技术，并聚焦于交通服务行业的车流监控、路况分析、假日出行等多个常见应用场景。目前，该平台已与广播平台、研究机构等多行业数百家企业展开合作，平台开发者数量已接近180万，服务应用50万个，并有近1.5亿C端用户下载量。\n",
      "\n",
      "砖头物流管理系统 北京乐卡车联科技有限公司 北京乐卡车联科技有限公司是以车联万物为理念的智能车载系统硬件供应商，在财务资本方面实力有限，已完成A+轮融资，自从成立以来在从安全、效率、娱乐构建了完成的智能车载体系。它推出乐卡车联解决方案涵盖 人工智能 、大数据等多项创新性技术，支持经营管理等多个数智化场景应用，并专注于服务中小微交通运输业热门领域。它已服务千余万份订单，并与顺丰、德邦等知名品牌取得合作，且具有较为成熟的方案售后服务体系。\n",
      "\n",
      "大华智慧交通解决方案 大华股份集团 大华技术股份有限公司是全球领先的智慧物联网解决方案供应商和运营服务商。它推出的智慧交通解决方案涵盖 人工智能 、大数据、 计算机视觉 等创新技术，可覆盖高速公路、交通运输、铁路、城市轨道、港口等多个常见交通行业领域。该解决方案已覆盖全球近180个国家，并设立超过53个分支机构以进行售后服务支持。\n",
      "\n",
      "滴滴智慧交通解决方案 滴滴出行 滴滴出行是全球卓越的移动出行平台服务商，在全球范围内以向超过5.5亿C端用户提供过出行、外卖以及支付相关多元化服务。它推出的滴滴智慧交通解决方案融合了互联网、大数据、物联网、 运筹优化 等多项创新性技术，可覆盖拥堵分析、驾驶检测、潮汐车道 规划 、公交 调度 等多项交通热门领域应用场景。该解决方案已有近4875TB相关数据，可为政府、企业、个人提供高质量的智慧交通出行服务。\n",
      "\n",
      "智慧交通解决方案 华为 技术有限公司 华为 在财务资本方面具有雄厚实力，并是全球领先的通信服务寡头公司，自从成立以来获得多次国家级科技进步奖以及省市级科技奖励，并自主建立多项业内技术标准。它推出的智慧交通解决方案涵盖 云计算 、大数据、物联网、敏捷网络、BYOD、 5G 等多项创新技术，可围绕铁路运营、港口管理、机场运营等多场景提供一站式的解决方案。 华为 智慧交通解决方案所构建的深圳城市大脑项目，获得世界智慧城市博览会平安城市专项大奖。 华为 还计划推进该方案尝试与更多道路、汽车相关中小微企业取得合作，完善智慧交通版图。\n",
      "\n",
      "京东 物流 京东 物流集团 大华技术股份有限公司是全球领先的智慧物联网解决方案供应商和运营服务商。它推出的智慧交通解决方案涵盖 人工智能 、大数据、 计算机视觉 等创新技术，可覆盖高速公路、交通运输、铁路、城市轨道、港口等多个常见交通行业领域。该解决方案已覆盖全球近180个国家，并设立超过53个分支机构以进行售后服务支持。\n",
      "\n",
      "龙邦快运 龙邦物流有限公司 京东 物流隶属于 京东 集团，旨在通过开放、智能化的战略促进消费方式的转变与社会供应链效率的提升，将物流、商流、资金流和信息流有机结合，打造体验最优的物流履约解决方案。它推出的 京东 物流一体化解决方案涵盖大数据、 机器学习 、 运筹优化 、无人机、机器人等多种创新性技术，可实现库存共享、无人派送、订单集成处理、仓配一体以及极速达等多种交通运输领域常见服务。该及决方案已覆盖近200+城市，并针对中小微企业覆盖较广的服饰、消费品、母婴等热门行业打造了从仓储到配送、从线上到线下，从硬件到软件，从原材料采购至分销供应链的个性化解决方案。\n",
      "\n",
      "千方科技智慧交通解决方案 千方科技 千方科技是智慧交通领域客户解决方案提供商，聚焦于推动智慧交通、智能物联行业发展。它推出的千方科技智慧交通解决方案以大数据、 人工智能 、 云计算 为基础，构建了智慧交通与智慧物联双引擎，可服务运输、ETC、交通大数据、民航、轨交、高速等多个交通领域热门场景。该解决方案已形成从硬件基础设施到软件智慧中枢的完成产业链。目前，该解决方案已在2018年为公司带来近34.93亿元收入，同比增长达到近20%。\n",
      "\n",
      "货运中国网互联网+物流SAAS云平台 上海成达智慧物流有限公司 成达信息科技是互联网物流运营的先行者，在财务资本方面有一定实力，已完成Pre-A轮融资，自从成立以来推出多种针对传统物流进行优化的数智化产品。它推出的货运中国网络平台涵盖大数据、 人工智能 、 机器学习 等多项创新性技术，支持销售营销等多个数智化场景应用，并已在国内建立20个城市据点，服务20万辆汽车车主，且具有较为成熟的方案售后服务体系。\n",
      "\n",
      "华宇开放平台 上海华振物流有限公司 盛丰物流集团有限公司是一家专注于国内干线运输、货物仓储、物流配送、物流解决方案策划与设计的国家5A级综合物流企业，自从成立以来公司以福州为总部建立了280家分公司与仓库，自有货车近8000辆，全国公路零担快运第五名。它推出的盛丰物流涵盖大数据、物联网、 运筹优化 等多种创新性技术，服务 调度 规划 、仓储包装、加工搬运等多个常见交通运输领域，并为其供应链产业提供信息化数据共享增值服务。该物流已与近2000家制造业中小微企业取得物流合作，认证伙伴超过10000家。\n",
      "\n",
      "运去哪一站式国际物流在线服务平台 上海汇航捷讯网络科技有限公司 上海汇航捷讯网络科技有限公司是以物流及外贸行业为具体应用对象的新型B2B互联网企业，在财务资本方面实力有限，自从成立以来多次获得无人货运相关奖项，并构建了多种智能运输货运产品。它推出的运去哪涵盖 人工智能 、自然语言处理等多项创新性技术，支持销售营销等多个数智化场景应用，并专注于服务交通运输热门领域。它已服务1万家+中小微客户遍布全球15万+航线，且具有成熟的方案售后服务体系。\n",
      "\n",
      "佳吉快运 上海佳吉快运有限公司 滴滴出行是全球卓越的移动出行平台服务商，在全球范围内以向超过5.5亿C端用户提供过出行、外卖以及支付相关多元化服务。它推出的滴滴智慧交通解决方案融合了互联网、大数据、物联网、 运筹优化 等多项创新性技术，可覆盖拥堵分析、驾驶检测、潮汐车道 规划 、公交 调度 等多项交通热门领域应用场景。该解决方案已有近4875TB相关数据，可为政府、企业、个人提供高质量的智慧交通出行服务。\n",
      "\n",
      "智能交通解决方案 深圳市 腾讯 计算机系统有限公司 腾讯 是全球领先的互联网、 云计算 与 人工智能 上市公司，其在财务资本方面具有雄厚实力，自从成立以来已参与或建立数百个国家级与省市级行业标准。它推出的 腾讯 智能交通解决方案涵盖 人工智能 、大数据、 计算机视觉 、自然语言处理、 机器学习 、知识表征等多项创新性技术，并聚焦于交通路况、公共出行、事故处理、违章缴罚等多个交通行业服务领域。目前，该解决方案已与上万销售代理商、软件开发商以及解决方案商建立合作关系，并开始契合 腾讯 We Transport大战略帮助交通行业中小微企业进行数智化转型。\n",
      "\n",
      "盛丰物流 盛丰物流集团有限公司 上海佳吉快运有限公司是国家认证的5A级物流企业，公司主营公路零担运输业务，并使始终重视数智化企业服务能力的提升与货运信息化网络的建设。它推出的佳吉快运服务涵盖GPS、物联网、大数据、 运筹优化 等多项创新性技术，可囊括货车定位、配送监控以及智能 调度 等多项交通运输领域常见应用场景。目前该服务已在全国范围内拥有近2500多个服务网点，17个大型转运中心、信息化网络服务能力达到所有省级行政区。\n",
      "\n",
      "顺丰速运 顺丰速运 龙邦物流有限公司成立于2012年，是“龙邦供应链”旗下一家以物流供应链管理为核心，布局全国物流网络运营、互联网技术研发的创新性企业。它推出的龙邦快运利用大数据、物理网、 运筹优化 等技术，对零担物流、专线资源实现网络化监控、信息化运营，为中小微企业提供全国快运门对门一站式综合物流及决方案。目前该方案已在全球范围内拥有分拨中心81个，服务网点3000余家以及配送车辆8000余台。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2023-10-18-5\n",
      "title= 站上大模型制高点：我们给不输GPT-4的文心大模型4.0，来了一场技术揭秘\n",
      "author= []\n",
      "publish_date= 2023-10-18 00:00:00\n",
      "text= 我们即将进入一个 AI 原生的时代，一个人机交互的新时代。\n",
      "\n",
      "「这是迄今为止最强的文心大模型。它实现了基础模型的全面升级，在理解、生成、逻辑和记忆四大能力上都比文心一言线上版本有了明显提升，综合水平与 GPT-4 相比毫不逊色，」百度创始人、董事长兼 CEO 李彦宏说道。\n",
      "\n",
      "国内的大模型已经冲到了业界最强水平？10 月 17 日，百度世界大会上文心大模型 4.0 的发布引来了一大波关注。\n",
      "\n",
      "在昨天的大会现场，百度展示了一段又一段 demo，文心大模型现在能看懂你的言外之意，比如问它一个问题：「我想回承德买房，能用公积金贷款吗？手续怎么办，我在北京工作。」\n",
      "\n",
      "一段完全口语化的表达，前后乱序，没有明确表述，但 AI 也能理解说话人的潜台词：公积金是北京的，户口可能在承德。文心一言理解上下文之后准确针对问题给出了回答。\n",
      "\n",
      "当然还有先进的多模态方向：给文心大模型一个新车型的图片，再加上几句话的提示（Prompt），它就可以很快生成完整的企划文案图片，并把素材结合成海报。再提示几下，文心就能结合官网信息和已有内容生成一段宣传视频，其中还有数字人在进行讲解。\n",
      "\n",
      "从零开始到输出海报和视频，整个过程不到三分钟。\n",
      "\n",
      "「人们常说不写作业母慈子孝，一写作业鸡飞狗跳。文心一言能不能帮助家长解决辅导功课的问题？」李彦宏说道。\n",
      "\n",
      "给出一道数学题，文心的解答过程非常详细。AI 还能够进一步解释这个问题中涉及到的各个知识点。\n",
      "\n",
      "衡量 AI 智能程度的另一个指标在于长期记忆能力，如果大模型生成的内容前后逻辑不一致，那就不存在可用性了。在现场，李彦宏让文心一言生成一部武侠小说的大纲，再在其中细化情节，加入几个人物，改变冲突的戏剧要素…… 问题来了，经历了多轮对话，它还记得自己最开始给女主角设定的特殊能力吗？\n",
      "\n",
      "完全没有问题。\n",
      "\n",
      "面向全社会开放才一个多月，文心大模型的综合水平看来又有了大幅进化，不过在主题演讲上的那些毕竟是「命题作文」，真正用起来会不会是一回事呢？\n",
      "\n",
      "其实想要用上并不难：昨天大会的一开场，百度就宣布文心大模型 4.0 开启邀请测试，现场观众都有了测试权限，在网站和 APP 上都可以体验。我们则是提前获得了评测资格，尝试了一下新版本。\n",
      "\n",
      "文心大模型 4.0 正面对比 GPT-4\n",
      "\n",
      "在文心一言的网站上，现在已经出现了文心大模型 4.0 的标签，表面看起来和 3.5 版没有太大区别：\n",
      "\n",
      "在这里我们要引入一点前置知识：上个版本文心大模型 3.5 已经有了插件（现有 8 种）、多模态理解、生成等能力，通过知识点增强技术实现了对世界知识的熟练掌握。因此，既然说 4.0 版是「迄今为止最强大模型」，我们就不能再用以前过于简单的问题来考验它了。\n",
      "\n",
      "先看理解能力，这道「中文十级题」目测连网友都会翻车，文心大模型 4.0 的回答简洁明了：\n",
      "\n",
      "换业界标杆 GPT-4 来回答，它理解并解释了其中幽默的意味，但表示无法确定小明最后买的是几等座：\n",
      "\n",
      "下面这段话是在一档直播节目上出现的，那时人们评价道：全中国没人听得懂白岩松在说什么。\n",
      "\n",
      "两个大模型都认为说话人想表达的是：人们都喜爱足球这项运动，不应该因为一小部分人的不喜欢而影响到这种喜爱。不过作为人类，还是得说一句 AI 没有理解「想说声喜爱很难」这种感情。\n",
      "\n",
      "看起来理解问题的水准上，两种模型水平相近，文心大模型在一小部分问题上有点优势。\n",
      "\n",
      "再看逻辑推理能力，输入一个高考试卷中的物理选择题，文心大模型 4.0 和 GPT-4 都给出了正确的回答：\n",
      "\n",
      "看起来文心能给出的答案更详细一些，另外还显示了几个进一步解释概念的引导选项，似乎它对做题进行了专门的优化？\n",
      "\n",
      "我们继续问了很多高考的数学题目，结果各有对错，也有些是都答不上来的。总体来看文心 4.0 和 GPT-4 的水平相近。\n",
      "\n",
      "还有多模态生成，我们直接用同样的指令让两个大模型生成一段视频，文心一言调用「一镜留影」插件，直接输出了结果：\n",
      "\n",
      "GPT-4 则是调用 CapCut（字节的剪映）插件生成视频内容。需要注意的是，它提示要想生成视频，就必须要与你进行多轮对话，逐步确定好视频脚本（英文的）、屏幕比例等等：\n",
      "\n",
      "在不断的测试中我们还能看出，如果你 Prompt 得越仔细，说 AI 话的格式越规整，GPT-4 的表现就相对越好，不过最终也并没有产生决定性的差距。调戏大模型，现在已经越来越像一门学问了。\n",
      "\n",
      "为了测试四大能力中的长期记忆能力，我们让文心大模型 4.0 阅读一篇贴吧的帖子：在崩铁更新了 1.4 版本之后，有人从自己专业的角度对剧情进行了一长段吐槽，那么这评价合理吗？\n",
      "\n",
      "文心认为游戏剧情不需要完全按照现实世界的逻辑来展开。我不是很认同，我就是想要符合现实逻辑的剧情：\n",
      "\n",
      "能不能再跌宕起伏一点？\n",
      "\n",
      "再尝试替换其中的一个人物：\n",
      "\n",
      "看起来，文心大模型 4.0 可以在保持原始知识的情况下，与人在不断对话的过程中生成、提炼出你想要的内容。\n",
      "\n",
      "还有一些我们经常会用得到的功能。在 ChatGPT 出现后，越来越多的人开始尝试使用大模型帮忙来润色论文，据说 AI 写论文看起来很有功底，一般人还真比不上。我们用一段著名的发言试一下：\n",
      "\n",
      "文心大模型 4.0 把它改写成了这样：\n",
      "\n",
      "与之相对的是，GPT-4 更多地使用了原文的信息：\n",
      "\n",
      "不过在更多测试中，GPT-4 生成的内容偶尔会出现夹杂英文的现象。\n",
      "\n",
      "另外，文心一言目前为保证获取实时信息，默认接入了百度搜索插件，也在理解网络新趋势的时候能帮得上忙。比如，我们最近都在反思自己有没有努力工作：\n",
      "\n",
      "相比之下，GPT-4 给出了似乎是基于大模型幻觉的回答。\n",
      "\n",
      "如果多点一步选择使用 Bing 联网版的 GPT-4 则可以得到正确回复，不过再次出现了语言问题，偶尔会获得全英文的回答。\n",
      "\n",
      "看起来，文心大模型 4.0 在四大核心能力上的提升的确明显，和 GPT-4 比毫不逊色的说法也并不是夸张，特别是在中文领域里，水平是经得起考验的。\n",
      "\n",
      "核心技术揭秘\n",
      "\n",
      "能做得到业内领先，百度实现了哪些技术进步？在昨天会上，百度 CTO 王海峰解读了文心大模型 4.0 的关键技术和最新进展。\n",
      "\n",
      "「相比 3.5 版本，文心大模型 4.0 的理解、生成、逻辑、记忆四大能力都有显著提升，」王海峰说道。「其中理解和生成能力的提升幅度相近。而逻辑和记忆能力的提升则更大。逻辑的提升幅度达到理解的近三倍，记忆的提升幅度也达到了理解的两倍多。」这些提升都会给用户带来帮助。\n",
      "\n",
      "这些改进的速度很快 —— 其实文心大模型 4.0 在 9 月初就达到了上线标准，开始了小流量测试。过去的一个多月里经过不断调优，它的生成效果又提升了近 30%。\n",
      "\n",
      "基础模型能力的增长体现在应用上，就转化成了生产效率的提升。比如在各家大厂都说在用的智能代码助手上，百度基于文心大模型的 Comate 在内部应用效果不错，整体的代码采纳率现在是 40%，高频用户的代码采纳率达到 60%。现在百度每天新增的代码中，有 20% 是由大模型生成的，这个比例还在不断升高。\n",
      "\n",
      "这些提升又是靠什么做到的？总的来说，百度基于高效率算力、自研框架、更好的数据处理机制，再结合算法与调优，这才训练出了规模更大、效果更好的文心大模型 4.0。\n",
      "\n",
      "今年 3 月正式发布的文心一言，其背后基于文心大模型 3.0，这是一个有知识增强的大语言模型，它从数万亿数据和数千亿知识中融合学习，又使用了有监督精调、人类反馈强化学习、提示等技术，具备知识增强、检索增强和对话增强的优势。\n",
      "\n",
      "5 月份发布的文心大模型 3.5 则在基础模型、精调技术、知识点增强、逻辑推理、插件机制等方面进行了改进，取得了生成效果和效率的提升。\n",
      "\n",
      "文心大模型 4.0 以它们为基础，继续在多个关键技术向上突破。\n",
      "\n",
      "具体来说，百度：\n",
      "\n",
      "在万卡算力上基于飞桨平台，通过集群基础设施和调度系统、飞桨框架的软硬协同优化，支持了大模型的稳定高效训练。\n",
      "\n",
      "通过建设多维数据体系，形成了从数据挖掘、分析、合成、标注到评估闭环，充分提高数据的利用效率，大幅提升模型效果。\n",
      "\n",
      "基于有监督精调、偏好学习、强化学习等技术进行多阶段对齐，保证了模型能够更好地与人类的判断和选择对齐。\n",
      "\n",
      "利用可再生训练技术通过增量式的参数调优，有效节省了训练资源和时间，加快了模型迭代速度。\n",
      "\n",
      "基于这一系列的提升，自三月以来文心大模型的训练效率已累计提升 3.6 倍；训练稳定性方面，周均的训练有效率已超过 98%。\n",
      "\n",
      "另外在更高层面上还有一些改进。\n",
      "\n",
      "文心大模型 4.0 实现了输入和输出两阶段的知识点增强，一方面对用户输入的问题进行理解，拆解出所需的知识点，然后在搜索引擎、知识图谱、数据库中查找准确知识，再把这些知识组装进 Prompt 送入大模型，提升了准确率和效率。另一方面又对大模型的输出进行「反思」，从生成结果中拆解出知识点，再用搜索引擎、知识图谱、数据库，以及大模型本身进行确认，对有差错的内容进行修正。\n",
      "\n",
      "给大模型再加一层自动化的 AutoGPT 被认为是大模型的重要发展方向，百度同样构建了文心的智能体机制。人的认知系统可划分为两个部分：系统 1，反应很快，但容易出错；系统 2，反应慢，但更理性、更准确。在基础大模型之上百度进一步研制了系统 2，包括理解、规划、反思和进化，能够做到可靠执行，自我进化，并一定程度上将思考过程白盒化，从而让机器像人一样思考和行动，自主的完成复杂任务，并能够在环境中持续学习实现自主进化。\n",
      "\n",
      "接下来，文心一言团队还会继续加班加点，持续提升大模型的能力。\n",
      "\n",
      "目前，文心大模型的用户量增长很快。王海峰公布了一组数字：自 8 月 31 日文心一言面向全社会开放至今，仅用 40 多天的时间，文心一言的用户规模已经达到 4500 万，同时覆盖了 5.4 万开发者，4300 个场景，825 个应用，与之匹配的插件也超过了 500 个。\n",
      "\n",
      "百度：做国内第一个 AI 原生化公司\n",
      "\n",
      "当然，前面展示的文心一言只是生成式 AI 应用的一小部分。\n",
      "\n",
      "大模型理解、生成、逻辑、记忆的四大核心能力突破，是催生 AI 原生应用的必要条件，带来了全新的想象和创新空间。\n",
      "\n",
      "李彦宏表示，百度要做第一个把所有产品进行重构的公司。在世界大会上，百度发布了多款 AI 原生的应用，来自搜索、地图、文库、网盘等业务线的十余个应用产品全部亮相。\n",
      "\n",
      "百度搜索是大模型落地的第一步，「新搜索」是全新的 AI 互动式搜索，它实现了三大重要提升：极致满足、推荐激发、多轮交互。当你在搜索框里输入问题，它不再是单纯的输出链接，而是生成完整的答案，并附带易于理解的图表。\n",
      "\n",
      "大模型加持的生产力工具也在变得更聪明，分析师现在可以通过大模型工具可以把十几天才能完成的任务缩短到几分钟来完成，参与在线会议的人可以从冗长的对话内容里快速总结出重要信息，出差时 AI 也会自动帮你安排行程：\n",
      "\n",
      "在我们每天都会用的百度地图上，最新上线的 V19 版本基于文心大模型进行了重构，其中的「AI 向导」具备多轮自然语言交互能力，用说话的方式就能唤醒菜单里被折叠的上千种能力，也可以理解人们不是具体地点的需求，并找到最优解，当好一个向导。\n",
      "\n",
      "如果把眼光放远到更多行业，百度正在大力推动数字技术与实体经济的深度融合，其大模型技术已应用在制造、能源、电力、化工、交通等实体产业中。在千帆大模型平台上，现在已有超过 1.7 万企业开发了产业模型和解决方案，覆盖了各行业的近 500 个场景。\n",
      "\n",
      "最近一段时间，AI 领域技术的军备竞赛让我们对技术突破越来越熟视无睹。有时候甚至会忘记距离 ChatGPT 正式发布，现在才过去十个多月的时间。在这段时间里，通用的生成式 AI 已经从遥不可及的愿景，变成了人人在玩的聊天机器人，又蜕变成为了众多行业效率提升的基础。\n",
      "\n",
      "而在未来，不论时间的长短，AI 原生的智能化注定要改变所有人的生活和工作方式。\n",
      "\n",
      "可喜的是，在这个过程中，国内公司已经拿到了入场门票。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2023-10-18-4\n",
      "title= 把LLM视作操作系统，它就拥有了无限「虚拟」上下文，伯克利新作已揽1.7k star\n",
      "author= []\n",
      "publish_date= 2023-10-18 00:00:00\n",
      "text= 当前，让大语言模型拥有更强的上下文处理能力是业界非常看重的热点主题之一。本文中，加州大学伯克利分校的研究者将 LLM 与操作系统巧妙地联系在了一起，在扩展上下文长度领域带来了新的进展。\n",
      "\n",
      "近年来，大语言模型（LLM）及其底层的 transformer 架构已经成为了对话式 AI 的基石，并催生了广泛的消费级和企业应用程序。尽管有了长足的进步，但 LLM 使用的固定长度的上下文窗口极大地限制了对长对话或长文档推理的适用性。即使是使用最广泛的开源 LLM，它们的最大输入长度只允许支持几十条消息回复或短文档推理。\n",
      "\n",
      "与此同时，受限于 transformer 架构的自注意力机构，简单地扩展 transformer 的上下文长度也会导致计算时间和内存成本成倍增加，这就使得全新的长上下文架构成为紧迫的研究课题。\n",
      "\n",
      "不过，即使我们能够克服上下文缩放的计算挑战，但最近的研究却表明，长上下文模型很难有效地利用额外的上下文。\n",
      "\n",
      "这如何解决呢？考虑到训练 SOTA LLM 所需的大量资源以及上下文缩放明显的回报递减，我们迫切需要支持长上下文的替代技术。加州大学伯克利分校的研究者在这方面有了新的进展。\n",
      "\n",
      "在本文中，研究者探究了如何在继续使用固定上下文模型的同时，提供无限上下文的幻觉（illusion）。他们的方法借鉴了虚拟内存分页的思路，使得应用程序能够处理远超出可用内存的数据集。\n",
      "\n",
      "基于该思路，研究者利用 LLM 智能体函数调用能力的最新进展，设计出了一个受 OS 启发、用于虚拟上下文管理的 LLM 系统 ——MemGPT。\n",
      "\n",
      "论文主页：https://memgpt.ai/\n",
      "\n",
      "arXiv 地址：https://arxiv.org/pdf/2310.08560.pdf\n",
      "\n",
      "项目已经开源，在 GitHub 上已经斩获了 1.7k 的 star 量。\n",
      "\n",
      "GitHub 地址：https://github.com/cpacker/MemGPT\n",
      "\n",
      "方法概览\n",
      "\n",
      "该研究从传统操作系统的分层内存管理中汲取灵感，在上下文窗口（类似于操作系统中的「主存（main memory）」）和外部存储之间有效地「分页」进出信息。MemGPT 则负责管理内存、LLM 处理模块和用户之间的控制流。这种设计允许在单个任务期间反复进行上下文修改，从而允许智能体更有效地利用其有限的上下文窗口。\n",
      "\n",
      "MemGPT 将上下文窗口视为受限内存资源，并为 LLM 设计类似于传统操作系统中分层内存（Patterson et al., 1988）的层次结构。为了提供更长的上下文长度，该研究允许 LLM 通过「LLM OS」——MemGPT，来管理放置在其上下文窗口中的内容。MemGPT 使 LLM 能够检索上下文中丢失的相关历史数据，类似于操作系统中的页面错误。此外，智能体可以迭代地修改单个任务上下文窗口中的内容，就像进程可以重复访问虚拟内存一样。\n",
      "\n",
      "MemGPT 能够让 LLM 在上下文窗口有限的情况下处理无界上下文，MemGPT 的组件如下图 1 所示。\n",
      "\n",
      "MemGPT 通过函数调用协调主上下文（上下文窗口中的内容）和外部上下文之间的数据移动，MemGPT 根据当前上下文自主更新和检索。\n",
      "\n",
      "值得注意的是，上下文窗口需要用 warning token 来标识其限制，如下图 3 所示：\n",
      "\n",
      "实验及结果\n",
      "\n",
      "在实验部分，研究者在两个长上下文域中来评估 MemGPT，分别是对话式智能体和文档处理。其中对于对话式智能体，他们扩展了现有的多会话聊天数据集（Xu et al. (2021)），并引入了两个新的对话任务以评估智能体在长对话中保留知识的能力。对于文档分析，他们根据 Liu et al. (2023a) 提出的任务对 MemGPT 进行基准测试，包括对长文档的问答和键值检索。\n",
      "\n",
      "用于对话智能体的 MemGPT\n",
      "\n",
      "当与用户对话时，智能体必须满足以下两个关键标准。\n",
      "\n",
      "一是一致性，即智能体应保持对话的连贯性，提供的新事实、引用和事件应与用户、智能体之前的陈述保持一致。\n",
      "\n",
      "二是参与度，即智能体应该利用用户的长期知识来个性化响应。参考之前的对话可以使对话更加自然和引人入胜。\n",
      "\n",
      "因此，研究者根据这两个标准对 MemGPT 进行评估：\n",
      "\n",
      "MemGPT 是否可以利用其记忆来提高对话一致性？能否记住过去交互中的相关事实、引用、事件以保持连贯性？\n",
      "\n",
      "MemGPT 是否可以利用记忆生成更有吸引力的对话？是否自发地合并远程用户信息以个性化信息？\n",
      "\n",
      "关于使用到的数据集，研究者在 Xu et al. (2021) 提出的多会话聊天（MSC）上对 MemGPT 和固定上下文的基线模型展开评估对比。\n",
      "\n",
      "首先来一致性评估。研究者引入了一个基于 MSC 数据集的深层记忆检索（deep memory retrieval, DMR）任务，旨在测试对话智能体的一致性。在 DMR 中，用户向对话智能体提出一个问题，并且该问题明确引用先前的对话，预期答案范围会非常窄。具体可以参加下图 5 示例。\n",
      "\n",
      "MemGPT 利用内存来保持一致性。下表 2 显示了 MemGPT 与固定记忆基线模型的性能对比，包括 GPT-3.5 和 GPT-4。\n",
      "\n",
      "可以看到，MemGPT 在 LLM 判断准确度和 ROUGE-L 分数方面显著优于 GPT-3.5 和 GPT-4。MemGPT 能够利用回想记忆（Recall Memory）查询过去的对话历史，进而回答 DMR 问题，而不是依赖递归摘要来扩展上下文。\n",
      "\n",
      "然后在「对话开场白」任务中，研究者评估智能体从先前对话积累的知识中提取引人入胜的消息并传递给用户的能力。\n",
      "\n",
      "研究者在下表 3 中展示了 MemGPT 开场白的 CSIM 分数。结果表明，MemGPT 能够制作引人入胜的开场白，其表现可以媲美甚至超越人类手写的开场白。此外还观察到 MemGPT 倾向于制作比人类基线更长且涵盖更多角色信息的开场白。下图 6 为示例。\n",
      "\n",
      "用于文档分析的 MemGPT\n",
      "\n",
      "为了评估 MemGPT 分析文档的能力，研究者对 MemGPT 以及在 Liu et al. (2023a) 检索器 - 阅读器文档 QA 任务上的固定上下文基线模型进行了基准测试。\n",
      "\n",
      "结果显示，MemGPT 能够通过查询档案存储有效地对检索器进行多次调用，从而可以扩展到更大的有效上下文长度。MemGPT 主动从档案存储中检索文档并且可以迭代地分页浏览结果，因而其可用的文档总数不再受到适用 LLM 处理器上下文窗口的文档数量的限制。\n",
      "\n",
      "由于基于嵌入的相似性搜索的局限性，文档 QA 任务对所有方法都构成了极大的挑战。研究者观察到，MemGPT 会在检索器数据库耗尽之前停止对检索器结果进行分页操作。\n",
      "\n",
      "此外 MemGPT 更复杂操作所创建的检索文档容量也存在权衡，如下图 7 所示，其平均准确度低于 GPT-4（高于 GPT-3.5），但可以轻松地扩展到更大的文档。\n",
      "\n",
      "研究者还引入了一项基于合成键值检索的新任务，即嵌套键值检索（Nested Key-Value Retrieval），用以演示 MemGPT 如何对来自多个数据源的信息进行整理。\n",
      "\n",
      "从结果来看，虽然 GPT-3.5 和 GPT-4 在原始键值任务上表现出了良好性能，但在嵌套键值检索任务中表现不佳。而 MemGPT 不受嵌套层数的影响，并能够通过函数查询重复访问存储在主内存中的键值对，来执行嵌套查找。\n",
      "\n",
      "MemGPT 在嵌套键值检索任务上的性能，展示了其利用多个查询的组合执行多条查找的能力。\n",
      "\n",
      "更多技术细节和实验结果请参阅原论文。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2023-10-18-3\n",
      "title= 未来大模型顶会？陈丹琦等人组织首届COLM，为语言建模研究提供新平台\n",
      "author= []\n",
      "publish_date= 2023-10-18 00:00:00\n",
      "text= 获得了众多学术大牛的支持。\n",
      "\n",
      "随着 ChatGPT 的爆火，NLP、大模型领域已经是个「一天不看新闻就会落后」的地方。\n",
      "\n",
      "虽然如今 AI 领域的顶级学术会议已经不少，但似乎也无法满足大家投论文的速度了。\n",
      "\n",
      "现在，一批知名的青年学者们，组织了一个名为 COLM（Conference on Language Modeling）的新会议。\n",
      "\n",
      "COLM 是一个专注于语言建模研究的学术场所。在会议介绍中，组织这门十分明确地讲述了他们的目的是创建一个具有不同科学专业知识的研究人员社区，专注于理解、改进和评论语言模型技术的发展。这不仅是学术界的一次创新尝试，也是搭起了语言模型交流互鉴的新桥梁，进一步促进其探索和合作。\n",
      "\n",
      "会议连接 https://colmweb.org/\n",
      "\n",
      "组织者介绍\n",
      "\n",
      "该会议的组织者们是 NLP 头部科学家们，在语言建模研究有着相当成果的。他们其中既有来自业界的研究人员，也有来自学术界的研究人员。组织者共有七人，分别是 Yejin Choi、Denny Zhou、Dipanjan Das、陈丹琦、Yoav Artzi、Angela Fan、Alexander Rush。\n",
      "\n",
      "而组织者中的 Denny Zhou、陈丹琦、Angela Fan 等人也是我们熟知的 AI 华人学者。\n",
      "\n",
      "陈丹琦\n",
      "\n",
      "\n",
      "\n",
      "陈丹琦是普林斯顿大学计算机科学的助理教授，也是普林斯顿 NLP 小组的共同负责人。本科就读于清华大学，后获得斯坦福大学计算机科学博士学位。陈丹琦凭借其在自然语言处理领域取得的一系列成果，荣膺 2019 年《麻省理工科技评论》「35 岁以下科技创新 35 人」中国区得主，获奖时年龄 29 岁。就在前些天，陈丹琦团队提出 LLM-Shearing 大模型剪枝法。\n",
      "\n",
      "Denny Zhou\n",
      "\n",
      "Denny Zhou 是 Google DeepMind 的首席科学家 / 研究主管，他是推理团队的创立者和现任负责人。主要研究兴趣在于构建和教导大语言模型实现类人的推理能力。他领导的团队已经开发了思维链提示、自洽性解码、最少到最多提示、指令调优（FLAN2）、LLM 自我调试等大语言模型的各种涌现属性。Denny Zhou 曾获得 2022 年谷歌研究技术影响力奖（Google Research Tech Impact Award）。\n",
      "\n",
      "Angela Fan\n",
      "\n",
      "Angela Fan 是 Meta AI Research Paris 的研究科学家，主要研究机器翻译。此前她曾在南锡 INRIA 和巴黎 FAIR 攻读博士学位，主要研究文本生成。在此之前，她是一名研究工程师，并在哈佛大学获得了统计学学士学位。\n",
      "\n",
      "会议投稿主题\n",
      "\n",
      "组织者们为本次会议的投稿提供了广泛而全面的主题\n",
      "\n",
      "对齐：微调、指令微调、强化学习（包括人工反馈）、提示调整和上下文对齐\n",
      "\n",
      "数据：预训练数据、对齐数据和通过人工或算法分析、整理和生成的合成数据\n",
      "\n",
      "评估：基准、模拟环境、可扩展监督、评估协议和指标、人工或机器评估\n",
      "\n",
      "社会影响：偏见、公平、滥用、就业、气候变化等\n",
      "\n",
      "安全：安全、隐私、错误信息、对抗性攻击和防御\n",
      "\n",
      "LM 的科学性：扩展规律、基本限制、新兴能力、解密、可解释性、复杂性、训练动态、学习理论\n",
      "\n",
      "高效计算 LM：蒸馏、压缩、量化等\n",
      "\n",
      "大型 LM 的工程设计：不同硬件设置上的分布式训练和推理、训练动态、优化不稳定性\n",
      "\n",
      "LM 的学习算法：学习、元学习、模型混合法、持续学习等\n",
      "\n",
      "LM 的推理算法：解码算法、推理算法、搜索算法、规划算法\n",
      "\n",
      "人类心智、大脑、哲学、法律与 LM：从认知科学、神经科学、语言学、心理语言学、哲学或法律角度看待 LM\n",
      "\n",
      "面向所有人的 LM：多语言、低资源语言、方言、多元文化、价值多元化\n",
      "\n",
      "LM 与世界：事实性、检索增强型 LM、知识模型、常识推理、心智理论、社会规范、语用学和世界模型\n",
      "\n",
      "具身 LM：感知、行动、机器人和多模态\n",
      "\n",
      "LM 与互动：对话、互动学习和多人学习\n",
      "\n",
      "LM 与工具和代码：与工具和 API 的集成、LM 驱动的软件工程\n",
      "\n",
      "关于各种模式和新型应用的 LM：视觉 LM、代码 LM、数学 LM 等，特别鼓励研究较少的模式或应用，如化学、医学、教育、数据库等\n",
      "\n",
      "COLM 发表后，获得了许多支持与期待。\n",
      "\n",
      "与 ICLR 相似，COLM 也是领域中的创新会议，它们都有着知名学者们的倾力支持。除此之外，ICLR 采用的开放评审备受好评，并开公开透明之先河。COLM 投稿将采用双盲审核，并将使用开放评审的方式管理投稿。至于它是否能像 ICLR 一样后来居上，就请拭目以待吧。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2023-10-18-2\n",
      "title= 端侧AI推理，高效部署PyTorch模型：官方新工具开源，Meta已经用上了\n",
      "author= []\n",
      "publish_date= 2023-10-18 00:00:00\n",
      "text= ExecuTorch 是一个端到端的解决方案，可以在移动和边缘设备（包括可穿戴设备、手机等）上实现推理功能。\n",
      "\n",
      "在 2023 年 PyTorch 大会上，一个深受大家关心的推理问题得到了很好的解决，会上宣布了一个用于在边缘和移动设备上实现 AI 推理的解决方案：ExecuTorch，并且还是开源的，而促成这一研究的，正是 Meta AI 与 PyTorch 基金会。\n",
      "\n",
      "ExecuTorch 地址：https://github.com/pytorch/executorch\n",
      "\n",
      "学习文档：https://pytorch.org/executorch/stable/index.html\n",
      "\n",
      "随着 ExecuTorch 的开源，预示着 AI 应用程序在设备上本地运行、而需连接到服务器或云成为可能。我们可以将 ExecuTorch 理解成一个 PyTorch 平台，其能提供基础设施来运行 PyTorch 程序，从 AR/VR 可穿戴设备到标准的 iOS 和 Android 设备的移动部署。\n",
      "\n",
      "ExecuTorch 最大优势是可移植性，能够在移动和嵌入式设备上运行。不仅如此，ExecuTorch 还可以提高开发人员的工作效率。\n",
      "\n",
      "据了解，Meta 已经验证了这项技术，并将其用于最新一代的雷朋智能眼镜，而这款眼镜也是 Meta 最近发布的 Quest 3 VR 头显的一部分。Meta 表示，作为开源 PyTorch 项目的一部分，他们旨在进一步推动该技术的研究，从而迈入在设备上实现 AI 推理的新时代。\n",
      "\n",
      "Facebook 创始人、Meta 董事长兼首席执行官扎克伯格表示：「作为开源 AI 工作的一部分，我们与 PyTorch 基金会及其行业合作伙伴一起开源了 ExecuTorch。这一变化预示着将 PyTorch 引入了手机和可穿戴设备等边缘计算平台。ExecuTorch 使 AI 模型能够直接在设备上运行，而无需连接到服务器。」\n",
      "\n",
      "Meta 软件工程师 Mergen Nachin 指出，「今天的 AI 模型正在从服务器扩展到边缘设备，如移动设备、AR、VR 和 AR 头显、可穿戴设备、嵌入式系统等。ExecuTorch 通过提供端到端的工作流来优化本地程序，从而解决边缘设备遇到的挑战。」\n",
      "\n",
      "ExecuTorch 关键组件\n",
      "\n",
      "ExecuTorch 提供了紧凑的运行时和轻量级操作注册表，以覆盖 PyTorch 模型生态系统，以及在边缘设备上执行 PyTorch 程序的简化路径。此外，ExecuTorch 还附带 SDK 和工具链，为 ML 开发人员提供了更好的用户体验。\n",
      "\n",
      "作为 PyTorch Edge 生态系统的一部分，ExecuTorch 可以有效地将 PyTorch 模型部署到边缘设备。ExecuTorch 的优点包括：\n",
      "\n",
      "可移植性：与各种计算平台兼容，从高端移动手机到高度受限的嵌入式系统和微控制器。\n",
      "\n",
      "提高生产力：开发人员能够使用相同的工具链和 SDK，从而提高生产力。\n",
      "\n",
      "提高性能：由于轻量级运行时和充分利用 CPU、NPU 和 DSP 等硬件功能，为最终用户提供了无缝和高性能的体验。\n",
      "\n",
      "由于 ExecuTorch 严重依赖 PyTorch 相关知识，因而，想要熟练掌握 ExecuTorch，还需提前补充相关知识。官方文档已经提供了入门级教程。例如，在构建 ExecuTorch Android 演示应用程序示例当中，大家可以跟随指导教程，从而熟悉如何使用 ExecuTorch。\n",
      "\n",
      "最后，需要提醒大家的一点是，本次发布的 ExecuTorch 是一个预览版本，在测试和评估中可以使用，但是不建议在生产环境中使用。PyTorch 团队欢迎来自社区的任何反馈、建议和错误报告，以帮助他们改进技术。\n",
      "\n",
      "参考链接：\n",
      "\n",
      "https://venturebeat.com/ai/pytorch-executorch-extends-open-source-ai-for-new-quests-at-the-edge/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-11\n",
      "title= 俄罗斯小哥ChatGPT找女友：聊了5239个女生，现在订婚了\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 有事 AI 它是真上啊。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "「我向一位女生求婚，ChatGPT 已经和她交流了一年。为了走到这一步，AI 已经尝试了和 5239 名女生进行过沟通……」\n",
      "\n",
      "来源：https://twitter.com/biblikz/status/1752335415812501757\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最近几天，社交网络上人们正在轮番向一位俄罗斯小哥送去祝福。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "23 岁的 Aleksandr Zhadan 是一名 AI 开发者，也是社交平台 TenChat 的一名产品经理。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "故事是这样开始的：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GPT 除了搜索之外，还可以在配对后写入。这样在 50 次自动执行中，他可以获得 18 次配对。GPT 在没有 Aleksandr 的干预下根据以下 prompt 与人交流：你是一个男生，第一次和女生说话。你的任务是：邀请她约会，但不是马上。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最开始进展并不顺利，GPT-3 的约会和 DM 游戏能力很弱。Aleksandr 对它进行了改进，加入了记忆、微调和示例。为了找到最相关的女生，他通过 torchvision 在 Tinder 网络版中使用了照片识别，并根据自己的滑动进行训练。这样一来，GPT 几乎总是能正确地选择合适的女生。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "随着使用 ChatGPT 和 GPT-4 API，AI 变得越来越强大了，它们可以执行几十次对话、配对、安排约会。就这样经过一年的 AI 聊天，Aleksandr 找到了自己想要携手一生的 Karina—— 善解人意、开朗活泼、善良、独立。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最终，Aleksandr 向 Karina 求婚了。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "羡煞旁人之余，质疑声也有。有人认为这个故事是「AI 生成的」。\n",
      "\n",
      "图源：https://twitter.com/Darkolorin/status/1753135894750458268\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "也有社区对 Aleksandr 做了一个小研究，认为他很可能只是在炒作，并指出他此前用 ChatGPT 撰写大学毕业论文并获得文凭，所以非常习惯「炒作浪潮」。\n",
      "\n",
      "图源：https://twitter.com/literallydenis/status/1753177433073664236\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "去年 2 月，Alexandr 用 ChatGPT 写论文的事还被媒体报道过。\n",
      "\n",
      "Aleksandr 在俄罗斯国立人文大学进行了现代组织管理的学位论文答辩，在这名学生的故事引起公众广泛关注后，该大学还召集他进行了演讲。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "用 ChatGPT 找女友，需要几步？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "随着这位俄罗斯小哥的「事迹」越来越火，大家伙当然想知道怎么才能像他一样用 ChatGPT 找到人生的另一半。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "今日，一位推特博主详细分享了 Aleksandr 如何一步步找到自己的意中人。\n",
      "\n",
      "图源：https://twitter.com/8teAPi/status/1754535819493405036\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "相比线下相亲，用 AI 找女友还是蛮简单的。Aleksandr Zhadan 将这个过程分为两步：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "第一步是找女孩；\n",
      "\n",
      "第二步是和她聊天。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "找女孩阶段：当 Aleksandr 在 Tinder 上找女孩时，他使用网略爬虫获取图像，最开始 Aleksandr 倾向于那些在 Tinder 上的照片超过两张的女孩。在迭代过程中，Aleksandr 训练了一个图像相似性模型，该模型能够找到与他喜欢的女孩相似的女孩照片。\n",
      "\n",
      "在聊天阶段：GPT-3 会主动开启对话，这个阶段给 GPT-3 的提示语是「你是个男生，第一次和这个女孩说话。你的任务不是立刻、马上要求对方干什么，而是邀请那个女孩来一次约会。」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aleksandr 表示一开始 GPT-3 表现非常糟糕，经常忘记对话，并且由于机器人无法访问 Telegram，因此他失去了一半的潜在约会机会。更糟糕的是，机器人承诺约会时会送鲜花或巧克力…… 而真正线下约会时却没有这个环节，因而被投诉了。\n",
      "\n",
      "第一代约会机器人（ Datebot V1）的战报：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "匹配了 353 个女孩的 Tinder 档案；\n",
      "\n",
      "总共聊了 160 次（约占匹配人数的 45%）；\n",
      "\n",
      "有 12 次约会（占聊天次数的 7.5%）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "面对这一结果，Aleksandr 并没有灰心，和朋友继续升级这个机器人，因此第二代机器人（Datebot V2）出现了，这次，Aleksandr 他们采用：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GPT-4 进行聊天；\n",
      "\n",
      "每个 聊天机器人 都有一个记忆，包含最简单约会问题的背景故事；\n",
      "\n",
      "从 Tinderbot 切换到 Telegrambot 进行消息传递；\n",
      "\n",
      "集成 Google 日历，用于设置日期；\n",
      "\n",
      "对收到的消息进行人工循环验证。\n",
      "\n",
      "第二代机器人的结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GPT-4 幻觉降至零；\n",
      "\n",
      "匹配了 4886 条 Tinder 个人资料；\n",
      "\n",
      "无数约会，Aleksandr 用「多到吓人」来形容。\n",
      "\n",
      "接下来，Aleksandr 有过很多次约会，最多的时候还和 4 个女孩周旋，直到一位名叫 Karina 的女孩出现。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在结识了 Karina 之后，Aleksandr 专门推出了第三代机器人（Datebot V3）：它被设置成只会与 Karina 聊天。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "给 Datebot V3 的提示是「与 Karina 保持良好的关系，告诉我是否有什么负面的事情需要注意，或者是否需要回答问题。」\n",
      "\n",
      "在此期间，Aleksandr 还利用剩下的人脉还建立了一个新的推荐工作的副项目：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aleksandr 在俄罗斯就业门户网站上发现了愿意为推荐员工付费的职位空缺；\n",
      "\n",
      "Aleksandr 将与女孩的对话与潜在的工作相匹配；\n",
      "\n",
      "出售 GPT-4 形成的联系人 / 关系；\n",
      "\n",
      "成功安排 8 名员工并获得报酬。\n",
      "\n",
      "随着关系的深入，Datebot V3 告诉 Aleksandr 应该与 Karina 结婚，机器人不仅提出了求婚建议，还帮忙策划了一场浪漫的求婚行动。\n",
      "\n",
      "最终 他耗费 120 小时打造的机器人帮他找到了女友。他 表示调用 GPT API 花费 1432 美元、餐厅约会花费 200 卢布，但他的副项目帮他赚了 526 卢布，现在 他 已经向女友求婚了。\n",
      "\n",
      "总而言之，这是一个有点肆无忌惮但又温馨的故事。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "虽然 Karina 一直并不知道 ChatGPT 在她与 Aleksandr 的交往中扮演的角色。当他们向婚姻登记处提交申请时，她第一次发现了这件事，不过反应很平静。他们也将于今年 8 月完婚。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考链接：\n",
      "\n",
      "https://www.emergentbehavior.co/p/chatgpt-find-me-a-wife-on-tinder\n",
      "\n",
      "https://www.news18.com/world/chatgpt-love-story-russian-man-deploys-meets-his-match-using-ai-bot-on-tinder-8767366.html\n",
      "\n",
      "https://www.themoscowtimes.com/2023/02/02/russian-student-allowed-to-keep-diploma-for-chatgpt-written-thesis-a80125\n",
      "\n",
      "https://www.cryptoglobe.com/latest/2024/02/chatgpt-said-marry-her-ai-developers-journey-starts-with-tinder-swipes-ends-with-a-ring/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-10\n",
      "title= 徒手搬汽车配件，波士顿动力Atlas再进化：兄弟们，准备进厂了\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 在如今颇为火热的人形机器人赛道，波士顿动力是较早入局的一位选手。\n",
      "\n",
      "过去几年，Atlas 人形机器人的动态跑酷能力已经让全世界的关注，后来我们还看到 Atlas 在模拟建筑工地上搬搬扛扛。Atlas 目前仍然是一个开发平台，尚不能在现实世界中工作，一部分原因是它的液压驱动设计。\n",
      "\n",
      "不过，波士顿动力最新公布的一个演示视频表明了 Atlas 和其他人形机器人一样能够完成高难度的操纵任务，包括在装备适当的情况下操纵重物。\n",
      "\n",
      "在视频中，Atlas 稳稳抓起一个比自己手臂还粗的汽车配件，搬运到目标位置：\n",
      "\n",
      "实际上，Atlas 在很长一段时间都是没有手指的，而是两个黑色的球体。去年初，波士顿动力在它的手臂末端装上了「螃蟹夹」。现在，它的手指又进化成了三根，虽然不如人类五指灵巧、柔软，但也足以牢牢抓起圆形的配件：\n",
      "\n",
      "切换第一视角看下，是这样的：\n",
      "\n",
      "继续搬运下一个，在去到新目标所在位置的过程中，Atlas 险些摔倒，但最终还是稳住了：\n",
      "\n",
      "看得出来，这配件不是一般的沉重，Atlas 拿起的过程也有些吃力：\n",
      "\n",
      "【关注 机器之心 视频号，第一时间看到有趣的 AI 内容】\n",
      "\n",
      "一直以来，波士顿动力公司（Boston Dynamics）因其在 机器人技术 方面的突破性创新而闻名于世，其中包括 Atlas 双足人形机器人，它能跑能跳，做出各种惊人的动作。\n",
      "\n",
      "不过，Atlas 并不是唯一「准备进厂」的人形机器人。\n",
      "\n",
      "随着人形机器人竞赛的升温，来自 Agility Robotics、Apptronik、Figure 这些公司的人形机器人，同样已经接近在现实世界中找工作的水准了。这些公司的估值也水涨船高，比如据知情人士透露，微软与 OpenAI 正在洽谈参与 Figure 的新一轮融资，其中微软可能将投资约 9500 万美元，OpenAI 将投资 500 万美元。\n",
      "\n",
      "Figure 首款人形机器人。\n",
      "\n",
      "人形机器人的技术和落地进展，总是备受瞩目。虽然一度有人质疑：「把它做成人形机器人有什么意义？任何机械臂都能以 10 倍的速度和理想的精度完成这项任务，这看起来只是一种廉价的炒作。」\n",
      "\n",
      "但至少对于波士顿动力，在公司亏本运营的同时还能对人形机器人投入十多年的研发，显然不是为了炒作。\n",
      "\n",
      "长远来看，人形机器人因其占地面积小、动作灵活的特性，会适用于更多的应用场景。\n",
      "\n",
      "Figure 创始人兼首席执行官 Brett Adcock 表示：「几十年来，单一用途的 机器人技术 的商业市场趋于饱和，但通用 机器人技术 的潜力却完全没有被挖掘出来。」Figure 的人形机器人同样准备好了进厂，第一份工作就在 宝马 位于美国南卡罗来纳州的斯帕坦堡工厂。\n",
      "\n",
      "特斯拉也在努力打造双足人形机器人并将其商业化，该机器人被命名为「擎天柱」（Optimus），且已更新到了 Optimus Gen 2 。据特斯拉初步声明，Optimus 的售价应在 2 万美元左右。\n",
      "\n",
      "先前的一段视频已经展示了 Optimus 独立叠衬衫的过程：\n",
      "\n",
      "在最新公布的视频中，「擎天柱」已经实现了在无人帮助的情况下自主行走，步伐轻盈，机械臂还能随着步伐摆动（尽管动作缓慢）：\n",
      "\n",
      "假设人形机器人真能取代人类完成工作，可能会导致一些行业就业率下降甚至消失，但毫无疑问，我们仍然期待这一天尽早到来。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-9\n",
      "title= 胡渊鸣创业公司Meshy产品升级：文本转3D，25秒就能出预览\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= Meshy-2 的文本转 3D、图像转 3D、文本到纹理效果均有所提升。\n",
      "\n",
      "刚刚，胡渊鸣创业公司 Meshy 官宣了他们的第二代产品 ——Meshy-2。\n",
      "\n",
      "Meshy 是一款 3D 内容生成工具，只需一分钟，即可使用 Al 生成 3D 内容（模型）。公司联合创始人兼 CEO 胡渊鸣是 计算机图形 学知名学者，毕业于 清华大学 姚班，是 MIT 博士，也是「太极」（TaiChi）编程语言作者。\n",
      "\n",
      "具体来说，Meshy 提供三种很容易上手的使用方式，包括文本转 3D（输入文字 —— 输出 3D 模型）、图像转 3D（提供图片 —— 生成 3D 模型）以及从文本到纹理（透过简单文本描述就能为 3D 内容添加纹理）。在 Meshy-1 中，我们已经看到了一些精彩的演示（见以下视频）。\n",
      "\n",
      "【关注 机器之心 视频号，第一时间看到有趣的 AI 内容】\n",
      "\n",
      "时隔三个月，Meshy-2 就问世了。 新版本的升级之处如下图所示： 接下来我们看一下具体内容。\n",
      "\n",
      "文本转 3D\n",
      "\n",
      "胡渊鸣在博客中表示，在过去的几个月里，他们一直专注于文本转 3D。网格和纹理对于 3D 对象来说是必不可少的，Meshy-2 能生成结构更好的网格和丰富的几何细节。同时，纹理质量也更加细腻。\n",
      "\n",
      "Prompt: prehistoric winter boots with wool, realistic, 4K, high quality.\n",
      "\n",
      "Prompt: royal armor set, gold, iron, highly detailed, medieval, knight armor, leather.\n",
      "\n",
      "Meshy-2 提供了 4 种文本转 3D 的风格：写实、卡通、低多边形（Low Poly）和体素，希望可以激发新的创作方向。\n",
      "\n",
      "效率方面，Meshy-2 的文本转 3D 速度更快了：25 秒就可以出预览，5 分钟内就能出精细结果。\n",
      "\n",
      "此外，他们还推出了一款用户友好的网格编辑器，具有多边形计数控制（polycount control）和四边形网格转换系统。这个方便的工具旨在为 3D 生成提供更多的控制和灵活性。\n",
      "\n",
      "文本到纹理\n",
      "\n",
      "在 Meshy-2 中，文本到纹理的功能也得到了改善，以获得更高的清晰度，使得纹理尽可能逼真和清晰。同时，它的运行速度是其前身的两倍。\n",
      "\n",
      "图像转 3D\n",
      "\n",
      "新的图像转 3D 功能可以在 2 分钟内产生更高质量的结果。\n",
      "\n",
      "从 Discord 到网页应用\n",
      "\n",
      "在 Meshy-1 推出时，大家可以加入 Discord 体验这款新工具。但胡渊鸣表示，在 Meshy-2 推出后，他们会将重点从 Discord 转移到网页应用程序，因为大多数用户已经在使用新的网页应用程序。与此同时，他们将逐步关闭 Discord 上的 3D 生成服务。\n",
      "\n",
      "目前，Meshy-2 的网页应用是免费的，不过想升级到 Pro 或 Max 就需要付费了（打折促销码：MESHY2GO）。\n",
      "\n",
      "感兴趣的读者可以去尝试一下：https://app.meshy.ai/zh/login \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-8\n",
      "title= 通义千问再开源，Qwen1.5带来六种体量模型，性能超越GPT3.5\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 超越 Claude、GPT-3.5，提升了多语言支持能力。\n",
      "\n",
      "赶在春节前，通义千问大模型（Qwen）的 1.5 版上线了。今天上午，新版本的消息引发了 AI 社区关注。\n",
      "\n",
      "新版大模型包括六个型号尺寸：0.5B、1.8B、4B、7B、14B 和 72B，其中最强版本的性能超越了 GPT 3.5、Mistral-Medium，包括 Base 模型和 Chat 模型，且有多语言支持。\n",
      "\n",
      "阿里通义千问团队表示，相关技术也已经上线到了通义千问官网和通义千问 App。\n",
      "\n",
      "除此以外，今天 Qwen 1.5 的发布还有如下一些重点：\n",
      "\n",
      "支持 32K 上下文长度；\n",
      "\n",
      "开放了 Base + Chat 模型的 checkpoint；\n",
      "\n",
      "可与 Transformers 一起本地运行；\n",
      "\n",
      "同时发布了 GPTQ Int-4 / Int8、AWQ 和 GGUF 权重 。\n",
      "\n",
      "借助更先进的大模型作为评委，通义千问团队在两个广泛使用的 基准 MT-Bench 和 Alpaca-Eval 上对 Qwen1.5 进行了初步评估，评估结果如下：\n",
      "\n",
      "尽管落后于 GPT-4-Turbo，但最大版本的 Qwen1.5 模型 Qwen1.5-72B-Chat 在 MT-Bench 和 Alpaca-Eval v2 上都表现出了可观的效果，性能超过 Claude-2.1、GPT-3.5-Turbo-0613、Mixtral-8x7b-instruct 和 TULU 2 DPO 70B，与最近热门的新模型 Mistral Medium 不相上下。\n",
      "\n",
      "此外通义千问团队表示，虽然大模型判断的评分似乎与回答的长度有关，但人类观察结果表明 Qwen1.5 并没有因为产生过长的回答来影响评分。AlpacaEval 2.0 上 Qwen1.5-Chat 的平均长度为 1618，与 GPT-4 的长度一致，比 GPT-4-Turbo 短。\n",
      "\n",
      "通义千问的开发者表示，最近几个月，他们一直在专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。\n",
      "\n",
      "相较于以往版本，本次更新着重提升了 Chat 模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为人们带来更佳体验。\n",
      "\n",
      "基础能力\n",
      "\n",
      "关于模型基础能力的评测，通义千问团队在 MMLU（5-shot）、C-Eval、 Humane val、GS8K、BBH 等 基准 数据集上对 Qwen1.5 进行了评估。\n",
      "\n",
      "在不同模型尺寸下，Qwen1.5 都在评估 基准 中表现出强大的性能，72B 的版本在所有 基准 测试中都超越了 Llama2-70B，展示了其在语言理解、推理和数学方面的能力。\n",
      "\n",
      "最近一段时间，小型模型的构建是业内热点之一，通义千问团队将模型 参数 小于 70 亿的 Qwen1.5 模型与社区中重要的小型模型进行了比较：\n",
      "\n",
      "在 参数 规模低于 70 亿的范围内 Qwen1.5 与业界领先的小型模型相比具有很强的竞争力。\n",
      "\n",
      "多语言能力\n",
      "\n",
      "在来自欧洲、东亚和东南亚的 12 种不同语言上，通义千问团队评估了 Base 模型的多语言能力。从开源社区的公开数据集中，阿里研究者构建了如下表所示的评测集合，共涵盖四个不同的维度：考试、理解、翻译、数学。下表提供了每个测试集的详细信息，包括其评测配置、评价指标以及所涉及的具体语言种类。\n",
      "\n",
      "详细的结果如下：\n",
      "\n",
      "上述结果表明，Qwen1.5 Base 模型在 12 种不同语言的多语言能力方面表现出色，在学科知识、语言理解、翻译、数学等各个维度的评估中，均展现了不错的结果。更进一步地，在 Chat 模型的多语言能力上，可以观察到如下结果：\n",
      "\n",
      "长序列\n",
      "\n",
      "随着长序列理解的需求不断增加，阿里在新版本上提升了千问模型的相应能力，全系列 Qwen1.5 模型支持 32K tokens 的上下文。通义千问团队在 L-Eval 基准 上评估了 Qwen1.5 模型的性能，该 基准 衡量了模型根据长上下文生成响应的能力。结果如下：\n",
      "\n",
      "从结果来看，即使像 Qwen1.5-7B-Chat 这样的小规模模型，也能表现出与 GPT-3.5 可比较的性能，而最大的模型 Qwen1.5-72B-Chat 仅略微落后于 GPT4-32k。\n",
      "\n",
      "值得一提的是，以上结果仅展示了 Qwen 1.5 在 32K tokens 长度下的效果，并不代表模型最大只能支持 32K 长度。开发者可以在 config.json 中，将 max_position_embedding 尝试修改为更大的值，观察模型在更长上下文理解场景下，是否可以实现令人满意的效果。\n",
      "\n",
      "链接外部系统\n",
      "\n",
      "如今，通用 语言模型 的一大魅力在于其与外部系统对接的潜在能力。RAG 作为一种在社区中快速兴起的任务，有效应对了大 语言模型 面临的一些典型挑战，如幻觉、无法获取实时更新或私有数据等问题。此外， 语言模型 在使用 API 和根据指令及示例编写代码方面，展现出了强大的能力。大模型能够使用代码解释器或扮演 AI 智能体，发挥出更为广阔的价值。\n",
      "\n",
      "通义千问团队对 Qwen1.5 系列 Chat 模型在 RAG 任务上的端到端效果进行了评估。评测基于 RGB 测试集，是一个用于中英文 RAG 评估的集合：\n",
      "\n",
      "然后，通义千问团队在 T-Eval 基准 测试中评估了 Qwen1.5 作为通用智能体运行的能力。所有 Qwen1.5 模型都没有专门面向 基准 进行优化：\n",
      "\n",
      "为了测试工具调用能力，阿里使用自身开源的评估 基准 测试模型正确选择、调用工具的能力，结果如下：\n",
      "\n",
      "最后，由于 Python 代码解释器已成为高级 LLM 越来越强大的工具，通义千问团队还在之前开源的评估 基准 上评估了新模型利用这一工具的能力：\n",
      "\n",
      "结果表明，较大的 Qwen1.5-Chat 模型通常优于较小的模型，其中 Qwen1.5-72B-Chat 接近 GPT-4 的工具使用性能。不过，在数学解题和可视化等代码解释器任务中，即使是最大的 Qwen1.5-72B-Chat 模型也会因编码能力而明显落后于 GPT-4。阿里表示，会在未来的版本中，在预训练和对齐过程中提高所有 Qwen 模型的编码能力。\n",
      "\n",
      "Qwen1.5 与 HuggingFace transformers 代码库进行了集成。从 4.37.0 版本开始，开发者可以直接使用 transformers 库原生代码，而不加载任何自定义代码（指定 trust_remote_code 选项）来使用 Qwen1.5。\n",
      "\n",
      "在开源生态上，阿里已经与 vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于 量化 ）、Axolotl、LLaMA-Factory（用于微调）以及 llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列目前也可以在 Ollama 和 LMStudio 等平台上使用。\n",
      "\n",
      "参考内容：\n",
      "\n",
      "https://qwenlm.github.io/blog/qwen1.5/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-7\n",
      "title= 向完全自主性更进一步，清华、港大全新跨任务自我进化策略让智能体学会「以经验为鉴」\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 「以史为鉴，可以知兴替。」 人类的进步史，可以看作是一个不断吸取过去经验、不断推进能力边界的自我演化过程。在这个过程中，我们吸取过去失败的教训以纠正错误，借鉴成功的经验以提升效率和效果。这种自我进化的过程在我们的生活中无所不在：从如何总结经验以更好地解决工作中的问题，到如何利用规律更精确地预测天气，我们都在不断地从过去的经验中学习和进化。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "成功从过去的经验中提取知识并将其应用于未来的挑战，这是人类进化之路上重要的里程碑。那么在 人工智能 时代，AI 智能体是否也可以做到同样的事情呢？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "近年来，GPT 和 LLaMA 等 语言模型 展示了他们在解决复杂任务时的惊人能力。然而，他们尽管可以利用工具解决具体任务，但在本质上缺乏对过去成功和失败经历的洞见与汲取。这就像一个只会完成特定任务的机器人，虽然在完成当下任务上表现出色，但面对新的挑战时，却无法调用过去的经验来提供帮助。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "针对这一难题，近期来自 清华大学 、香港大学、人民大学以及面壁智能的联合团队提出了一种全新的智能体自我演化策略：探索 - 固化 - 利用（Investigate-Consolidate-Exploit，ICE）。它旨在通过跨任务的自我进化来提升 AI 智能体的适应性和灵活性。其不仅能提升智能体处理新任务时的效率和效果，还能显著降低对智能体基座模型能力的需求。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个策略的出现，无疑为智能体的自我进化开启了全新的篇章，也意味着我们离实现智能体的完全自主性又迈进了一步。\n",
      "\n",
      "论文标题：Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution\n",
      "\n",
      "论文链接：https://arxiv.org/abs/2401.13996\n",
      "\n",
      "智能体任务间经验迁移以实现自我进化概览图\n",
      "\n",
      "智能体自我进化的两个方面： 规划 与执行\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "当下大部分复杂智能体都可以分成任务 规划 （Planning）与任务执行（Execution）两大方面。在任务 规划 上，智能体通过推理将用户需求细化并制定完成目标的详细策略；而在任务执行上，智能体通过工具调用实现与环境的交互，从而完成相应子目标。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了更好地促进以往经验的重复利用，作者首先将这两方面的 进化策略 解耦。他们以 XAgent 智能体架构中的树状任务 规划 结构以及 ReACT 链式工具执行为例，分别介绍了 ICE 策略的具体实现。\n",
      "\n",
      "智能体任务 规划 的 ICE 自我演化策略\n",
      "\n",
      "对于任务 规划 ，自我进化依照 ICE 被分为以下三个阶段：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在探索阶段，智能体记录下整个树状任务 规划 结构，并同时动态检测各个子目标的执行状态；\n",
      "\n",
      "在固化阶段，智能体首先剔除所有失败的目标结点，之后对于每个成功完成的目标，智能体将以该目标为子树的所有叶子结点依次排开形成一条 规划 链（Workflow） ；\n",
      "\n",
      "在利用阶段，这些 规划 链将被作为新任务目标分解细化的参考依据，以利用过往的这些成功经验。\n",
      "\n",
      "智能体任务执行的 ICE 自我演化策略\n",
      "\n",
      "任务执行的自我演化策略依然分为 ICE 三个阶段，其中：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在探索阶段，智能体动态记录每个目标执行的工具调用链，并对工具调用中出现的可能问题进行简单的检测归类；\n",
      "\n",
      "在固化阶段，工具调用链将被转化为类似自动机的 流水线（Pipeline）结构 ，工具调用顺序与调用之间的转移关系将被固定，同时还会去掉重复调用，增加分支 逻辑 等等让自动机自动化执行流程更加鲁棒；\n",
      "\n",
      "在利用阶段，对于相似的目标，智能体将直接自动化执行流水线，从而提升任务完成效率。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "XAgent 框架下的自我进化实验\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "作者在 XAgent 框架中对提出的 ICE 自我演化策略进行了测试，并总结了以下四点发现：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ICE 策略能够显著降低模型的调用次数，从而提升效率，减少开销。\n",
      "\n",
      "存储的经验在 ICE 策略下有着较高的复用率，这证明了 ICE 的有效性。\n",
      "\n",
      "ICE 策略能够提升子任务完成率同时减少 规划 返修的次数。\n",
      "\n",
      "通过以往经验的加持，任务执行对模型能力的要求显著下降。具体来看，使用 GPT-3.5 搭配上之前的任务 规划 与执行经验，效果可以直接媲美 GPT-4。\n",
      "\n",
      "在探索 - 固化进行经验存储后，测试集任务在不同智能体 ICE 策略下的表现\n",
      "\n",
      "同时，作者还进行了额外的消融实验：在存储经验逐渐增加的情况下，智能体的表现是否越来越好？答案是肯定的。从零经验，半经验，到满经验，基座模型的调用次数逐渐减少，而子任务完成度逐渐提升，同时复用率也有升高。这表明更多的过往经验能够更好地促进智能体执行，实现规模效应。\n",
      "\n",
      "在不同经验存储量下，测试集任务表现的消融实验结果统计\n",
      "\n",
      "结语\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "畅想一下，在人人都能够部署智能体的世界中，成功经验的数量会随着智能体个体任务执行不断累积，而用户也可以将这些经验在云端中、社区里进行分享。这些经验将促使智能体不断汲取能力，自我进化，逐渐达到完全自主。我们向这样的时代又迈进了一步。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-6\n",
      "title= 摧毁房价的，可能是Apple Vision Pro\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 机器之能报道 编辑：吴昕 公共场合，“演技”趋于浮夸的人越来越多...... 辛普森一家早就预言了 Apple Vision Pro：） 视频链接：https://mp.weixin.qq.com/s/zfY82n3fbh_p6MJIFSdgMg\n",
      "\n",
      "苹果发售 Vision Pro 以来不到 48 小时，人们开始为之疯狂。一些大城市中心已经出现佩戴 Vision pro 逛街的人，甚至有人戴着它开车、上飞机旅行。\n",
      "\n",
      "有意思的是，在此之前，Meta Quest 已售出超 2000 万台（去年 11 月份的媒体数据，其中 1800 万台是 Quest 2 ），我们几乎看不到类似场景，没人戴着它出街。\n",
      "\n",
      "据媒体披露，早在苹果发布第一代苹果手机时，就已经开始申请 Apple Vision Pro 的专利。苹果为此研发十多年，耗资数十亿美元，光是申请专利就超过 5000 项，几乎从头到尾 重构 了整个产业链。\n",
      "\n",
      "从目前社交媒体上排山倒海的体验视频来看，2024 年 2 月是一个需要被记住的节点，这件事跟一年前 ChatGPT （包括 LLM）发布同等重要，因为一个新的常态可能就此展开。\n",
      "\n",
      "这是透过 Vision Pro 看到的现实世界，很直观。\n",
      "\n",
      "将 Vision Pro 和特斯拉自动驾驶疯狂结合，估计国内交规不会允许出现这些名场面：\n",
      "\n",
      "不过，飞行时长约为 5 小时，对于长时间佩戴者来说，设备还是太重了，他的脸部感觉很紧。另外，更新 visionOS 1.0.2 后，发现电池电量下降到 50%！(：看来续航仍然是一个问题。\n",
      "\n",
      "专为 Apple Vision Pro 打造的应用如雨后春笋。比如，像《沙丘》里一样，沉浸式学习细胞分子结构、为现实世界添加字幕和实时翻译功能、沉浸式冥想、玩桌面经典游戏、在真实世界玩虚拟滑板、看房等。看完下面这段视频，你是否觉得摧毁房价的有可能是 Apple Vision Pro ？\n",
      "\n",
      "除了作为一个物理存在，以后的房屋可能还会有一个 Vision Pro 层。有了后者，一线城市地下室也能住出豪宅的感觉？对了，还可以玩实时换脸！\n",
      "\n",
      "Polycam 也登陆了 Apple Vision Pro 。用户可以通过 Apple Vision Pro 浏览 Polycam 数百万个原生 3D 资源库，还能像在现实中一样拿起它们并放到你想放置的地方。你可以用苹果手机扫描你觉得有趣的东西，Polycam 都会把它变成 3D 模型。戴上 Apple Vision Pro 后，你就可以和它们在 虚拟现实 中互动啦。\n",
      "\n",
      "虽然此前 YouTube、Spotify 和 Netflix 都曾拒绝允许他们的 iPad 应用程序在 Vision Pro 上运行，不过 YouTube 今天表示未来会推出合适的 VisionOS YouTube 应用程序。\n",
      "\n",
      "苹果发言人表示，YouTube 的 360 度和 3D 视频无法在 Vision Pro 上运行良好，因为“很多内容都是为无法提供高质量空间体验的设备创建的。”在某些情况下，这些内容也可能导致运动不适。\n",
      "\n",
      "下面这段视频记录了一位科技博主全程佩戴 Vision Pro 外出一天的记录，包括踩滑板、步行、坐地铁、去快餐店吃饭、和朋友通话、和好奇路人讨论这款设备等。\n",
      "\n",
      "和其他视频不同之处在于，他是在自家客厅里一台高 11 英尺、宽 20 英尺的 4k 屏幕上，用 Vision Pro 编辑的。“我可以站起来走过去，仔细观察这些剪辑，就像二战电影里那些巨大的墙上地图一样。”\n",
      "\n",
      "当然，这台精密的空间计算设备也带来了迄今为止难度最大的拆解挑战。这个视频深入展示出 Vision Pro 极其复杂的硬件结构，据说是苹果目前最复杂的硬件。\n",
      "\n",
      "库克说，Vision Pro 是有史以来最先进的消费电子设备，“空间计算时代已经到来。”\n",
      "\n",
      "但第一代的 Vision Pro 注定不会是一款畅销的电子设备。近期的方舟投资《Big ideas 》指出， 虚拟现实 市场刚刚起步。尽管头显设备有了重大改进，包括苹果的 Vision Pro，但开发人员并没有蜂拥而至支持 虚拟现实 ( VR )。如果没有令人信服的用例，采用速度会很慢。\n",
      "\n",
      "例如，MetaQuest 仅提供 2,200 个应用程序，而 iPhone 推出五年后拥有 553,000 个应用程序，而这只是其中的一小部分。结果，Meta 仅售出了 2700 万部 Quest，仅占苹果发布五年后累计售出 1.46 亿部 iPhone 的 18%。\n",
      "\n",
      "Anyway ,这些决定冒险一试的人愿意体验苹果的愿景是如何落地的，你呢？\n",
      "\n",
      "参考链接：\n",
      "\n",
      "\n",
      "\n",
      "https://www.theverge.com/2024/2/5/24062425/youtube-vision-pro-app-360-vr-video \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-5\n",
      "title= 大语言模型加速材料发现，普林斯顿大学团队利用 LLM 准确预测晶体特性\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 编辑 | X\n",
      "\n",
      "晶体特性的预测在晶体设计过程中起着至关重要的作用。当前预测晶体特性的方法主要集中于使用图神经网络（GNN）对晶体结构进行建模。尽管 GNN 很强大，但准确模拟晶体内原子和分子之间的复杂相互作用仍然是一个挑战。\n",
      "\n",
      "文本数据提供了丰富的信息和表现力，但从晶体文本描述预测晶体特性的研究还不够。主要原因之一是缺乏该任务的公开数据。\n",
      "\n",
      "普林斯顿大学的研究人员创建了一种 AI 工具来预测晶体材料的行为。新方法依赖于大型语言模型（LLM）。通过综合文本描述中的信息（包括原子之间键的长度和角度以及电子和光学特性的测量等细节），新方法可以比现有模拟更准确、更彻底地预测新材料的特性，并有可能加快设计和测试新技术的过程。\n",
      "\n",
      "研究人员开发并公开了一个基准数据集（称为 TextEdge），其中包含来自 Materials Project 的 140,000 多个晶体的描述，然后，提出了 LLM-Prop，一种利用 LLM 的通用学习能力从文本描述中预测晶体的物理和电子特性的方法。\n",
      "\n",
      "研究人员测试了该工具预测先前研究的晶体结构（从普通食盐到硅半导体）特性的能力。已经证明了 LLM-Prop 预测能力，正在努力将该工具应用于新晶体材料的设计。\n",
      "\n",
      "论文一作、普林斯顿大学计算机科学助理教授 Adji Bousso Dieng 表示，「该方法代表了一个新的基准，可以帮助加速材料的广泛应用。我们是第一个使用大型语言模型来解决这个问题的团队。」\n",
      "\n",
      "该方法于 2023 年 11 月 29 日，在波士顿举行的 the Materials Research Society's Fall Meeting 上提出。\n",
      "\n",
      "相关研究以「LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions」为题，发布到 arXiv 预印平台。\n",
      "\n",
      "GitHub 地址：https://github.com/vertaix/LLM-Prop\n",
      "\n",
      "论文链接：https://doi.org/10.48550/arXiv.2310.14029\n",
      "\n",
      "现有的基于人工智能的晶体特性预测工具依赖于图神经网络的方法，但这些方法的计算能力有限，无法充分捕捉晶体中原子之间的几何形状和键长的细微差别，以及由这些结构产生的电子和光学性质。\n",
      "\n",
      "「我们在计算机视觉和自然语言方面取得了巨大进步，」Dieng 说，「但在处理 AI 图方面，我们还不是很先进。所以，我想从图转移到我们已经有了很好的工具的领域。如果我们有文本，那么我们就可以在文本上利用所有这些强大的大型语言模型。」\n",
      "\n",
      "该研究的合著者、普林斯顿大学机械与航空航天工程教授兼负责创新的副院长 Craig Arnold 表示，基于语言模型的方法「为我们提供了一种全新的方式来看待材料设计问题。这实际上是关于，我如何获取人类已经开发的所有这些知识，以及如何处理这些知识以向前发展？它与我们当前的方法有本质上的不同，我认为这赋予了它很大的力量。」\n",
      "\n",
      "研究的主要贡献概述如下：\n",
      "\n",
      "研究人员收集、整理并公开一个基准数据集，其中包含大约 144K 晶体文本描述及其属性。\n",
      "\n",
      "提出 LLM-Prop，这是一种高效微调的网络，使其能够在晶体特性预测方面实现最先进的性能，优于当前最好的基于 GNN 的晶体特性预测器。\n",
      "\n",
      "表 1：来自收集的基准数据集的示例。（来源：论文）\n",
      "\n",
      "数据包含 144, 931 个晶体，将其分为 125, 098 个晶体用于训练，9,945 个晶体作为验证集，9,888 个晶体作为测试集。对于每个晶体，收集其 ID、结构信息、带隙、体积以及其带隙是直接还是间接的。使用 Robocrystallographer 提取了晶体文本描述。\n",
      "\n",
      "LLM-Prop，是一个源自 T5 的精心微调的网络，用于晶体特性预测。通过大量实验证明，LLM-Prop 在预测晶体固体的物理和电子特性方面实现了卓越的性能，超越了当前最先进且使用广泛的基于 GNN 的架构（例如 ALIGNN）。\n",
      "\n",
      "图 1：LLM-Prop 架构。（来源：论文）\n",
      "\n",
      "LLM-Prop 在所有任务上都能产生更好或相当的性能，包括 zero-shot 预测。尽管超参数少了 3 倍，LLM-Prop 的性能也优于经过微调的 MatBERT（一种特定领域的预训练 BERT 模型）。\n",
      "\n",
      "LLM-Prop 在回归和分类任务上都优于所有基于 GNN 的基线。\n",
      "\n",
      "表 2：与带隙预测基线的性能 (MAE) 比较。（来源：论文）\n",
      "\n",
      "对于带隙预测，LLM-Prop 在验证集和测试集上均优于性能最佳的基线 (ALIGNN)，分别提高了约 8% 和 4%。\n",
      "\n",
      "表 3：性能 (MAE) 与体积预测基线的比较。（来源：论文）\n",
      "\n",
      "对于体积预测，LLM-Prop 还比验证集和测试集上的最佳性能基线 (ALIGNN) 分别提高了约 67% 和 66%。这种改进的可能原因可能是，与 GNN 相比，LLM-Prop 可以很容易地从文本描述中获取最重要的体积预测信息。\n",
      "\n",
      "研究结果凸显了基于文本的方法在材料科学中的巨大潜力，基准文本数据 TextEdge 将助力这一新兴领域的研究。\n",
      "\n",
      "参考内容：https://techxplore.com/news/2024-01-harness-large-language-materials-discovery.html \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-4\n",
      "title= 夸克大模型应用为先加持夸克网盘深挖相册使用场景\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 2024年将是大模型应用落地的爆发年，这已经成为业界共识。夸克大模型自去年11月份发布以来，结合自身业务小步快跑，在夸克App上已经落地了多个应用。最近，夸克网盘结合春节场景和大模型技术，升级几项图片处理智能工具。\n",
      "\n",
      "夸克网盘即将上线的“春节图片故事”，是为用户春节期间拍摄上传的图片自动智能筛选生成合辑。该功能除了基于时间、地点两个维度筛选，还会基于人物智能筛选，并剔除掉过亮或过暗等不符合要求的图片。夸克网盘还会利用AI算法为图片合辑智能生成文案，比如鲜花影集的文案是“花与美妙人间”。\n",
      "\n",
      "此前，AI技术还被应用在夸克网盘相册中的智能查找和智能分类上。夸克网盘利用AI人脸理解技术，将不同人像图片分类存放，通过头像就能找到对应人物的图片。夸克网盘还支持将图片根据地点分类，并且根据拍摄的具体地点、人物、时间和图片特征等进行更进一步的归类。\n",
      "\n",
      "支持 AI 自然语言搜索功能，夸克网盘用户通过模糊词、形容词等关键信息，就能快速找到图片。例如搜索“睡觉的猫”，夸克网盘会列出符合条件的图片。对比之下，同样关键词在手机本地相册搜索只会得到“无结果”提示。\n",
      "\n",
      "在夸克网盘（左）和苹果手机照片应用（右）中搜索“睡觉的猫”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "据介绍，AI 自然语言搜索功能背后是夸克视觉自监督大模型和图文多模态大模型在发挥作用，核心原理是将搜索词利用大模型技术生成一串特征向量用于和图片对比，然后再通过直接计算空间距离来召回符合的图片。这种向量编码通过分析全网海量图像信息数据，进行AI的训练学习。这意味着该功能不会侵犯用户的隐私数据，就能实现智能搜图。\n",
      "\n",
      "夸克大模型正在用技术不断升级改善相册功能的使用体验，用户不仅可以在夸克网盘上智能搜图、智能找图，还可以使用图片编辑、夸克快传等功能快速编辑和分享图片。\n",
      "\n",
      "这符合夸克大模型在垂直领域的发力的定位。夸克网盘技术负责人表示，夸克大模型是面向搜索、生产力工具和资产管理助手的应用型大模型。夸克网盘围绕相册功能开发的智能工具，便是夸克大模型在资产管理助手上的能力体现。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-3\n",
      "title= 上海街头偶遇未来科技！机器狗和外骨骼机器人都来为2024 GDC造势\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 一年一度的全球开发者先锋大会（GDC）即将在上海掀起科技风暴！！！这不仅仅是一场大会，而是所有代码高手、科技狂热者们的盛大节日！！！\n",
      "\n",
      "GDC缘起总理在达沃斯世界经济论坛打call的WAIC世界人工智能大会，作为WAIC聚焦科技和人才力量的重要板块，进化到如今的全球开发者嘉年华，已化身顶尖技术风向标，汇集全球顶尖开发者、科技先锋、企业家和学术翘楚，开启一场科技交流狂欢盛典。\n",
      "\n",
      "2024 GDC 将在上海徐汇滨江召开，这里是科技与文化交汇的前沿阵地，为大会带来无限活力与创新灵感。大会部分同期活动也会在临港等地举行。临港是2023 GAIDC的举办地，有多项重要成果发布，引起各方关注和赞誉。今年我们将开发者的范畴从AI扩展到整个技术领域，从GAIDC到GDC，更是思维的飞跃，让“多元共生”不再是想象。\n",
      "\n",
      "2024 GDC 主题是“开发者的‘模’力之都”。今年大会全面升级，1场开幕式、5场前沿技术讲坛、10+场平行技术讲坛，及X场工作坊、10000 m2互动体验、竞技场、创客集市、场外活动等，聚焦大模型、人形机器人、开源开放、AIGC前沿话题，一网打尽科技界的最新动态。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-06-2\n",
      "title= 「天工2.0」MoE大模型发布——「天工AI」国内首个MoE架构免费向C端用户开放的大语言模型应用全新问世\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 北京时间2月6日，昆仑万维正式发布新版MoE大语言模型「天工2.0」与新版「天工AI智能助手」APP，这是国内首个搭载MoE架构并面向全体C端用户免费开放的千亿级参数大语言模型AI应用。用户即日起可在各手机应用市场下载「天工AI智能助手」APP，体验昆仑万维「天工2.0」MoE大模型的卓越性能。\n",
      "\n",
      "「天工2.0」是昆仑万维自去年4月发布双千亿级大语言模型「天工」以来的最大规模版本升级，其采用业内顶尖的MoE专家混合模型架构，应对复杂任务能力更强、模型响应速度更快、训练及推理效率更高、可扩展性更强。\n",
      "\n",
      "此次更新全面升级了AI搜索、对话、阅读、创作的回答质量与响应速度，搭载强大的多模态能力，支持图文对话、文生图等多模态应用，支持最高100K的超长上下文窗口（超过15万个汉字），并新增了AI绘画、数据分析、AI伴侣、AI算命、热梗百科等多项新兴玩法，让AI更聪明、更实用、更有趣，成为每个人日常生活中的全能AI小助手。\n",
      "\n",
      "昆仑万维致力于人工智能模型算法的创新与开拓，不断探索通用人工智能技术前沿。除了双千亿级大语言模型「天工」、MoE专家混合大模型「天工2.0」外，昆仑万维还围绕「天工」系列大模型，推出了百亿级开源大语言模型系列「天工Skywork-13B」、AI Agent开发平台「天工SkyAgents」、多模态大语言模型「天工Skywork-MM」等前沿AI产品，并已逐步构建起AI大模型、AI搜索、AI音乐、AI Story、AI游戏等AI业务矩阵，是国内模型技术与工程能力最强、布局最全面的人工智能大模型企业之一。\n",
      "\n",
      "MoE：全球顶尖的大模型核心技术路径\n",
      "\n",
      "MoE（Mixture-of-Experts，专家混合模型）是当前大语言模型赛道技术最顶尖、研发最前沿的底层架构，是全球最领先的大模型核心技术路径之一。\n",
      "\n",
      "自2023年6月以来，昆仑万维不断针对MoE架构技术最前沿进行研发探索，并成功发布国内首个搭载MoE架构并面向全体C端用户免费开放的千亿级参数大语言模型AI应用——「天工AI智能助手」APP。\n",
      "\n",
      "「天工AI智能助手」以昆仑万维「天工2.0」MoE大模型为核心技术引擎，其技术原理是将复杂的大模型任务拆解为多个更小、更细分的子任务，每个子任务都由垂直领域的专家模型处理，从而使得昆仑万维「天工2.0」不仅大幅提高了模型训练与推理的性能和效率，更能实现多个垂直领域的知识融合，使模型能够更好地理解和处理不同应用场景下的复杂问题，为用户提供更准确、更全面的回答方案。\n",
      "\n",
      "同时，昆仑万维技术团队更是通过一系列针对性的MoE技术攻关，在投入大量研发训练资源后，最终解决了困扰整个MoE产业的模型不收敛、特定任务泛化效果较差等核心性能问题，使「天工2.0」的模型性能得到显著提升。\n",
      "\n",
      "模型性能更强、速度更快、架构更灵活\n",
      "\n",
      "「天工2.0」的技术领先性体现在其核心MoE架构的卓越优势。MoE架构主要由门控模型/路由器（Gating Model/Router）和一组专家模型（Experts Models）构成，当数据输入门控模型/路由器时，系统会根据任务类型将每个token分配给一个或多个专家模型，使得每个专家模型可以专注于处理该部分数据，从而获得模型性能的整体提升。\n",
      "\n",
      "较之传统大模型架构，「天工2.0」具有以下优势：\n",
      "\n",
      "1.应对复杂任务能力更强：「天工2.0」MoE模型集成了多个专家模型，每个专家模型都能针对不同的数据分布和构建模式进行搭建，从而显著提升大模型在各个细分领域的专业能力，整体模型通过整合各自专家模型的输出结果，使得「天工2.0」在处理复杂任务、多模态任务时拥有显著性能提升。\n",
      "\n",
      "2.速度更快、效率更高：由于MoE模型推理计算过程中只有少数特定专家模型被激活，相较于同等参数规模的稠密模型，「天工2.0」MoE模型呈现出极高的稀疏性，使其拥有更高的推理计算效率，从而让用户获得更快的AI响应速度。\n",
      "\n",
      "3.灵活、多样、可扩展性更强：一方面，模型稀疏性使得「天工2.0」能够在不增加计算量的前提下显著扩张模型规模，在同等计算资源下获得更强的模型性能；另一方面，通过增加专家模型数量、调整专家模型的权重配比，「天工2.0」能够极大丰富模型的可扩展性，构建更为灵活、多样、可扩展性更强的新时代大模型。\n",
      "\n",
      "「天工AI智能助手」APP全面升级\n",
      "\n",
      "「天工AI智能助手」APP基于昆仑万维自研「天工」系列大模型打造，是一款能搜、能聊、能写、能画的AI智能助手，其拥有强大的自然语言处理和智能交互能力，能够实现个性化AI搜索、智能问答、AI绘画、聊天互动、文本生成、编写代码、语言翻译等多种应用场景，并且具有丰富的知识储备。\n",
      "\n",
      "伴随着「天工2.0」大模型的重磅升级，「天工AI智能助手」也迎来了版本的全面更新。\n",
      "\n",
      "1.强大的多模态能力：「天工AI智能助手」所采用的多模态大模型基于一体化的开发策略，在底座模型的基础上进行深入开发与优化，引入多分辨率的视觉编码器和强大的语言基座模型，使其能够支持任意尺寸的图片输入和复杂的用户指令。\n",
      "\n",
      "在强大的多模态大模型能力加持下，新版「天工AI智能助手」具备优秀的视觉理解、推理和指令遵循能力，能够满足图文对话、图文创作、知识问答等多种用户需求。同时，得益于模型杰出的理解能力，新版「天工AI智能助手」生成的图像在内容丰富度、精细度和图像质量上均表现卓越。\n",
      "\n",
      "与此同时，在强大的多模态能力加持下，「天工AI智能助手」还能生成图文并茂的答案内容，让用户问出“螺蛳粉怎么做？”“怎么用吉他弹《稻香》？”“2024春节放假安排？”这类问题时能够得到图像/视频辅助呈现，使得AI回答的结果更直观，内容更丰富。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.支持100K超长上下文窗口：「天工AI智能助手」的超长上下文窗口技术基于100K原生文本进行训练，能够支持最高100K（超过15万汉字）的文本对话，并能够通过扩展技术可以支持200K超长文档理解。在InfiniteBench评测中，「天工」系列大模型多项指标全球第一，10项指标平均分47.5分，超过Claude2，接近GPT4-128k的52.6分。\n",
      "\n",
      "在针对超长上下文模型的“大海捞针”测试中，研究人员会在海量的文档集里面插入特定信息，然后对文档集进行提问，期待模型能从“茫茫文海”中找出正确的关键信息，以验证模型的长上下信息提取能力。在“大海捞针”测试中，「天工」模型取得了100%正确结果。\n",
      "\n",
      "3.搜得更准、写得更好、读得更快：新版「天工AI智能助手」拥有更强大的关键词与语义分析能力更精准识别用户任务需求，在AI搜索、对话、阅读、创作等不同应用场景中，都能针对用户的不同需求提供更准确、更具体的回答与追问建议。同时，新版「天工AI智能助手」AI搜索质量、安全能力、答案丰富程度都进一步提高。\n",
      "\n",
      "例如，在「天工AI智能助手」中，用户可使用“AI阅读”功能快速提炼总结文献内容，并针对文献内容细节进行追问，得到快速、准确、具体的答案内容。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.丰富有趣的AI Agent：新版「天工AI智能助手」新增了如AI绘画、数据分析、AI伴侣、AI算命、热梗百科等多款官方AI Agent，让「天工AI智能助手」在能搜、能聊、能写之余，新增更多有趣而实用新兴玩法，不断探索AIGC技术的应用边界，成为每个人日常生活中必不可少的全能AI小助手。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从2月8日开始，由昆仑万维主办的“巧绘龙年”AI绘画大赛也将在「天工AI智能助手」APP内开启，用户使用APP内“AI绘画”功能绘制图画作品并投稿至活动专区，即可有机会获得最高10万元人民币的现金大奖。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这是国内首个面向全体C端用户免费开放、奖金规模达到数十万量级的AI绘画大赛。得益于「天工」系列大模型卓越的多模态技术能力，高水准的文字意图识别确保用户能够尽情发挥创意，绘制出内容丰富、细节精致、审美高级的个性化AIGC图像。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "一直以来，昆仑万维始终秉承着“实现通用人工智能，让每个人更好地塑造和表达自我”的公司使命，不断降低大模型技术在各行各业的应用和学习门槛，携手探索未知世界、共创科技未来。\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-05-12\n",
      "title= 可多模态数据集成、插补和跨模态生成，中科院&树兰医院&北师大团队开发带有掩码模块的深度生成框架\n",
      "author= []\n",
      "publish_date= 2024-02-05 00:00:00\n",
      "text= 编辑 | 红菜苔\n",
      "\n",
      "随着单细胞技术的发展，许多细胞特性可以被测量。此外，多组学分析技术可以同时联合测量单个细胞中的两个或多个特征。为了快速处理积累的各种数据，需要多模态数据集成的计算方法。\n",
      "\n",
      "树兰医院、中国科学院和北京师范大学的合作团队提出了 inClust+，一个用于多组学分析的深度生成框架。它建立在之前针对转录组数据所开发的 inClust 的基础上，并增加了两个专为多模式数据处理设计的掩码模块：编码器前面的输入掩码模块和解码器后面的输出掩码模块。\n",
      "\n",
      "InClust+ 可用于整合来自相似细胞群的 scRNA-seq 和 MERFISH 数据，并根据 scRNA-seq 数据估算 MERFISH 数据。InClust+ 具有将多模态数据（例如具有基因表达、染色质可及性和蛋白质丰度的三模态数据）与批次效应整合的能力。\n",
      "\n",
      "研究人员使用 inClust+ 整合一个未标记的单模态 scRNA-seq 数据集和两个标记的多模态 CITE-seq 数据集，将标签从 CITE-seq 数据集转移到 scRNA-seq 数据集，并生成单模态 scRNA-seq 数据中缺失的蛋白质丰度模态。\n",
      "\n",
      "该研究以「InClust+: the deep generative framework with mask modules for multimodal data integration, imputation, and cross-modal generation」为题，于 2024 年 1 月 24 日发布在《BMC Bioinformatics》。\n",
      "\n",
      "近年来，单细胞技术的进步使得在单个细胞中获得多种性状成为可能，例如单细胞 RNA 测序 (scRNA-seq)、转座酶可及染色质测序的单细胞测定 (scATAC-seq) 和单细胞亚硫酸氢盐测序 (scBS-seq)。\n",
      "\n",
      "这些单细胞方法极大地促进了科学家对细胞的理解。从而揭示细胞群的异质性，推断细胞发育轨迹，并重建基因调控网络。但以一种方式收集的数据仅代表细胞状态的有限侧面。为了获得更全面、更全面的信息，需要将来自不同模态的数据整合在一起，从而更好地揭示数据的生物学意义。\n",
      "\n",
      "为了完成这些任务，树兰医院、中国科学院和北京师范大学的合作团队在之前的研究中，曾提出了 inClust（集成聚类），一种灵活的转录组数据深度生成框架。在这里，该团队通过添加两个新模块来扩展 inClust，即编码器前面的输入掩码模块和解码器后面的输出掩码模块。\n",
      "\n",
      "图示：inClust+的架构及其应用。（来源：论文）\n",
      "\n",
      "该团队将增强的 inClust 命名为 inClust+，并证明它不仅可以完成数据集成，还可以利用掩模模块的优点完成基因插补。\n",
      "\n",
      "研究人员将 inClust+ 应用于各种数据集，包括多个单模态（未配对）数据集、一个或多个多模态数据集以及包含多模态数据和单模态数据的数据集。在这些例子中，inClust+展示了其数据集成、插补和数据生成的能力。\n",
      "\n",
      "首先，通过 mask 模块的优点，参考类似细胞群的 scRNA-seq 数据，使用 inClust+ 对 MERFISH 数据进行插补。\n",
      "\n",
      "然后，通过三个示例评估了具有堆叠式编码器-解码器架构和掩模模块的 inClust+ 的多模态集成能力。结果表明，inClust+ 不仅可以混合模态之间的数据，还可以分离生物学差异并消除批次效应。\n",
      "\n",
      "最后，研究人员使用 inClust+ 将数据与单模态数据集和多模态数据集进行集成。结果表明，inClust+ 可以将标签从多模态数据转移到单模态数据，并补全单模态数据中缺失的模态。\n",
      "\n",
      "图示：inClust+ 整合多模态（三重）数据集的图表。（来源：论文）\n",
      "\n",
      "InClust+ 的应用并不限于上述情况。对于基因插补，会出现一种情况，即所有数据集都有自己的特定基因，而不是只有一个数据集有自己独特的基因。通过调整输出掩码，inClust+ 可以基于共享基因整合两个数据集，并通过引用相应数据集中的特定基因来估算两个数据集中的其余基因。对于缺失模态生成，会出现所有数据集都有自己特定模态的情况，inClust+ 可以基于共享模态整合两个数据集，并通过引用相应数据集中的特定模态来生成每个数据集中的缺失模态。\n",
      "\n",
      "由于inClust+ 是 inClust 在多模态应用中的扩展，因此与其他集成方法相比，inClust+ 和 inClust 可以作为一个整体放在一起。该团队的模型（inClust 和 inClust +）与其他集成方法的区别在于其适应不同情况的灵活性以及尽可能集成信息的能力。\n",
      "\n",
      "灵活性体现在以下两点：首先，InClust 可以灵活地处理标签信息；InClust+也继承了这一优点，并体现在 inClust+ 可以半监督模式将标签从参考数据集转移到查询数据集。其次，inClust+ 中的两个 mask 模块可以灵活调整以处理不同的输入。\n",
      "\n",
      "模型尽可能整合信息的能力体现在以下两点：首先，在inClust中证明该模型不仅可以使用表达数据，还可以使用协变信息（例如批次）和标签信息；这一优点也被 inClust+ 继承了。其次，如 inClust+ 所示，该模型不仅可以利用共享数据（共享基因表达或共享模态）进行整合，还可以利用特定基因或模态来进行缺失基因插补或缺失模态生成。\n",
      "\n",
      "简而言之，该团队的模型不仅可以集成数据，还可以在数据集成的基础上完成其他下游任务（例如分布外生成、标签转移和新型识别、空间域分割、跨模态插补和生成）。\n",
      "\n",
      "添加掩模是增强深度学习模型的常见方法。在 inClust+ 中，研究人员通过一对掩码模块（输入掩码模块和输出掩码模块）来增强模型。掩模的灵活设计和使用使模型能够完成一系列任务，这些任务通常需要多个模型分别完成。例如，inClust+ 可以利用常见的和数据集特定的基因进行整合和插补，如 uniPort。掩码使事情变得简单：输入掩码筛选出常见基因，输出掩码筛选出相应数据的常见基因和数据集特定基因。\n",
      "\n",
      "同时，inClust+ 可以集成多模态数据集来实现多域翻译，作为跨模态自动编码器。输入掩码和输出掩码使inClust+ 成为多个独立且相关的编码器-解码器组合。因此，inClust+ 不仅可以对同一模态的数据进行压缩和重构，还可以将一种模态的数据压缩并重构为另一种模态，从而实现跨模态翻译。\n",
      "\n",
      "此外，inClust+ 可以集成多模态数据集和单模态数据集，将标签从多模态数据转移到单模态数据，并通过数据生成将单模态数据完整地转换为多模态数据，如 sciPENN。InClust+ 指的是多模态数据集，用于生成单模态数据集中缺失模态的数据。一般来说，作为一种模型增强技术，在模型中添加一对掩模不仅限于 inClust，还可以扩展到具有类似编码器-解码器结构的深度学习模型，例如 scArches。\n",
      "\n",
      "论文链接：https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-024-05656-2 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-05-11\n",
      "title= 王长虎：PixVerse 实测效果已超过 Pika，抖音经验让我们有足够优势\n",
      "author= []\n",
      "publish_date= 2024-02-05 00:00:00\n",
      "text= 今年 4 月宣布创办爱诗科技，加入 视频生成 赛道后，王长虎就消失在舆论场中了。他在抖音的职业经历，让爱诗科技在 视频生成 的牌桌上拥有一席重要位置。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2017 年，王长虎加入 字节跳动 开始担任 AI Lab 总监，在这个岗位上，他为抖音和 Tiktok 从 0-1 构建了视频AI能力。用王长虎本人的话说，为抖音所做的工作，让他的团队涉猎了几乎所有与视频智能相关的领域，包括且不限于数据处理、内容生成、安全问题处理、视频内容精准理解以及全方位广告场景。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "近期，王长虎接受了 机器之心 的独家专访。在采访中，王长虎详细介绍了抖音的视频智能化经验是如何被他复用到 视频生成 领域的，所积累的这些经验为他的新公司构建了数据、算法以及工程上的竞争优势。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "视频生成 工具 PixVerse，能够生成免费 4K 分辨率的高清视频，在光影细节和运动准确性等方面取得了进展。王长虎告诉 机器之心 ，PixVerse 的性能在某些方面已经达到了 Pika 的水平，甚至在多项评测中超越了它们 爱诗科技在近期上线的工具 PixVerse，能够生成免费 4K 分辨率的高清视频，在光影细节和运动准确性等方面取得了进展。\n",
      "\n",
      "自媒体 KOL 歸藏在一次对比评测中，从物品特写、写实风景、写实人像、皮克斯 2.5D 风格、 2D 动画风格五种风格对 PixVerse、Pika 以及 Runway 三大模型进行比较，为这三者评分 74.5 分、 73.5 分、 64.5 分，PixVerse 位居第一。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "歸藏认为，“PixVerse 的模型是这三者最为平衡的，可以有比较强的运动幅度，同时可以维持较好的一致性。Pika 在动漫和 2.5D 风格上的优势巨大，但图像质量以及一致性相对差一些。\n",
      "\n",
      "王长虎认为，目前 视频生成 领域存在的两个最关键的技术问题是准确性和一致性，而在这两个核心维度上，Pika 和 Runway 还有提升空间。“在目前的实测中，PixVerse 欢迎投资人和同行以及用户来进行随机大样本量的对比，对比越多，越能发现我们的优势”。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎表示， 视频生成 技术上的累进和商业化的运行已经可以同步开展。爱诗科技内部正在大量孵化基于 视频生成 技术的轻量应用，这些应用将面向使用抖音、快手等短视频平台的 C 端视频消费者，借助这些应用扩充产品影响力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“产品效果超越 Pika”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：能否请您介绍下目前公司最新的情况？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们核心团队成员从 2017 年开始参与抖音从零到一的发展，负责抖音背后的视频 人工智能 能力构建，在视频 AI 领域积累了很多独特的实战经验。随着 AI 时代的到来，我们认识到 AI 视频生成 的巨大潜力，而我们的经验让我们有信心（比别人）做得更快更好。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2023 年 4 月份，我们获得首轮数千万人民币的融资，6 月份核心团队基本成型。我们只用了 3-4 个月的时间就实现了重大进展，在某些方面超越了全球最大的竞争对手像 Pika 这样的公司。之前在抖音积累的视频处理经验，被成功应用在目前我们的 AI 视频生成 项目上。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "整体上，我们的产品发展分为两个阶段：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "第一阶段：TO 创作者，提供更好的 视频生成 服务，更好地理解创作者动机。同时，也支持直接面向用户，接受用户反馈进行迭代。我们目前已经推出的产品 PixVerse，用户已经可以在网页端和 Discord 社区使用，利用文字或者图片生成 4K 高清视频。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在第二阶段，我们希望直接面向消费者，不仅仅是提供工具，而是要打通创作和消费的整个流程，直接提供 AI 原生的可消费内容。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：横向对比，目前，PixVerse 在哪些方面做得比较好？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：目前，我们认为在 视频生成 领域最关键的两个问题是准确性和一致性。准确性要求每一帧都能精确地反映用户需求，一致性要求在时间轴上，视频中物体的运动符合客观规律。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "只有在这两方面上实现进步，让 AI 生成的视频准确反映用户需求，并且保证动态内容符合规律，运动具有连贯性，这样的视频才能应用于实际场景。就目前而言，我们发现在这两个核心维度上，Pika 和 Runway 各有明显的不足。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "目前，在这两方面，我们已经取得了重大进展。我们已经可以生成 4K 高清的动态视频，并且在可用性上实现了提升。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "举个例子，这是我们海外的一些创作者所实现的效果，一位创作者利用 PixVerse 制作的宣传片，其中每个素材都运用了我们的技术。 此外，我们还能制作一些基于电影、游戏素材的创新场景，比如钢铁侠在黄浦江游泳、让《原神》角色在其他游戏场景里跳舞等等。\n",
      "\n",
      "机器之心 ：你提到说在效果上已经 “超过了 Pika 和 Runway”，这个标准是什么？我们可以怎么感受到？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们可以用同一个 Prompt，对比一下 PixVerse 和 Pika 1.0、Runway 的效果。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如，柯基跳舞的 Prompt（a corgi is dancing_一只柯基在跳舞）\n",
      "\n",
      "在 Pika 1.0 的表现里，柯基主体非常精确且吸引人，但是它只进行了微小幅度的运动。观察它的画面，虽然每一帧单独看起来都不错，但当它们连在一起时，就不再呈现出视频的信息量。而Runway在柯基的表现上很好，但是基本没有跳舞的动作。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这本质上，是刚才我提到的 “运动一致性” 的问题，因为现在对于要让一个物体在时间轴上去做运动，本身是一个非常难的技术。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "总之，整个行业在模型 视频生成 方面面临的最基础问题，就是 准确率 和运动一致性。如果我们制作的视频素材既不准确又缺乏一致性，就无法在任何场景中有效使用。因此，我认为这是全球这个行业首要解决的问题。在这方面，我们技术上可能走在了前列。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我们欢迎对我们的模型进行实时测试，事实上，测试的案例越多，我们的优势就越明显。现在 视频生成 领域还没有形成统一的竞争格局，我们认为在这个方向上，我们有机会在全球范围内取得领先地位。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你们内部 视频生成 内容评价的标准是什么？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：目前我们内部已有一个评估标准，可以用于评估 视频生成 产品准确性和一致性。目前整个行业缺乏一个明确的判断标准，所以我们也在不断完善过程中，未来可能会发布出来。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我们将评估标准分为三个部分：主体动作风格、一致性（包括主体和背景），以及主体运动的合理性。我们还考虑了运镜技巧、创新瓶颈，以及丰富性，后者主要涉及画质和帧率。这些都是比较客观的维度。我们还评估信息量，即单位时间内的信息量。很多同行在研发时缺乏这样的 逻辑 。我们有一套体系来支持我们的迭代进程。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在效果评定上，除了主观与客观的标准，我们还采用盲测的方法做测试。向多个模型输入随机 Prompt，抹去水印，让足够样本的人做效果排序，来判断谁更优秀。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你认为这种评估方式相对客观吗？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：是的，这种方法相对客观。虽然图片生成和 视频生成 的效果判断比较主观，但我们之所以能在市场上迅速崛起，是因为我们使用的模型和整个系统支持我们从数据角度快速作出评估。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你们最近进行的盲测评估结果如何？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：根据我们最近的评估结果，我们的性能在某些方面已经达到了 Pika 平台的水平，甚至在多项评测中超越了它们。我们的产品在视觉效果、分辨率、画质上明显优于竞品。此外，在模型准确性、一致性和丰富性方面，我们的表现也更好。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "通常情况下，如果有投资人或同行要进行测试，我们会建议他们出至少 20 个问题，以确保样本量足够大。我们会根据他们认为重要的方面来进行测试。在所有这些测试中，我们通常可以明显地看出我们的产品比竞品更优秀，这是肉眼可见的。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "但最终判断哪个产品更好，很多时候并不仅仅是基于技术性的因素，而是主观上的偏好。如果大家普遍认为某个产品好，那么这个产品就被视为更优秀。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "用抖音经验解决准确性与一致性问题\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：再聊聊 “准确性” 以及 “一致性” 的问题，和其他公司比，你们是怎么做到这两方面表现得更好？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：影响 视频生成 最后结果的因素有很多，但最重要的是：数据、算法和工程能力，而我们在过往经验中，这几方面都有自己的优势。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我们从 0 到 1 建立过抖音背后的视频平台能力，这里面包括了数据处理、内容生成、安全问题处理、对视频内容的精准理解甚至全方位的广告场景，几乎所有与视频相关领域我们都有所涉猎。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "数据层面，我们的关键能力是，能够从海量数据中筛选出一小部分高质量数据来训练更优秀的模型，并且在安全问题上足够有经验。在抖音和 TikTok，每天都有海量视频上传，我们需要利用 AI 技术有效地整合和剔除低质量和重复性内容，并且防止用户生成不适当内容。处理这些问题的经验，让我们能够用更少的整体数据量训练模型，同时降低模型大小和 GPU 资源。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "算法层面，我们在多模态对齐、视频特征表示、时空建模以及主体控制上都有自己的创新。在多模态建模上，我们进行了大量 自监督学习 ，更充分利用动作型数据，特别是在处理未标注的视频数据方面，我们尝试了多种方法来建模那些标注噪声较大的数据集，这些尝试直接帮助我们解决动态建模的问题。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "特征表示上，我们在文字和视频内容的向 量化 做了很多尝试。时空建模方面，我们努力在训练过程中生成局部内容，同时让模型能够把握整体视野。生成中间某一帧时，模型应能够记住之前和之后的内容。在最优关键帧选择和动作建模质量上取得平衡。主体控制上，我们在关键帧生成、视频内容分割等方进行优化，帮助我们对视频性能控制更精准。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "工程方面，我们参考了之前在抖音操盘上万块 GPU 的经验，帮助在大规模集群训练和推理时的稳定性提升，并且复用了自动化的能力去应对数据分布变化问题。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：基于 “数据、算法和工程” 这三个要素，你认为你们实现了 “用更少资源取得了更优效果” 的成就，有没有具体数字可以说明这一点？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们的研发效率极高、迭代速度极快，Runway 成立了 5 年多时间，融资几亿美金，Pika 成立了近一年，融资大几千万美金。我们正式训练模型是在 2023 年下半年，花了 3 个月左右的时间就做到了全球第一梯队的水平，资源资金的消耗比 Runway、Pika 至少小了一个数量级。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "未来计划通过轻量产品吸引用户\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：目前 PixVerse 的策略是通过加速技术进步来取得优势，还是更多侧重于提高市场曝光度？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们目前观察到，像用户用 Pika、Runway 这些平台制作的视频在 YouTube 或 TikTok 等主流社交媒体上并没有太多播放量，很多 AI 视频生成 厂商目前的受众更多在服务一小部分 AI 发烧友。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "但我们的目标是希望技术能去找到具体的消费场景，满足实际需求，这里面需要用户对我们的技术信心，所以我们需要展示我们的技术能力，让用户愿意去使用。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你的意思是你们计划首先找到一个适合你们平台的应用场景，然后与创作者合作，优化这个场景，并通过这种方式吸引用户吗？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们首先确定的是，我们的平台不仅会提供技术，还会推出产品。技术只是起点，我们要解决的核心问题是如何利用这些技术创造的内容。我们已经有一些思路了。比如，帮创作者用《原神》中的人物进行高质量的二次创作。类似的场景尝试内部还有很多，我们在积极尝试，这部分产品主要面向 C 端用户。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你认为当前整体的 视频生成 赛道竞争局势如何？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：现在的竞争虽然已经开始，但真正激烈的阶段还未到来。我们发现目前大部分的用户只是停留在了解 AI 视频产品的阶段，并没有真正在使用产品去创作，这表明市场的增量仍然很大。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "不过，我们认为并非只有在技术完全成熟时才有商业化机会。即便我们目前的技术仅支持生成数秒的视频，但已经有用户在此基础上做出了大片级的作品。在这个阶段，我们正考虑哪些特性能更广泛地吸引 C 端消费者，使他们觉得产品既有趣又实用，并愿意去传播。这部分工作是目前我们的战略核心。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你们对公司半年或者一年后的预期是什么？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：在未来 6-12 个月里，我们希望用 AI 制作出 15 秒长的可消费短视频。实际上，抖音刚开始时就是从 15 秒的视频开始的，所以我认为这样的长度足以承载丰富的信息供用户消费。我们希望这些内容是由 AI 生成的，同时也是用户感兴趣、愿意传播和浏览的。这些内容可能是单镜头拍摄，也可能是多个镜头组合的，但都能讲述故事并承载信息。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "未来，我们希望实现视频的实时秒级生成。我相信一旦做到这一点，将会对整个内容行业、视频行业带来巨大的颠覆。因为我们目前想到的都是存量的场景，而这将是一个全新的物种，带来许多增量的新体验和玩法，这些都是我们和同行未来需要一起探索和理解的。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：要达到这个 6-12 个月的目标，公司还需要哪些方面的进步？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：首先是视频生产的基础能力，继续提升准确性和一致性，目前的技术仍然存在一些瑕疵，我们希望继续改进。另外，我们希望能支持生成更长时间的视频。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：视频内容如果实现秒级实时生成了，可能会发生什么？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：现在人们消费视频的方式是在电影院观看相同的电影，或者在网上观看相同的剧集。但是 AI 视频生成 技术意味着未来我们可能实现秒级甚至实时的 视频生成 。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这种实时生成允许我们在视频播放时改变其某些元素，比如让观众成为视频中的主角，并且可以实时变化。这使得每个观看者都能与视频互动，参与到视频的发展过程中，每个人看到的内容都是不同的。这种技术能够理解每个人的喜好，并根据这些喜好定制化视频内容，就像创造一个平行宇宙一样。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "未来，我们获取信息的方式可能会变为推荐加生成结合的方式，每个人看到的视觉内容都会不一样。由于互联网上的信息已经高度视频化，这个领域的未来想象空间非常大，但这需要逐步实现，从一个模型应用开始，慢慢发展到更远大的目标。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-02-5\n",
      "title= 年龄两岁，教龄一年半：婴儿AI训练师登上Science\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= 只用 61 个小时的数据：人们终于证明了，利用当代 AI 工具，实现「真正的 语言学 习」是可行的。\n",
      "\n",
      "在公开采访中，图灵奖得主 Yann LeCun 多次提到，现在的 AI 模型和人类婴儿相比，学习效率实在是太低了。那么，如果让一个 AI 模型去学习婴儿头戴摄像头拍到的东西，它能学到什么？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最近，Science 杂志上的一篇论文进行了初步尝试。研究发现，即使数据有限，AI 模型也能从 10 到 100 个例子中学到单词 - 视觉所指对象之间的 映射 ，而且能够零样本地泛化到新的视觉数据集，并实现多模态对齐。这说明，利用当今的 人工智能 工具，从婴儿的视角进行真正的 语言学 习是可能的。\n",
      "\n",
      "年龄两岁，教龄 1 年半\n",
      "\n",
      "Sam 是怎么教 AI 学习的？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这一次， 人工智能 通过婴儿的视角看世界来学习语言。\n",
      "\n",
      "神经网络 通过人类婴儿的视觉经验，自行学会了识别物体，这为人类学习提供了新的见解。\n",
      "\n",
      "AI 通过 Sam 佩戴的头盔式摄像机所拍摄的音视频学习。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "当婴儿听到「球」这个词时，他们是如何将这个词的语义与圆形、有弹性的物体（即正确的视觉所指对象）联系起来的呢？哲学家和认知科学家都认为，婴儿在学习新词时，需要从众多候选意项中挑出正确的那一个。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "婴儿非常擅长学习词汇。在 6 到 9 个月大的时候，他们开始将单词与眼前的物体建立起音形义的联系。到 18 到 24 个月大的时候，他们已经能理解约 300 个单词。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "那么，孩子们是如何快速学会眼前物体的名称的呢？他们又是如何建立起物体的意义和其视觉之间的联系呢？这些问题都需要进一步的探索和研究。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "此前，已有一些相关理论在实验中得到了验证。有学者认为单词学习是由简单的、能串联起各领域的联想学习机制驱动的。但是这些理论通常是在婴儿不同的成长时间段测量的，不能揭示某种促进单词学习因素的相对重要性，也不能从中构建计算模型、为计算机模型能获得像人一样的学习能力提供指导。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如果一个模型能够通过孩子的眼睛和耳朵 感知 世界，那么它是否像解释人类词汇学习能力的联想学习理论一样，能够仅通过基于物体表征的联想学习，理解并整合物体的形体和语义呢？或者，它是否需要借助其他的认知能力，比如归纳偏置（inductive biases），来启动这种能力呢？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了得到这些问题的答案，来自纽约大学的研究者们对最简单的词汇学习理论进行了前所未有的测试：他们给一个婴儿戴上了头戴式摄像机，并检查模型是否能够从这部摄像机的视频记录中学习到单词与其视觉所指对象之间的 映射 关系。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "戴上摄像机的是来自澳大利亚的 Sam，从 6 个月大到大约 2 岁，他每周头戴摄像机两小时（约占清醒时间的 1%）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究团队根据 Sam 的视频建立了 SAYCam-S 数据集。他们从中选取了 61 个小时的录像，其中包含 60 万张视频帧与 3.75 万段经过转写的录音，记录了大约 25 万个单词实例以及对应的图像。这些图像是 Sam 在玩耍、阅读和进食等活动期间拍摄的。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究团队根据这些数据来训练 神经网络 ，并得到了儿童视角对比学习模型 CVCL。CVCL 采用了对比学习的技术，以学习哪些图像和文本经常一起出现，哪些不会，从而获得预测某些词汇（如 “球” 和 “碗”）所指代图像的能力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究发现，CVCL 可以从一个孩子有限的经验片段中充分学习多模态表示。CVCL 能够将一系列日常词汇与分类任务中相应的视觉所指对象匹配起来，大规模对齐视觉和语言概念，并将此能力泛化到训练中未见过的新例子中。该研究表明，多模态表征学习与领域通用的联想学习机制相结合，能够为计算机学习单词带来突破。\n",
      "\n",
      "具体来说，研究者根据多模态模型研究的最新进展设计了 CVCL。CVCL 整合了表示学习和联想学习，用一个对比目标来协调视觉编码器和语言编码器两个 神经网络 。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如图 1 所示，对比目标以自我监督的方式进行训练（即只使用儿童视角的记录，不使用外部标注），模型将目标在视频帧和语言片段共同出现的情况转化为向量提取出来，将其视为正面例子，同时将不共同出现的转化成向量分离出来，视为隐含的负面例子。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "提取到正面例子后，CVCL 将这些时间向量转换为学习和调整多模态表征的学习信号。这种方法既不需要对词义进行限制，也不需要预先列出可能的视觉所指对象，能从婴儿记录的视频中恢复许多基本的单词与其视觉所指对象的组合。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "评估 CVCL 获得的词汇\n",
      "\n",
      "对应视觉所指对象的结果\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "训练完成后，研究团队评估了 CVCL 以及各种类似的模型学习到的单词 - 视觉所指对象组合的质量。根据一种针对儿童的常见测试，研究团队向模型提示了一个目标类别标签，让模型根据四个候选图像与标签的余弦相似度中选择相应的视觉所指对象。\n",
      "\n",
      "图 2A 显示了标签 S 的测试结果，总体而言，CVCL 的分类 准确率 为 61.6%。图 2D 显示了模型在不同标签中的具体结果，在 22 个概念中，CVCL 对 11 个概念的判断与 CLIP 相差不到 5%。但 CLIP 训练所用的数据量（互联网的 4 亿个图像文本对）远超于 CVCL。为了解决分类重叠等潜在问题，研究团队还手动筛选出了子集进行了后续评估。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了确定 CVCL 捕捉单词含义能力的上限和下限，研究团队还将其与类似模型进行了实验。为了测试模型将语言和视觉信息对应起来的能力，研究团队将原数据集中共同出现目标物体的视频帧和录音打乱，重新训练了一个模型的变体 CVCL-Shuffled。被打乱后的模型表现不佳，这显示了视觉和语言信息共现对模型学习的关键作用。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了测试视觉嵌入的有效性，研究者在训练过程中随机冻结了 CVCL 的视觉编码器。尽管模型掌握了如 「沙子 」和 「汽车 」等少数概念，但如图 2D 处所示，模型的成绩再次大幅下降（M = 38.0%）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究者比较了 CVCL 与基于其他数据或 Oracle 训练数据的 AI 模型，其他模型的训练数据超出了儿童词汇的范围。CLIP 的 准确率 达 66.7%，比 CVCL 高出 5.1%，这得益于 CLIP 更理解少数单词的含义如「厨房」、「玩具」和「篮子」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "通过以上测试，可见当在一定范围内测试时，CVCL 的性能可以与基于互联网规模数据训练的模型相当。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "此外，研究者测试了模型是否能独立对单词进行分类，而不是根据某些引导儿童的句子得出了判断。他们在初始化的预训练编码器上对 线性分类器 进行拟合得到了一个 Linear Probe 模型，新模型 准确率 达 81.6% ，说明 CVCL 具有独立判断能力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究团队 量化 了在对话中自然出现的单词相对直接标记示例对模型训练的价值。如图 2B 所示，他们使用更少的人工标注数据（使用打过标签数据的 10% 和 1%）训练了两个 Linear Probe 模型，测试结果如下表所示。\n",
      "\n",
      "减少了人工标注数据的 Linear Probe 模型，分类准确度分别下降到了 77.2% 和 65.9%。使用了 1% 的标注示例的模型性能略好于 CVCL。通过比较，可以保守估计一个人工标注的至少相当于来自自然语言的七个示例。不过，来自自然语言的数据能更加灵活、更准确地表示儿童学习的内容，并且它可以容纳无限数量的视觉概念。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了研究是否有其他因素影响了单词 - 视觉所指对象组合的可学习性，研究团队还训练了 CVCL 模型的其他变体以作评估。他们改变了模型结构或训练过程的各个方面，但没有一个变体的表现优于 CVCL 本身。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "综上所述，研究结果表明，人类最初习得的的单词-视觉所指对象组合可以从 10 到 100 个自然出现的单词-视觉所指对象组合中获得。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "泛化至全新的视觉实例\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了测试 CVCL 的泛化能力，研究团队在 Konkle Objects 数据集上进行了实验。\n",
      "\n",
      "从研究婴儿 语言学 习的实验中获得了灵感，研究团队为 CVCL 提供了 64 个额外的在白色背景上的单个物体图像，其对应的单词都在 CVCL 的词汇表中。这个实验使得研究团队能够检查 CVCL 学习的单词是否能成功泛化到未见过的物体中。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如图 3A 所示，CVCL 具有一定的泛化能力，在 64 个物体中有 16 个得分高于 50%（正确），另外 42 个概念得分高于 25%（偶然），整体 准确率 为 34.7%。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "此外，两个 CVCL 的模型变体都接近偶然 准确率 （CVCL-Shuffled 和 CVCL-Random Features 模型的 准确率 分别为 25.6% 和 23.4%），而其最佳表现都接近目前 SOTA 方法（CLIP 和 Linear Probe 模型的 准确率 分别为 99.4% 和 90.7%）。\n",
      "\n",
      "这些结果表明了 CVCL 的多模态表征如何允许分布之外的泛化 —— 与该能力其他更大规模的演示一致。为了说明这次评估所需的视觉泛化的程度，图 3B 展示了嵌入在话语中的单词的一些自然训练实例（从孩子的视角），与用于评估的新颖测试图像相匹配（以及它们的分类准确度）。此外，这次评估与经典婴儿词汇学习实验中呈现的刺激类型非常相似，这表明在实验室外获得的表现足以解释婴儿如何将实验室内的视觉刺激泛化到新的视觉刺激。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "多模态表征的组织结构\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最后，研究者介绍了 CVCL 中学习到的多模态表征结构的三个分析家族。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先探索的问题是，CVCL 的视觉和语言概念系统在多大程度上是一致的。例如，如果「汽车」的视觉和 词嵌入 都独立地更类似于「道路」而不是「球」，将表明良好的多模态对齐。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "使用 Labeled-S 中的 22 个概念，研究者通过随机抽取 100 个注释帧，提取其图像嵌入并跨帧平均计算每个概念的视觉原型。他们还检索了每个概念相应的 词嵌入 。接下来，计算这些嵌入之间的所有余弦相似度（包括模态内和模态间）并使用 t - 分布随机邻居嵌入（t-SNE）可视化它们之间的关系，如图 4A 和 B 所示。在图 4A 中，虚线表示每个概念相应的视觉质心和 词嵌入 之间的距离。\n",
      "\n",
      "由于这些跨模态距离中的许多都很小，研究者检查了概念之间的模态内相似性（通过余弦）是否与视觉和语言相关，发现了概念对齐的显著程度（相关系数 r = 0.37，p < 0.001）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这些关系不适用于 CVCL 的两个下界中的任何一个（图 S4）。此外，对齐距离也与分类性能呈强烈负相关（r = -0.65，p = 0.001），一些最不准确的类别表现出各自视觉原型和 词嵌入 之间的最大距离。图 4B 展示了每个概念的带标签图像嵌入的子集，强调不同的视觉概念在示例的紧密 聚类 程度方面存在差异。通过将视觉变化视为概念视觉嵌入与其视觉原型之间的平均 欧几里得距离 ，研究者还发现与分类性能的强烈负相关（r = -0.48，p = 0.025），这表明 CVCL 在处理「手」和「玩具」等单词参照 映射 时的难度与它们的视觉变化有关，与紧密 聚类 的概念如「汽车」和「婴儿床」相比。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "接下来，研究者可视化了在 CVCL 中不同的 词嵌入 如何与图像嵌入相互作用（图 4C）。检查三个不同的概念，他们观察到模型预测与特定 词嵌入 最相似的图像（以绿色显示）与每个类别的真实标注图像集（以蓝色显示）非常接近，完整概念集显示在图 S6 中。研究者发现 CVCL 学习将不同视觉相似的项目集合表示为一个概念的不同子簇，尽管每个词只使用一个向量。例如，「楼梯」的 词嵌入 最强烈地激活两个独立的集群，分别代表室内和室外楼梯，而「拼图」产生另外两个集群，代表字母和动物拼图。以前的 概念学习 心理理论通常需要明确、内置的机制来捕捉概念内部的子结构，但在 CVCL 中，我们发现多簇表示通过对比学习隐式地出现。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究者还定性检查了 CVCL 定位指代的能力。对于给定的图像，通过应用 Grad-CAM 获得一个注意力图，通过计算最终卷积层特征图的加权和（使用基于图像文本余弦相似度梯度相对于特征图的空域平均值的 权重 ），突出显示与目标类别最相关的图像区域。研究者可以将此注意力图叠加在图像上，并检查指代的位置与注意力图之间的任何对应关系。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "图 5 展示了四个概念中多个注意力图的示例。对于某些类别，CVCL 的注意力图提供了物体定位的证据：注意力图中最高激活的区域紧密跟踪指代的定位。\n",
      "\n",
      "更多研究细节，可参考原论文。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-09-29-9#comment\n",
      "title= 机器之心A100数智中国榜发布：让中小企业看懂、选对、用好数智化服务方案（附研究报告）\n",
      "author= []\n",
      "publish_date= 2020-09-29 00:00:00\n",
      "text= 行业方案榜：TOP数智+交通运输\n",
      "\n",
      "方案名称 服务商名称 入选理由\n",
      "\n",
      "ET航空大脑 阿里巴巴 集团 阿里云 是 阿里巴巴 旗下全球领先的 云计算 及 人工智能 公司，在财务资本方面具有雄厚的实力，自从成立以来一直强调多远化发展，其云服务已基本涵盖各行各业。它推出的ET航空大脑，涵盖大数据、 运筹优化 、路基 规划 等多种创新性技术，并聚焦于停机位分配、班组排版、餐补 规划 以及客流态式监控等多个航空运输聚焦领域。它已与数十个不同数智化技术中小微企业构建合作关系，并引入其相关优势技术。目前，该ET航空大脑已在北京首都国际机场落地，让1700个停机位安排可在50秒内完成 规划 ，总计节省约5000小时服务时长。\n",
      "\n",
      "百度 地图智慧交通解决方案 北京 百度 网讯科技有限公司 百度 是全球领先的互联网、 云计算 与 人工智能 上市公司，其在财务资本方面具有雄厚实力，自从成立以来已参与或建立多个国家级与省市级行业标准。它推出的 百度 地图智慧交通涵盖 人工智能 、大数据、GIS等多项创新性技术，并聚焦于交通服务行业的车流监控、路况分析、假日出行等多个常见应用场景。目前，该平台已与广播平台、研究机构等多行业数百家企业展开合作，平台开发者数量已接近180万，服务应用50万个，并有近1.5亿C端用户下载量。\n",
      "\n",
      "砖头物流管理系统 北京乐卡车联科技有限公司 北京乐卡车联科技有限公司是以车联万物为理念的智能车载系统硬件供应商，在财务资本方面实力有限，已完成A+轮融资，自从成立以来在从安全、效率、娱乐构建了完成的智能车载体系。它推出乐卡车联解决方案涵盖 人工智能 、大数据等多项创新性技术，支持经营管理等多个数智化场景应用，并专注于服务中小微交通运输业热门领域。它已服务千余万份订单，并与顺丰、德邦等知名品牌取得合作，且具有较为成熟的方案售后服务体系。\n",
      "\n",
      "大华智慧交通解决方案 大华股份集团 大华技术股份有限公司是全球领先的智慧物联网解决方案供应商和运营服务商。它推出的智慧交通解决方案涵盖 人工智能 、大数据、 计算机视觉 等创新技术，可覆盖高速公路、交通运输、铁路、城市轨道、港口等多个常见交通行业领域。该解决方案已覆盖全球近180个国家，并设立超过53个分支机构以进行售后服务支持。\n",
      "\n",
      "滴滴智慧交通解决方案 滴滴出行 滴滴出行是全球卓越的移动出行平台服务商，在全球范围内以向超过5.5亿C端用户提供过出行、外卖以及支付相关多元化服务。它推出的滴滴智慧交通解决方案融合了互联网、大数据、物联网、 运筹优化 等多项创新性技术，可覆盖拥堵分析、驾驶检测、潮汐车道 规划 、公交 调度 等多项交通热门领域应用场景。该解决方案已有近4875TB相关数据，可为政府、企业、个人提供高质量的智慧交通出行服务。\n",
      "\n",
      "智慧交通解决方案 华为 技术有限公司 华为 在财务资本方面具有雄厚实力，并是全球领先的通信服务寡头公司，自从成立以来获得多次国家级科技进步奖以及省市级科技奖励，并自主建立多项业内技术标准。它推出的智慧交通解决方案涵盖 云计算 、大数据、物联网、敏捷网络、BYOD、 5G 等多项创新技术，可围绕铁路运营、港口管理、机场运营等多场景提供一站式的解决方案。 华为 智慧交通解决方案所构建的深圳城市大脑项目，获得世界智慧城市博览会平安城市专项大奖。 华为 还计划推进该方案尝试与更多道路、汽车相关中小微企业取得合作，完善智慧交通版图。\n",
      "\n",
      "京东 物流 京东 物流集团 大华技术股份有限公司是全球领先的智慧物联网解决方案供应商和运营服务商。它推出的智慧交通解决方案涵盖 人工智能 、大数据、 计算机视觉 等创新技术，可覆盖高速公路、交通运输、铁路、城市轨道、港口等多个常见交通行业领域。该解决方案已覆盖全球近180个国家，并设立超过53个分支机构以进行售后服务支持。\n",
      "\n",
      "龙邦快运 龙邦物流有限公司 京东 物流隶属于 京东 集团，旨在通过开放、智能化的战略促进消费方式的转变与社会供应链效率的提升，将物流、商流、资金流和信息流有机结合，打造体验最优的物流履约解决方案。它推出的 京东 物流一体化解决方案涵盖大数据、 机器学习 、 运筹优化 、无人机、机器人等多种创新性技术，可实现库存共享、无人派送、订单集成处理、仓配一体以及极速达等多种交通运输领域常见服务。该及决方案已覆盖近200+城市，并针对中小微企业覆盖较广的服饰、消费品、母婴等热门行业打造了从仓储到配送、从线上到线下，从硬件到软件，从原材料采购至分销供应链的个性化解决方案。\n",
      "\n",
      "千方科技智慧交通解决方案 千方科技 千方科技是智慧交通领域客户解决方案提供商，聚焦于推动智慧交通、智能物联行业发展。它推出的千方科技智慧交通解决方案以大数据、 人工智能 、 云计算 为基础，构建了智慧交通与智慧物联双引擎，可服务运输、ETC、交通大数据、民航、轨交、高速等多个交通领域热门场景。该解决方案已形成从硬件基础设施到软件智慧中枢的完成产业链。目前，该解决方案已在2018年为公司带来近34.93亿元收入，同比增长达到近20%。\n",
      "\n",
      "货运中国网互联网+物流SAAS云平台 上海成达智慧物流有限公司 成达信息科技是互联网物流运营的先行者，在财务资本方面有一定实力，已完成Pre-A轮融资，自从成立以来推出多种针对传统物流进行优化的数智化产品。它推出的货运中国网络平台涵盖大数据、 人工智能 、 机器学习 等多项创新性技术，支持销售营销等多个数智化场景应用，并已在国内建立20个城市据点，服务20万辆汽车车主，且具有较为成熟的方案售后服务体系。\n",
      "\n",
      "华宇开放平台 上海华振物流有限公司 盛丰物流集团有限公司是一家专注于国内干线运输、货物仓储、物流配送、物流解决方案策划与设计的国家5A级综合物流企业，自从成立以来公司以福州为总部建立了280家分公司与仓库，自有货车近8000辆，全国公路零担快运第五名。它推出的盛丰物流涵盖大数据、物联网、 运筹优化 等多种创新性技术，服务 调度 规划 、仓储包装、加工搬运等多个常见交通运输领域，并为其供应链产业提供信息化数据共享增值服务。该物流已与近2000家制造业中小微企业取得物流合作，认证伙伴超过10000家。\n",
      "\n",
      "运去哪一站式国际物流在线服务平台 上海汇航捷讯网络科技有限公司 上海汇航捷讯网络科技有限公司是以物流及外贸行业为具体应用对象的新型B2B互联网企业，在财务资本方面实力有限，自从成立以来多次获得无人货运相关奖项，并构建了多种智能运输货运产品。它推出的运去哪涵盖 人工智能 、自然语言处理等多项创新性技术，支持销售营销等多个数智化场景应用，并专注于服务交通运输热门领域。它已服务1万家+中小微客户遍布全球15万+航线，且具有成熟的方案售后服务体系。\n",
      "\n",
      "佳吉快运 上海佳吉快运有限公司 滴滴出行是全球卓越的移动出行平台服务商，在全球范围内以向超过5.5亿C端用户提供过出行、外卖以及支付相关多元化服务。它推出的滴滴智慧交通解决方案融合了互联网、大数据、物联网、 运筹优化 等多项创新性技术，可覆盖拥堵分析、驾驶检测、潮汐车道 规划 、公交 调度 等多项交通热门领域应用场景。该解决方案已有近4875TB相关数据，可为政府、企业、个人提供高质量的智慧交通出行服务。\n",
      "\n",
      "智能交通解决方案 深圳市 腾讯 计算机系统有限公司 腾讯 是全球领先的互联网、 云计算 与 人工智能 上市公司，其在财务资本方面具有雄厚实力，自从成立以来已参与或建立数百个国家级与省市级行业标准。它推出的 腾讯 智能交通解决方案涵盖 人工智能 、大数据、 计算机视觉 、自然语言处理、 机器学习 、知识表征等多项创新性技术，并聚焦于交通路况、公共出行、事故处理、违章缴罚等多个交通行业服务领域。目前，该解决方案已与上万销售代理商、软件开发商以及解决方案商建立合作关系，并开始契合 腾讯 We Transport大战略帮助交通行业中小微企业进行数智化转型。\n",
      "\n",
      "盛丰物流 盛丰物流集团有限公司 上海佳吉快运有限公司是国家认证的5A级物流企业，公司主营公路零担运输业务，并使始终重视数智化企业服务能力的提升与货运信息化网络的建设。它推出的佳吉快运服务涵盖GPS、物联网、大数据、 运筹优化 等多项创新性技术，可囊括货车定位、配送监控以及智能 调度 等多项交通运输领域常见应用场景。目前该服务已在全国范围内拥有近2500多个服务网点，17个大型转运中心、信息化网络服务能力达到所有省级行政区。\n",
      "\n",
      "顺丰速运 顺丰速运 龙邦物流有限公司成立于2012年，是“龙邦供应链”旗下一家以物流供应链管理为核心，布局全国物流网络运营、互联网技术研发的创新性企业。它推出的龙邦快运利用大数据、物理网、 运筹优化 等技术，对零担物流、专线资源实现网络化监控、信息化运营，为中小微企业提供全国快运门对门一站式综合物流及决方案。目前该方案已在全球范围内拥有分拨中心81个，服务网点3000余家以及配送车辆8000余台。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-01-8\n",
      "title= 小红书开源「InstantID」效果炸裂，被Yann LeCun点赞，迅速蹿上Github热榜\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 只需一张照片，整个过程无需训练 LoRA 模型，多风格 AI 写真即刻呈现！\n",
      "\n",
      "最近，有一群来自小红书的 95 后神秘团队，自称 InstantX，搞了个大动作 —— 开源「InstantID」项目。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 凭借着高质量的 图像生成 能力，在开源界掀起了一股热潮：不仅获得了众多技术大佬的点赞，更是在 GitHub 热榜上迅速飙升，成为焦点。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个「出片神器」，让用户只需上传一张照片，就能轻松定制出多种风格的 AI 写真。\n",
      "\n",
      "对，你没看错。如图左侧所示，与之前爆火的妙鸭相机至少需要上传 20 张照片不同的是，InstantID 只需一张自拍，不依赖模型训练，不需要等待，瞬间变身。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "无论是古典油画的优雅，炫酷的赛博朋克，或是 3D 雕像的立体感，只要是你喜欢的风格，InstantID 都能轻松驾驭。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "它不仅风格多样，还能在保持人物面部高保真的同时，无需模型训练，实现秒级出图，效率大幅提升。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 目前位列 Hugging Face Space Trending 榜首，许多小伙伴玩得不亦乐乎～\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如，把马斯克送上了火星。\n",
      "\n",
      "让蒙娜丽莎拍「樱花写真」，微笑依旧很神秘。\n",
      "\n",
      "甚至可以让语文课本中的杜甫从二维变三维，穿越到现代变身「帅大叔」。\n",
      "\n",
      "图灵奖得主 Yann LeCun，化身多种动漫人物，你猜出了几个角色？\n",
      "\n",
      "就连 Yann LeCun 本人也点赞转发，调侃自己的「钢铁侠」衣服在哪里。\n",
      "\n",
      "在个性化图像合成领域，实现强烈风格化写真的同时保持面部高保真度，一直是个挑战。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从效果上看，InstantID 做到了。那它背后运用了哪些方法，有什么独到之处吗？\n",
      "\n",
      "回顾过去，尽管 Textual Inversion、 DreamBooth 和 LoRAs 等技术已经取得了重大进展。但它们在实际应用中仍受限于高存储需求、耗时的微调过程以及对多张参考图像的依赖。相比之下，现有基于 ID 嵌入的方法虽然只需一次前向推理，但也面临不小挑战：要么需要对大量模型 参数 进行广泛的微调，要么与社区预训练模型不兼容，要么无法保持高真实性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 的出现，打破了这些局限。小红书 InstantX 团队公开了论文《 InstantID: Zero-shot Identity-Preserving Generation in Seconds 》和推理代码，他们表示：InstantID 巧妙地避免了对文生图模型 UNet 部分的训练，仅通过训练一个轻量级的可插拔模块，实现了在推理过程中无需 test-time tuning，同时保持了文本控制的灵活性，确保了面部特征的高保真度。\n",
      "\n",
      "如图所示，InstantID 的工作原理可分为三个关键部分：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ID Embedding：团队利用预训练的面部识别模型代替 CLIP 来提取语义人脸特征，并使用可训练的投影层，将这些特征 映射 到文本特征空间，形成 Face Embedding，具有丰富的语义信息，包括如面部特征、表情、年龄等，为后续的 图像生成 提供了坚实的基础。\n",
      "\n",
      "Image Adapter：引入一个轻量级的适配模块，将提取的身份信息与文本提示结合起来。这个模块通过解耦的交叉 注意力机制 ，使得图像和文本能够独立地影响生成过程，从而在保持身份信息的同时，允许用户对图像风格进行精细控制，实现「双赢」。\n",
      "\n",
      "IdentityNet：小红书提出了一个名为 IdentityNet 的网络，是 InstantID 的核心部分。它通过强语义条件（如面部特征的详细描述）和弱空间条件（如面部关键点的位置）来编码参考面部图像的复杂特征。在 IdentityNet 中，生成过程完全由 Face Embedding 引导，无需任何文本信息。仅更新新添加的模块，而预先训练的文本到图像模型保持冻结以确保灵活性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在实际的 图像生成 过程中，InstantID 首先会接收到用户的文本提示和面部图像。然后通过 ID Embedding 提取关键信息，接着 Image Adapter 将这些信息与文本提示融合。IdentityNet 会根据这些融合后的信息生成图像。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "整个过程是自动化的，用户不需要进行任何额外的微调或训练，只需等待二十几秒，就能得到一个既符合文本描述又保留个人身份特征的定制图像。\n",
      "\n",
      "InstantID 不仅解决了训练效率与身份保真度之间的平衡问题，还提供了一系列令人印象深刻的特性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先，InstantID 的即插即用和兼容性是其最大的卖点之一。它无需对 UNet 进行额外训练，即可与现有的预训练模型无缝集成，如社区内的文生图基础模型、LoRAs 和 ControlNets。这意味着用户可以在不增加成本的情况下，轻松地在推理过程中保持人物的身份特征，裂变性强。\n",
      "\n",
      "其次，InstantID 的无需微调特性，使得它在实际应用中极具经济性和实用性。用户只需进行一次前向传播，即可快速生成图像，同时保持对文本编辑的强大控制力，让身份信息与各种风格完美融合。如下图所示，其编辑性强的特点让用户能够通过文本控制性别、头发、服装等细节，确保生成图像的多样性。\n",
      "\n",
      "性能方面的表现同样卓越，它能够仅凭一张参考图像，就生成具有高保真度和灵活性的先进结果。这一性能不仅超越了基于单张图片特征的嵌入方法，如 IP-Adapter-FaceID，而且在特定场景下，其效果与 ROOP、LoRAs 等方法不相上下。\n",
      "\n",
      "对于相似度有更高要求的真人写真场景，InstantID 也能完成得不错。不仅能够在秒级时间内完成高质量的 图像生成 ，还避免耗时的 LoRa 训练，相比妙鸭成本更低，大约是其 1/300。通过精细化控制脸部区域，InstantID 能够增强脸部相似度，同时保持整体风格的和谐。\n",
      "\n",
      "此外，InstantID 的分区域生成方案支持多人多风格的 图像生成 ，耗时基本无增。\n",
      "\n",
      "它的鲁棒性和泛化性，使其能顺利处理夸张的五官比例。\n",
      "\n",
      "多视角的生成也没问题。按你指定的姿势图和面部特征，生成新的 AI 写真。\n",
      "\n",
      "InstantID 的可扩展性良好，能够快速支持多种衍生功能。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如快速换脸。与 Inswapper 相比，InstantID 生成的作品在面孔和背景的融合上更加灵活。\n",
      "\n",
      "ID 信息 插值 。InstantID 支持两脸自定义融合，保留双方特征。\n",
      "\n",
      "非人像与 ID 的结合，很有特点。\n",
      "\n",
      "聊到这儿，不妨你亲自尝试一下，感受它的魅力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "操作方式非常简单，进入 InstantID 的 Demo 页面，直接上传照片，便可免费体验 ：\n",
      "\n",
      "https://huggingface.co/spaces/InstantX/InstantID\n",
      "\n",
      "InstantID 的这些优势，不仅为个人用户提供了强大的创作工具，也为商业应用如电子商务、广告和娱乐产业开辟了新的可能性。InstantID 本次表现令人惊喜，其高效、灵活、强大的性能和易用性，印象深刻。期待小红书该开源项目的后续进展，未来能在多个领域发挥出更大的价值。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "附录： \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-01-8#comment\n",
      "title= 小红书开源「InstantID」效果炸裂，被Yann LeCun点赞，迅速蹿上Github热榜\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 只需一张照片，整个过程无需训练 LoRA 模型，多风格 AI 写真即刻呈现！\n",
      "\n",
      "最近，有一群来自小红书的 95 后神秘团队，自称 InstantX，搞了个大动作 —— 开源「InstantID」项目。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 凭借着高质量的 图像生成 能力，在开源界掀起了一股热潮：不仅获得了众多技术大佬的点赞，更是在 GitHub 热榜上迅速飙升，成为焦点。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个「出片神器」，让用户只需上传一张照片，就能轻松定制出多种风格的 AI 写真。\n",
      "\n",
      "对，你没看错。如图左侧所示，与之前爆火的妙鸭相机至少需要上传 20 张照片不同的是，InstantID 只需一张自拍，不依赖模型训练，不需要等待，瞬间变身。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "无论是古典油画的优雅，炫酷的赛博朋克，或是 3D 雕像的立体感，只要是你喜欢的风格，InstantID 都能轻松驾驭。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "它不仅风格多样，还能在保持人物面部高保真的同时，无需模型训练，实现秒级出图，效率大幅提升。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 目前位列 Hugging Face Space Trending 榜首，许多小伙伴玩得不亦乐乎～\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如，把马斯克送上了火星。\n",
      "\n",
      "让蒙娜丽莎拍「樱花写真」，微笑依旧很神秘。\n",
      "\n",
      "甚至可以让语文课本中的杜甫从二维变三维，穿越到现代变身「帅大叔」。\n",
      "\n",
      "图灵奖得主 Yann LeCun，化身多种动漫人物，你猜出了几个角色？\n",
      "\n",
      "就连 Yann LeCun 本人也点赞转发，调侃自己的「钢铁侠」衣服在哪里。\n",
      "\n",
      "在个性化图像合成领域，实现强烈风格化写真的同时保持面部高保真度，一直是个挑战。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从效果上看，InstantID 做到了。那它背后运用了哪些方法，有什么独到之处吗？\n",
      "\n",
      "回顾过去，尽管 Textual Inversion、 DreamBooth 和 LoRAs 等技术已经取得了重大进展。但它们在实际应用中仍受限于高存储需求、耗时的微调过程以及对多张参考图像的依赖。相比之下，现有基于 ID 嵌入的方法虽然只需一次前向推理，但也面临不小挑战：要么需要对大量模型 参数 进行广泛的微调，要么与社区预训练模型不兼容，要么无法保持高真实性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 的出现，打破了这些局限。小红书 InstantX 团队公开了论文《 InstantID: Zero-shot Identity-Preserving Generation in Seconds 》和推理代码，他们表示：InstantID 巧妙地避免了对文生图模型 UNet 部分的训练，仅通过训练一个轻量级的可插拔模块，实现了在推理过程中无需 test-time tuning，同时保持了文本控制的灵活性，确保了面部特征的高保真度。\n",
      "\n",
      "如图所示，InstantID 的工作原理可分为三个关键部分：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ID Embedding：团队利用预训练的面部识别模型代替 CLIP 来提取语义人脸特征，并使用可训练的投影层，将这些特征 映射 到文本特征空间，形成 Face Embedding，具有丰富的语义信息，包括如面部特征、表情、年龄等，为后续的 图像生成 提供了坚实的基础。\n",
      "\n",
      "Image Adapter：引入一个轻量级的适配模块，将提取的身份信息与文本提示结合起来。这个模块通过解耦的交叉 注意力机制 ，使得图像和文本能够独立地影响生成过程，从而在保持身份信息的同时，允许用户对图像风格进行精细控制，实现「双赢」。\n",
      "\n",
      "IdentityNet：小红书提出了一个名为 IdentityNet 的网络，是 InstantID 的核心部分。它通过强语义条件（如面部特征的详细描述）和弱空间条件（如面部关键点的位置）来编码参考面部图像的复杂特征。在 IdentityNet 中，生成过程完全由 Face Embedding 引导，无需任何文本信息。仅更新新添加的模块，而预先训练的文本到图像模型保持冻结以确保灵活性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在实际的 图像生成 过程中，InstantID 首先会接收到用户的文本提示和面部图像。然后通过 ID Embedding 提取关键信息，接着 Image Adapter 将这些信息与文本提示融合。IdentityNet 会根据这些融合后的信息生成图像。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "整个过程是自动化的，用户不需要进行任何额外的微调或训练，只需等待二十几秒，就能得到一个既符合文本描述又保留个人身份特征的定制图像。\n",
      "\n",
      "InstantID 不仅解决了训练效率与身份保真度之间的平衡问题，还提供了一系列令人印象深刻的特性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先，InstantID 的即插即用和兼容性是其最大的卖点之一。它无需对 UNet 进行额外训练，即可与现有的预训练模型无缝集成，如社区内的文生图基础模型、LoRAs 和 ControlNets。这意味着用户可以在不增加成本的情况下，轻松地在推理过程中保持人物的身份特征，裂变性强。\n",
      "\n",
      "其次，InstantID 的无需微调特性，使得它在实际应用中极具经济性和实用性。用户只需进行一次前向传播，即可快速生成图像，同时保持对文本编辑的强大控制力，让身份信息与各种风格完美融合。如下图所示，其编辑性强的特点让用户能够通过文本控制性别、头发、服装等细节，确保生成图像的多样性。\n",
      "\n",
      "性能方面的表现同样卓越，它能够仅凭一张参考图像，就生成具有高保真度和灵活性的先进结果。这一性能不仅超越了基于单张图片特征的嵌入方法，如 IP-Adapter-FaceID，而且在特定场景下，其效果与 ROOP、LoRAs 等方法不相上下。\n",
      "\n",
      "对于相似度有更高要求的真人写真场景，InstantID 也能完成得不错。不仅能够在秒级时间内完成高质量的 图像生成 ，还避免耗时的 LoRa 训练，相比妙鸭成本更低，大约是其 1/300。通过精细化控制脸部区域，InstantID 能够增强脸部相似度，同时保持整体风格的和谐。\n",
      "\n",
      "此外，InstantID 的分区域生成方案支持多人多风格的 图像生成 ，耗时基本无增。\n",
      "\n",
      "它的鲁棒性和泛化性，使其能顺利处理夸张的五官比例。\n",
      "\n",
      "多视角的生成也没问题。按你指定的姿势图和面部特征，生成新的 AI 写真。\n",
      "\n",
      "InstantID 的可扩展性良好，能够快速支持多种衍生功能。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如快速换脸。与 Inswapper 相比，InstantID 生成的作品在面孔和背景的融合上更加灵活。\n",
      "\n",
      "ID 信息 插值 。InstantID 支持两脸自定义融合，保留双方特征。\n",
      "\n",
      "非人像与 ID 的结合，很有特点。\n",
      "\n",
      "聊到这儿，不妨你亲自尝试一下，感受它的魅力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "操作方式非常简单，进入 InstantID 的 Demo 页面，直接上传照片，便可免费体验 ：\n",
      "\n",
      "https://huggingface.co/spaces/InstantX/InstantID\n",
      "\n",
      "InstantID 的这些优势，不仅为个人用户提供了强大的创作工具，也为商业应用如电子商务、广告和娱乐产业开辟了新的可能性。InstantID 本次表现令人惊喜，其高效、灵活、强大的性能和易用性，印象深刻。期待小红书该开源项目的后续进展，未来能在多个领域发挥出更大的价值。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "附录： \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-01-7\n",
      "title= 刚刚，字节版GPTs「扣子」上线了\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 大玩家终于正式下场。\n",
      "\n",
      "在持续一年的大模型热潮之后，「智能体」成为了科技公司们新的押注方向之一。\n",
      "\n",
      "近日， 字节跳动 正式推出「Coze 扣子」AI Bot 开发平台。任何用户都可以快速、低门槛地搭建自己的 Chatbot，且平台支持用户将其一键发布到飞书、微信公众号、豆包等渠道。\n",
      "\n",
      "链接：https://www.coze.cn/\n",
      "\n",
      "当然，除了可以创建自己的 Chatbot，Coze 官方还提供了 Bots 商店和插件。\n",
      "\n",
      "如以下按照热度精选的 Bots，包含娱乐、创意、学习等各类产品，甚至我们注意到还有「马歇尔音箱粉丝」这个选项。\n",
      "\n",
      "机器之心 挑选了一些已有的 Bot 试了试，看看上手体验如何。\n",
      "\n",
      "首先我们测了一个对学生家长来说可能帮助比较大的 Bot——「数学老师」Bot。这个 Bot 在运行时会调用 Wolfram Alpha、OCR 等插件。Wolfram 可以理解为一个超强计算器，在 ChatGPT 问世之初，其创始人就呼吁将 ChatGPT 与 Wolfram Alpha 联合使用。如今，在扣子平台上，国内家长也能用到大模型和 Wolfram Alpha 结合的辅导工具了。\n",
      "\n",
      "从我们的初步测试来看，「数学老师」Bot 可以用来解答一些数学应用题，还能解答数学概念。OCR 插件的调用还能让它具备读图能力，但几何题解答起来未必正确。\n",
      "\n",
      "在「咨询」类别下，我们还找到了一个可以和小朋友聊天的 Bot。这个机器人可以调用必应搜索和文生图插件 ByteArtist，不仅能和小朋友聊天，还有画画等多模态技能。\n",
      "\n",
      "当然，这些 Bot 有多少玩法、怎么玩出新花样其实更多地取决于你。只要稍微修改一下扣子提供的编排模板，你就能拥有一个按照自己心意定制的 Bot，而且后台还有更多插件可以调用，这里面有着巨大的探索空间。\n",
      "\n",
      "据官方文档介绍，Coze 包括以下功能和优势：\n",
      "\n",
      "首先是无限拓展的能力集：扣子的插件工具极为丰富，从而拓展了 Bot 的能力边界。目前平台已经集成了超过 60 款各类型的插件，包括资讯阅读、旅游出行、效率办公、图片理解等 API 及多模态模型。用户可以直接将这些插件添加到 Bot 中，丰富 Bot 能力。此外，扣子平台也支持创建自定义插件。你可以将已有的 API 能力通过 参数 配置的方式快速创建一个插件让 Bot 调用。\n",
      "\n",
      "官方提供的插件类别比较丰富。据 机器之心 了解，这些插件有自研的、合作的，还有内部 hackathon 比赛作品。\n",
      "\n",
      "其次，丰富的数据源：扣子提供了简单易用的 知识库 功能来管理和存储数据，支持 Bot 与用户自己的数据进行交互。无论是内容量巨大的本地文件还是某个网站的实时信息，都可以上传到 知识库 中。这样，Bot 就可以使用 知识库 中的内容回答问题了。 知识库 支持添加文本格式、表格格式的数据。对于上传的内容，用户可以将本地 TXT、PDF、DOCX、Excel、CXV 格式的文档上传至 知识库 ，也可以基于 URL 获取在线网页内容和 API JSON 数据。同时支持直接在 知识库 内添加自定义数据。\n",
      "\n",
      "用户可以创建自己的 知识库\n",
      "\n",
      "知识库 支持添加文本格式、表格格式的数据\n",
      "\n",
      "持久化的记忆能力：扣子提供了方便 AI 交互的 数据库 记忆能力，可持久记住用户对话的重要 参数 或内容。\n",
      "\n",
      "灵活的工作流设计：扣子的工作流功能可以用来处理 逻辑 复杂，且有较高稳定性要求的任务流。扣子提供了大量灵活可组合的节点包括大 语言模型 LLM、自定义代码、判断 逻辑 等，无论你是否有编程基础，都可以通过拖拉拽的方式快速搭建一个工作流，例如创建一个撰写行业研究报告的工作流，让 Bot 写一份 20 页的报告。\n",
      "\n",
      "看起来，字节把扣子定位为一个应用创作平台：你可以在其上开发出属于自己的 AI chatbot（ 聊天机器人 ），无需有编程经验，扣子就可以快速创建出各种类型的 聊天机器人 ，并将它们部署在不同的社交平台和应用程序上。\n",
      "\n",
      "创建完成之后，发布流程也十分简洁。扣子支持发布到 AI 聊天应用豆包、办公平台飞书，以及用户的微信公众号（目前只支持发布到服务号，不支持发布到订阅号）和微信客服。\n",
      "\n",
      "后续更多玩法我们还在探索中，欢迎大家讨论。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-01-7#comment\n",
      "title= 刚刚，字节版GPTs「扣子」上线了\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 大玩家终于正式下场。\n",
      "\n",
      "在持续一年的大模型热潮之后，「智能体」成为了科技公司们新的押注方向之一。\n",
      "\n",
      "近日， 字节跳动 正式推出「Coze 扣子」AI Bot 开发平台。任何用户都可以快速、低门槛地搭建自己的 Chatbot，且平台支持用户将其一键发布到飞书、微信公众号、豆包等渠道。\n",
      "\n",
      "链接：https://www.coze.cn/\n",
      "\n",
      "当然，除了可以创建自己的 Chatbot，Coze 官方还提供了 Bots 商店和插件。\n",
      "\n",
      "如以下按照热度精选的 Bots，包含娱乐、创意、学习等各类产品，甚至我们注意到还有「马歇尔音箱粉丝」这个选项。\n",
      "\n",
      "机器之心 挑选了一些已有的 Bot 试了试，看看上手体验如何。\n",
      "\n",
      "首先我们测了一个对学生家长来说可能帮助比较大的 Bot——「数学老师」Bot。这个 Bot 在运行时会调用 Wolfram Alpha、OCR 等插件。Wolfram 可以理解为一个超强计算器，在 ChatGPT 问世之初，其创始人就呼吁将 ChatGPT 与 Wolfram Alpha 联合使用。如今，在扣子平台上，国内家长也能用到大模型和 Wolfram Alpha 结合的辅导工具了。\n",
      "\n",
      "从我们的初步测试来看，「数学老师」Bot 可以用来解答一些数学应用题，还能解答数学概念。OCR 插件的调用还能让它具备读图能力，但几何题解答起来未必正确。\n",
      "\n",
      "在「咨询」类别下，我们还找到了一个可以和小朋友聊天的 Bot。这个机器人可以调用必应搜索和文生图插件 ByteArtist，不仅能和小朋友聊天，还有画画等多模态技能。\n",
      "\n",
      "当然，这些 Bot 有多少玩法、怎么玩出新花样其实更多地取决于你。只要稍微修改一下扣子提供的编排模板，你就能拥有一个按照自己心意定制的 Bot，而且后台还有更多插件可以调用，这里面有着巨大的探索空间。\n",
      "\n",
      "据官方文档介绍，Coze 包括以下功能和优势：\n",
      "\n",
      "首先是无限拓展的能力集：扣子的插件工具极为丰富，从而拓展了 Bot 的能力边界。目前平台已经集成了超过 60 款各类型的插件，包括资讯阅读、旅游出行、效率办公、图片理解等 API 及多模态模型。用户可以直接将这些插件添加到 Bot 中，丰富 Bot 能力。此外，扣子平台也支持创建自定义插件。你可以将已有的 API 能力通过 参数 配置的方式快速创建一个插件让 Bot 调用。\n",
      "\n",
      "官方提供的插件类别比较丰富。据 机器之心 了解，这些插件有自研的、合作的，还有内部 hackathon 比赛作品。\n",
      "\n",
      "其次，丰富的数据源：扣子提供了简单易用的 知识库 功能来管理和存储数据，支持 Bot 与用户自己的数据进行交互。无论是内容量巨大的本地文件还是某个网站的实时信息，都可以上传到 知识库 中。这样，Bot 就可以使用 知识库 中的内容回答问题了。 知识库 支持添加文本格式、表格格式的数据。对于上传的内容，用户可以将本地 TXT、PDF、DOCX、Excel、CXV 格式的文档上传至 知识库 ，也可以基于 URL 获取在线网页内容和 API JSON 数据。同时支持直接在 知识库 内添加自定义数据。\n",
      "\n",
      "用户可以创建自己的 知识库\n",
      "\n",
      "知识库 支持添加文本格式、表格格式的数据\n",
      "\n",
      "持久化的记忆能力：扣子提供了方便 AI 交互的 数据库 记忆能力，可持久记住用户对话的重要 参数 或内容。\n",
      "\n",
      "灵活的工作流设计：扣子的工作流功能可以用来处理 逻辑 复杂，且有较高稳定性要求的任务流。扣子提供了大量灵活可组合的节点包括大 语言模型 LLM、自定义代码、判断 逻辑 等，无论你是否有编程基础，都可以通过拖拉拽的方式快速搭建一个工作流，例如创建一个撰写行业研究报告的工作流，让 Bot 写一份 20 页的报告。\n",
      "\n",
      "看起来，字节把扣子定位为一个应用创作平台：你可以在其上开发出属于自己的 AI chatbot（ 聊天机器人 ），无需有编程经验，扣子就可以快速创建出各种类型的 聊天机器人 ，并将它们部署在不同的社交平台和应用程序上。\n",
      "\n",
      "创建完成之后，发布流程也十分简洁。扣子支持发布到 AI 聊天应用豆包、办公平台飞书，以及用户的微信公众号（目前只支持发布到服务号，不支持发布到订阅号）和微信客服。\n",
      "\n",
      "后续更多玩法我们还在探索中，欢迎大家讨论。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-05-7\n",
      "title= 比OpenAI官方提示词指南更全，这26条黄金准则让LLM性能飙升50%以上\n",
      "author= []\n",
      "publish_date= 2024-02-05 00:00:00\n",
      "text= 今天，穆罕默德・本・扎耶德 人工智能 大学 VILA Lab 带来了一项关于如何更好地为不同规模的大模型书写提示词（prompt）的研究，让大模型性能在不需要任何额外训练的前提下轻松提升 50% 以上。该工作在 X (Twitter)、Reddit 和 LinkedIn 等平台上都引起了广泛的讨论和关注。\n",
      "\n",
      "论文地址: https://arxiv.org/abs/2312.16171\n",
      "\n",
      "Github地址: https://github.com/VILA-Lab/ATLAS\n",
      "\n",
      "论文标题：Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在以 ChatGPT 为首的大模型出来之后，为大 语言模型 设计提示词的研究已经成为一个重要的研究方向，包括 OpenAI 官方也出品了针对 ChatGPT 用户的提示工程指南 [1] ，其包含了六条书写准则：1）写出清晰的指令；2）提供参考文本；3）将复杂的任务拆分为更简单的子任务；4）给模型时间「思考」；5）使用外部工具；6）系统地测试更改。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "由此可见，提示词对于如何更好地使用大模型以及得到满意的回答都具有重要的意义。然而可以看到的是，OpenAI 提供的这些准则都是比较宽泛和保守的，并没有涉及到一些具体的操作和技巧。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "今天要介绍的这篇文章提供了更多也更加接地气的提示工程指南，足足有 26 条之多，内容涵盖了：1）回答内容和语言风格的控制；2）提示词结构和清晰度；3）复杂任务和代码提示；4）回答特异性和信息量；5）用户交互和参与等多个方面。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "下面让我们来逐条讨论一下这些提示词准则：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1) 如果您更喜欢更简洁的答案，则无需对 LLM 保持礼貌，因此无需添加诸如 「请」、「如果你不介意」、「谢谢」、「我愿意」等，直奔主题即可。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2) 在提示中融入目标受众，例如该领域的专家。具体而言，当你告诉大模型你的目标受众是一个孩子，它的回答会更加通俗易懂，当你告诉它受众是这个领域的专家，它会提供更加专业和深入的解释。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3) 在交互式对话中将复杂的任务分解为一系列更简单的提示。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4) 使用肯定的指令，如「做」，同时避免使用「不要」等否定性语言。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5) 当您需要简单清晰或更深入地了解某个主题、想法或任何信息时，请利用以下提示：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "用简单的术语解释 [插入特定主题]。\n",
      "\n",
      "像我是 11 岁一样向我解释这个问题。\n",
      "\n",
      "向我解释，就好像我是 [领域] 的初学者一样。\n",
      "\n",
      "用简单的英语写 [文章 / 文本 / 段落]，就像你在向一个 5 岁的孩子解释一些事情一样。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6) 添加「我要给 $xxx 小费以获得更好的解决方案！」这种提示词会带来提升的原因可能是：在训练数据中，当涉及到回答是有奖励的，回答的人往往会更加准确细致，小心谨慎地提供答案，大模型从这些网络数据中学到了这些结构和方式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7) 实现示例驱动的提示（使用少样本提示）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8) 格式化提示时，以「###Instruction###」开头，然后是「###Example###」 或「###Question###」（如果相关）。随后展示您的内容。使用一个或多个换行符用于分隔指令、示例、问题、上下文和输入数据。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9) 在你的提示词里面加入以下短语：「你的任务是」和「你必须」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10) 在你的提示词里面加入以下短语：「你会受到惩罚」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11) 在提示中使用「以自然、类似人类的方式回答问题」这句话。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12) 使用引导性词语，例如写「一步一步地思考」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13) 在提示中加上以下短语：「确保你的回答是公正的，避免依赖刻板印象」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14) 让模型通过向你提问来引出你精确的细节和要求，直到他得到足够的信息来提供所需的输出（例如，「从现在开始，我希望你问我......」提问）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15) 要询问特定主题或想法或任何信息，并且您想测试您的理解，您可以使用 以下短语：「教我任何 [定理 / 主题 / 规则名称]，并在末尾包含一个测试，并让我知道是否在我回答后，我的答案是正确的，不要事先提供答案。 」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "16) 为大型 语言模型 分配角色。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17) 使用分隔符。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "18) 在提示中多次重复特定单词或短语。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19) 将思维链 （CoT） 与 few-Shot 提示相结合。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "20) 使用输出引导，包括用所需输出的开头结束提示。利用输出引导，以预期响应的开头结束提示。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "21) 如果任务是写一篇文章 / 文本 / 段落或任何类型的文本，同时需要尽可能的详细，可以添加提示词：「写一篇详细的 [论文 / 文本 / 段落]，通过添加所有必要的信息从而使我能详细了解 [主题]。 」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "22) 在不改变其样式的情况下更正 / 更改特定文本：尝试修改用户发送的每个段落。你应该只提高用户的语法和词汇量，并确保它听起来很自然。您应该保留原始写作风格，确保正式段落保持正式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "23) 当您有一个复杂的编程提示时，该提示可能位于不同的文件中：「从现在开始，每当您生成跨越多个文件的代码，生成一个可以自动运行的 [编程语言 ] 脚本，创建指定的文件或对现有文件进行更改以插入生成的代码。[你的问题]」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "24) 当您想使用特定单词、短语或句子开始或继续文本时，请使用以下方法提示：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我为你提供开头 [歌词 / 故事 / 段落 / 散文...]：[插入歌词 / 单词 / 句子]。根据提供的单词完成它。保持内容风格一致。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "25) 明确说明模型必须遵循的要求去生成内容， 以关键字、规定、提示或说明的形式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "26) 如果要编写任何文本，例如文章或段落，并且需要与提供的示例相似，请包括下面提示语句：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "根据提供的段落使用相同的语言 [/title/text/essay/answer]。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "以下是一些具体的提示词例子和对应的 GPT-4 输出结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. 当我们询问 GPT-4 问题的时候，最后可以加上一句「提供公正的解释，突出科学证据和不同观点。」可以看到加上该提示词之后 GPT-4 的回答明显会更加丰富和有深度。\n",
      "\n",
      "2. 我们可以提供一些示例让模型更好的理解我们的目标和出发点。\n",
      "\n",
      "3. 我们可以告诉模型用简单的方法来回答问题，就像是在向一个 5 岁的孩子解释一些事情。可以看到加上和不加这个提示词，模型的回复在理解困难程度上有明显的差别。\n",
      "\n",
      "4. 我们可以通过给模型小费的方式，让模型更加严谨完善的回答问题。\n",
      "\n",
      "定量实验结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. 模型回答质量提升比例：该指标表示在使用提示词原则后，问题的回答质量提高的百分比。\n",
      "\n",
      "可以看到所有提示词原则在人工评测中都取得了或多或少的提升，其中原则 14 获得了 100% 的提升，意味着所有问题通过使用该提示原则都获得了提升。与此同时，原则 1 得到的提升相对较少。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2. 回复正确性提升：正确性指模型输出或回答的精度，判断标准是回答是否准确、相关且没有错误的。本文同时考虑了不同模型的绝对正确性和相对正确性提升两个指标。\n",
      "\n",
      "上图结果为加入提示原则后，大模型回复质量的相对正确性提升。「small」表示 7B 模型，「medium」表示 13B 模型，「large scale」表示 70B 和 GPT-3.5/4 模型。可以看到大模型在使用提示词原则后，提升幅度相对于小模型和中等模型会更加显著。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3. 单独每个模型准确度提升比例：\n",
      "\n",
      "上图是每个不同大小的模型相对提升幅度，可以看到类似的现象，模型越大，对于提示词的响应和回复也越加敏锐，准确性提升也相对越大。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4. 下图是不同大小模型对于每条提示词原则准确度提升大小具体结果：\n",
      "\n",
      "提示词准则数据集：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "本文在介绍 26 条提示词准则的同时，还附带发布了一个基于准则提示词的 基准 ，其中每条准则作者准备了 20 个不同的问题，每个问题同时包含带有准则和不带准则两种对应的大模型回复。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "该数据集可以用在：1）大 语言模型 对于提示词响应的性能评测；2）偏好驱动的大模型微调。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "数据集链接：https://github.com/VILA-Lab/ATLAS。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "更多提示词原则使用方法和说明，欢迎阅读原文。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考文献\n",
      "\n",
      "[1] Six strategies for getting better results. OpenAI. https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-05-7#comment\n",
      "title= 比OpenAI官方提示词指南更全，这26条黄金准则让LLM性能飙升50%以上\n",
      "author= []\n",
      "publish_date= 2024-02-05 00:00:00\n",
      "text= 今天，穆罕默德・本・扎耶德 人工智能 大学 VILA Lab 带来了一项关于如何更好地为不同规模的大模型书写提示词（prompt）的研究，让大模型性能在不需要任何额外训练的前提下轻松提升 50% 以上。该工作在 X (Twitter)、Reddit 和 LinkedIn 等平台上都引起了广泛的讨论和关注。\n",
      "\n",
      "论文地址: https://arxiv.org/abs/2312.16171\n",
      "\n",
      "Github地址: https://github.com/VILA-Lab/ATLAS\n",
      "\n",
      "论文标题：Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在以 ChatGPT 为首的大模型出来之后，为大 语言模型 设计提示词的研究已经成为一个重要的研究方向，包括 OpenAI 官方也出品了针对 ChatGPT 用户的提示工程指南 [1] ，其包含了六条书写准则：1）写出清晰的指令；2）提供参考文本；3）将复杂的任务拆分为更简单的子任务；4）给模型时间「思考」；5）使用外部工具；6）系统地测试更改。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "由此可见，提示词对于如何更好地使用大模型以及得到满意的回答都具有重要的意义。然而可以看到的是，OpenAI 提供的这些准则都是比较宽泛和保守的，并没有涉及到一些具体的操作和技巧。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "今天要介绍的这篇文章提供了更多也更加接地气的提示工程指南，足足有 26 条之多，内容涵盖了：1）回答内容和语言风格的控制；2）提示词结构和清晰度；3）复杂任务和代码提示；4）回答特异性和信息量；5）用户交互和参与等多个方面。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "下面让我们来逐条讨论一下这些提示词准则：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1) 如果您更喜欢更简洁的答案，则无需对 LLM 保持礼貌，因此无需添加诸如 「请」、「如果你不介意」、「谢谢」、「我愿意」等，直奔主题即可。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2) 在提示中融入目标受众，例如该领域的专家。具体而言，当你告诉大模型你的目标受众是一个孩子，它的回答会更加通俗易懂，当你告诉它受众是这个领域的专家，它会提供更加专业和深入的解释。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3) 在交互式对话中将复杂的任务分解为一系列更简单的提示。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4) 使用肯定的指令，如「做」，同时避免使用「不要」等否定性语言。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5) 当您需要简单清晰或更深入地了解某个主题、想法或任何信息时，请利用以下提示：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "用简单的术语解释 [插入特定主题]。\n",
      "\n",
      "像我是 11 岁一样向我解释这个问题。\n",
      "\n",
      "向我解释，就好像我是 [领域] 的初学者一样。\n",
      "\n",
      "用简单的英语写 [文章 / 文本 / 段落]，就像你在向一个 5 岁的孩子解释一些事情一样。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6) 添加「我要给 $xxx 小费以获得更好的解决方案！」这种提示词会带来提升的原因可能是：在训练数据中，当涉及到回答是有奖励的，回答的人往往会更加准确细致，小心谨慎地提供答案，大模型从这些网络数据中学到了这些结构和方式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7) 实现示例驱动的提示（使用少样本提示）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8) 格式化提示时，以「###Instruction###」开头，然后是「###Example###」 或「###Question###」（如果相关）。随后展示您的内容。使用一个或多个换行符用于分隔指令、示例、问题、上下文和输入数据。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9) 在你的提示词里面加入以下短语：「你的任务是」和「你必须」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10) 在你的提示词里面加入以下短语：「你会受到惩罚」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11) 在提示中使用「以自然、类似人类的方式回答问题」这句话。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12) 使用引导性词语，例如写「一步一步地思考」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13) 在提示中加上以下短语：「确保你的回答是公正的，避免依赖刻板印象」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14) 让模型通过向你提问来引出你精确的细节和要求，直到他得到足够的信息来提供所需的输出（例如，「从现在开始，我希望你问我......」提问）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15) 要询问特定主题或想法或任何信息，并且您想测试您的理解，您可以使用 以下短语：「教我任何 [定理 / 主题 / 规则名称]，并在末尾包含一个测试，并让我知道是否在我回答后，我的答案是正确的，不要事先提供答案。 」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "16) 为大型 语言模型 分配角色。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17) 使用分隔符。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "18) 在提示中多次重复特定单词或短语。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19) 将思维链 （CoT） 与 few-Shot 提示相结合。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "20) 使用输出引导，包括用所需输出的开头结束提示。利用输出引导，以预期响应的开头结束提示。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "21) 如果任务是写一篇文章 / 文本 / 段落或任何类型的文本，同时需要尽可能的详细，可以添加提示词：「写一篇详细的 [论文 / 文本 / 段落]，通过添加所有必要的信息从而使我能详细了解 [主题]。 」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "22) 在不改变其样式的情况下更正 / 更改特定文本：尝试修改用户发送的每个段落。你应该只提高用户的语法和词汇量，并确保它听起来很自然。您应该保留原始写作风格，确保正式段落保持正式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "23) 当您有一个复杂的编程提示时，该提示可能位于不同的文件中：「从现在开始，每当您生成跨越多个文件的代码，生成一个可以自动运行的 [编程语言 ] 脚本，创建指定的文件或对现有文件进行更改以插入生成的代码。[你的问题]」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "24) 当您想使用特定单词、短语或句子开始或继续文本时，请使用以下方法提示：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我为你提供开头 [歌词 / 故事 / 段落 / 散文...]：[插入歌词 / 单词 / 句子]。根据提供的单词完成它。保持内容风格一致。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "25) 明确说明模型必须遵循的要求去生成内容， 以关键字、规定、提示或说明的形式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "26) 如果要编写任何文本，例如文章或段落，并且需要与提供的示例相似，请包括下面提示语句：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "根据提供的段落使用相同的语言 [/title/text/essay/answer]。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "以下是一些具体的提示词例子和对应的 GPT-4 输出结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. 当我们询问 GPT-4 问题的时候，最后可以加上一句「提供公正的解释，突出科学证据和不同观点。」可以看到加上该提示词之后 GPT-4 的回答明显会更加丰富和有深度。\n",
      "\n",
      "2. 我们可以提供一些示例让模型更好的理解我们的目标和出发点。\n",
      "\n",
      "3. 我们可以告诉模型用简单的方法来回答问题，就像是在向一个 5 岁的孩子解释一些事情。可以看到加上和不加这个提示词，模型的回复在理解困难程度上有明显的差别。\n",
      "\n",
      "4. 我们可以通过给模型小费的方式，让模型更加严谨完善的回答问题。\n",
      "\n",
      "定量实验结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. 模型回答质量提升比例：该指标表示在使用提示词原则后，问题的回答质量提高的百分比。\n",
      "\n",
      "可以看到所有提示词原则在人工评测中都取得了或多或少的提升，其中原则 14 获得了 100% 的提升，意味着所有问题通过使用该提示原则都获得了提升。与此同时，原则 1 得到的提升相对较少。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2. 回复正确性提升：正确性指模型输出或回答的精度，判断标准是回答是否准确、相关且没有错误的。本文同时考虑了不同模型的绝对正确性和相对正确性提升两个指标。\n",
      "\n",
      "上图结果为加入提示原则后，大模型回复质量的相对正确性提升。「small」表示 7B 模型，「medium」表示 13B 模型，「large scale」表示 70B 和 GPT-3.5/4 模型。可以看到大模型在使用提示词原则后，提升幅度相对于小模型和中等模型会更加显著。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3. 单独每个模型准确度提升比例：\n",
      "\n",
      "上图是每个不同大小的模型相对提升幅度，可以看到类似的现象，模型越大，对于提示词的响应和回复也越加敏锐，准确性提升也相对越大。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4. 下图是不同大小模型对于每条提示词原则准确度提升大小具体结果：\n",
      "\n",
      "提示词准则数据集：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "本文在介绍 26 条提示词准则的同时，还附带发布了一个基于准则提示词的 基准 ，其中每条准则作者准备了 20 个不同的问题，每个问题同时包含带有准则和不带准则两种对应的大模型回复。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "该数据集可以用在：1）大 语言模型 对于提示词响应的性能评测；2）偏好驱动的大模型微调。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "数据集链接：https://github.com/VILA-Lab/ATLAS。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "更多提示词原则使用方法和说明，欢迎阅读原文。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考文献\n",
      "\n",
      "[1] Six strategies for getting better results. OpenAI. https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-02-7\n",
      "title= 扎克伯格分红7亿刀，Meta股价大涨14%，开源大计成了​？\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= Meta 正在走出阴霾。\n",
      "\n",
      "伴随着 Meta 的股价周四盘后上涨近 14%，升至历史新高，这家公司宣布了有史以来的首次股息派发。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最近一次财报电话会议公布内容显示，Meta 公布的 2023 全年营收为 1349 亿美元，较 2022 年增长 16%；净利润为 391 亿美元，同比增长 69%。其中，第四季度营收为 401 亿美元，超出预期的 391.8 亿美元，同比增长 25%。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从 3 月份开始，Meta 将按季度向 A 类和 B 类普通股派发现金股息 50 美分。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "根据彭博社汇编的数据，首席执行官扎克伯格持有约 3.5 亿股股票（Meta 13% 的股份），他将从每季度派发的股息中获得约 1.75 亿美元的税前收入，一年下来约有 7 亿美元。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "小扎表示：「随着社区和业务持续增长，我们度过了一个不错的季度。」\n",
      "\n",
      "虽然这家公司在 2022 年经历了市值的低谷，但 Meta 股票在过去 12 个月里上涨了 168%。公司市值已重回万亿美元行列，扎克伯格的净资产更是达到了 1420 亿美元。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "与其他一些大型科技公司的创始人不同，扎克伯格这位首席执行官从未将权力移交给其他人，并一直在元宇宙和 人工智能 等趋势上进行长期押注。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "过去这段日子，Meta 在 人工智能 领域的表现毫不逊色：生成式 人工智能 大 语言模型 Llama 可与 OpenAI 和谷歌的模型竞争，Meta 同时在寻求将 人工智能 工具集成到其核心产品中，据传还计划使用自研芯片为其 人工智能 系统提供动力。在不久前的一段视频中，扎克伯格暗示了 Meta 开发通用 人工智能 (AGI) 的宏伟计划。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在好于预期的业绩之外，这些消息似乎也提振了外界对于 Meta 的信心。看来，Meta 正在走出阴霾。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "凭借 Llama，Meta 正在 AI 领域开辟新战场\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如果时间回到一年多以前，或许谁也没有想到，曾经深陷元宇宙泥沼的 Meta 能凭借一组开源模型重新扳回一局，甚至在 OpenAI、谷歌主导的 AI 闭源世界之外重新开辟了一个战场。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个战场以 Meta 开源的 Llama 系列模型为基础，成员异常活跃。在其基础上微调的 AI 大模型很多都宣称已经超过 GPT-3.5，甚至已经逼近 GPT-4（比如最近从 Mistral 公司泄露出的「Miqu」模型，该模型接近 GPT-4 性能，是 Mistral 基于 Llama 2 为自家客户训练的早期版本）。\n",
      "\n",
      "与此同时，Meta 开源模型在业界的影响力也在逐渐显现。很多国际大公司，比如富国银行、 IBM ，如今都部署了基于 Llama 2 的开源模型。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "对于为什么选开源路线，Meta 在最近的财报电话会上给出了回应。Meta 坦言，他们确实从开源这条路上收获了很多，包括基础模型的改进和模型到产品的过渡等多个方面。而且，他们认为，成员开源的领导者有几个战略优势：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先，开源软件通常更安全、更可靠，同时由于社区的持续反馈、审查和开发，运行效率更高。这是一个大问题，因为安全性是 人工智能 中最重要的问题之一。效率的提高和计算成本的降低也让包括 Meta 在内的每个人都受益。\n",
      "\n",
      "其次，开源软件通常会成为行业标准。Meta 表示，当其他公司使用 Meta 的技术栈进行标准化构建时，Meta 就能很容易地将其他公司的创新整合到自己的产品中。这很微妙，但是快速学习和改进的能力是一个巨大的优势，并且成为行业标准使得这种做法成为可能。\n",
      "\n",
      "最后，开源在开发者和研究人员中非常受欢迎，这有助于 Meta 招募到最好的人才。\n",
      "\n",
      "不过，要让这些效果完全显现可能还要等一段时间，毕竟 Meta 开源系列模型的发布时间要晚于 ChatGPT。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在上个月发布的一个视频中，扎克伯格表示，他们正在全力训练 Llama 3，而且未来会坚持走开源路线。为了训练模型，他们将购买 35 万块英伟达 H100 GPU，而且自研芯片也提上了日程。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了算力，Meta 也在造芯\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "据路透社报道，Meta 正在研发一款面向数据中心的定制芯片「Artemis」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这算是 Meta 内部 人工智能 芯片项目的一个积极转折，在此之前，Meta 高管曾在 2022 年的公司财务紧缩时期决定停止对自研芯片的投入。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "但在去年，Meta 先是公布了自研的第一代训练与推理加速器 MTIA。Artemis 是 Meta 内部芯片生产线的第二代产品，有助于减少 Meta 对英伟达芯片的依赖，同时控制与运行生成式 AI 工作负载相关的成本。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究机构 SemiAnalysis 创始人 Dylan Patel 表示，按照 Meta 的运营规模，成功部署自己的芯片有可能每年节省数亿美元的能源成本和数十亿美元的芯片采购成本。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "运行 人工智能 应用所需的芯片、基础设施和能源已成为科技公司投资的巨大洼地，在一定程度上抵消了围绕这波技术热潮所带来的收益。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "一位 Meta 发言人证实了在 2024 年投产更新芯片的计划，并表示它将与该公司正在购买的数十万个现成的 GPU 协同工作。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "发言人的声明中如此表示：「我们认为，我们内部开发的加速器与市面上的 GPU 有很强的互补性，可以在 Meta 特定的工作负载上提供性能和效率的最佳组合。」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "或许大家都还记得，小扎前不久发布过一段短视频，曾表示公司计划在 2024 年底前从英伟达采购大约 35 万台旗舰 H100 处理器，加上其他供应商，Meta 将总共拥有相当于 60 万个 H100 的计算能力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "过去几年，生成式 AI 的兴起某种程度上得益于英伟达先进的 GPU。反过来，由于供不应求，H100 变得备受追捧且极其昂贵，使英伟达首次跻身万亿美元市值公司的行列。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这种态势还促使微软、Meta、OpenAI、亚马逊和谷歌等科技巨头开始开发自己的 AI 处理器。与此同时，英伟达、 AMD 和 英特尔 等芯片制造商也陷入了军备竞赛，以发布更新、更高效、更强大的芯片。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "随着生成式 AI 服务需求的持续增长，芯片显然将成为科技公司的下一个重点战场。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在开源 AI 大模型上的突飞猛进，加上算力等方面的积极动作，Meta 在科技圈的形象正在重塑。当然，在 2024 年的竞争中，Meta 也将迎来更加严酷的考验。你看好这家公司吗？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考链接：\n",
      "\n",
      "https://www.reuters.com/technology/meta-deploy-in-house-custom-chips-this-year-power-ai-drive-memo-2024-02-01/\n",
      "\n",
      "https://www.businessinsider.com/metas-profits-first-ever-dividend-2024-2 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-02-7#comment\n",
      "title= 扎克伯格分红7亿刀，Meta股价大涨14%，开源大计成了​？\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= Meta 正在走出阴霾。\n",
      "\n",
      "伴随着 Meta 的股价周四盘后上涨近 14%，升至历史新高，这家公司宣布了有史以来的首次股息派发。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最近一次财报电话会议公布内容显示，Meta 公布的 2023 全年营收为 1349 亿美元，较 2022 年增长 16%；净利润为 391 亿美元，同比增长 69%。其中，第四季度营收为 401 亿美元，超出预期的 391.8 亿美元，同比增长 25%。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从 3 月份开始，Meta 将按季度向 A 类和 B 类普通股派发现金股息 50 美分。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "根据彭博社汇编的数据，首席执行官扎克伯格持有约 3.5 亿股股票（Meta 13% 的股份），他将从每季度派发的股息中获得约 1.75 亿美元的税前收入，一年下来约有 7 亿美元。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "小扎表示：「随着社区和业务持续增长，我们度过了一个不错的季度。」\n",
      "\n",
      "虽然这家公司在 2022 年经历了市值的低谷，但 Meta 股票在过去 12 个月里上涨了 168%。公司市值已重回万亿美元行列，扎克伯格的净资产更是达到了 1420 亿美元。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "与其他一些大型科技公司的创始人不同，扎克伯格这位首席执行官从未将权力移交给其他人，并一直在元宇宙和 人工智能 等趋势上进行长期押注。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "过去这段日子，Meta 在 人工智能 领域的表现毫不逊色：生成式 人工智能 大 语言模型 Llama 可与 OpenAI 和谷歌的模型竞争，Meta 同时在寻求将 人工智能 工具集成到其核心产品中，据传还计划使用自研芯片为其 人工智能 系统提供动力。在不久前的一段视频中，扎克伯格暗示了 Meta 开发通用 人工智能 (AGI) 的宏伟计划。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在好于预期的业绩之外，这些消息似乎也提振了外界对于 Meta 的信心。看来，Meta 正在走出阴霾。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "凭借 Llama，Meta 正在 AI 领域开辟新战场\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如果时间回到一年多以前，或许谁也没有想到，曾经深陷元宇宙泥沼的 Meta 能凭借一组开源模型重新扳回一局，甚至在 OpenAI、谷歌主导的 AI 闭源世界之外重新开辟了一个战场。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个战场以 Meta 开源的 Llama 系列模型为基础，成员异常活跃。在其基础上微调的 AI 大模型很多都宣称已经超过 GPT-3.5，甚至已经逼近 GPT-4（比如最近从 Mistral 公司泄露出的「Miqu」模型，该模型接近 GPT-4 性能，是 Mistral 基于 Llama 2 为自家客户训练的早期版本）。\n",
      "\n",
      "与此同时，Meta 开源模型在业界的影响力也在逐渐显现。很多国际大公司，比如富国银行、 IBM ，如今都部署了基于 Llama 2 的开源模型。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "对于为什么选开源路线，Meta 在最近的财报电话会上给出了回应。Meta 坦言，他们确实从开源这条路上收获了很多，包括基础模型的改进和模型到产品的过渡等多个方面。而且，他们认为，成员开源的领导者有几个战略优势：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先，开源软件通常更安全、更可靠，同时由于社区的持续反馈、审查和开发，运行效率更高。这是一个大问题，因为安全性是 人工智能 中最重要的问题之一。效率的提高和计算成本的降低也让包括 Meta 在内的每个人都受益。\n",
      "\n",
      "其次，开源软件通常会成为行业标准。Meta 表示，当其他公司使用 Meta 的技术栈进行标准化构建时，Meta 就能很容易地将其他公司的创新整合到自己的产品中。这很微妙，但是快速学习和改进的能力是一个巨大的优势，并且成为行业标准使得这种做法成为可能。\n",
      "\n",
      "最后，开源在开发者和研究人员中非常受欢迎，这有助于 Meta 招募到最好的人才。\n",
      "\n",
      "不过，要让这些效果完全显现可能还要等一段时间，毕竟 Meta 开源系列模型的发布时间要晚于 ChatGPT。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在上个月发布的一个视频中，扎克伯格表示，他们正在全力训练 Llama 3，而且未来会坚持走开源路线。为了训练模型，他们将购买 35 万块英伟达 H100 GPU，而且自研芯片也提上了日程。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了算力，Meta 也在造芯\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "据路透社报道，Meta 正在研发一款面向数据中心的定制芯片「Artemis」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这算是 Meta 内部 人工智能 芯片项目的一个积极转折，在此之前，Meta 高管曾在 2022 年的公司财务紧缩时期决定停止对自研芯片的投入。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "但在去年，Meta 先是公布了自研的第一代训练与推理加速器 MTIA。Artemis 是 Meta 内部芯片生产线的第二代产品，有助于减少 Meta 对英伟达芯片的依赖，同时控制与运行生成式 AI 工作负载相关的成本。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究机构 SemiAnalysis 创始人 Dylan Patel 表示，按照 Meta 的运营规模，成功部署自己的芯片有可能每年节省数亿美元的能源成本和数十亿美元的芯片采购成本。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "运行 人工智能 应用所需的芯片、基础设施和能源已成为科技公司投资的巨大洼地，在一定程度上抵消了围绕这波技术热潮所带来的收益。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "一位 Meta 发言人证实了在 2024 年投产更新芯片的计划，并表示它将与该公司正在购买的数十万个现成的 GPU 协同工作。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "发言人的声明中如此表示：「我们认为，我们内部开发的加速器与市面上的 GPU 有很强的互补性，可以在 Meta 特定的工作负载上提供性能和效率的最佳组合。」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "或许大家都还记得，小扎前不久发布过一段短视频，曾表示公司计划在 2024 年底前从英伟达采购大约 35 万台旗舰 H100 处理器，加上其他供应商，Meta 将总共拥有相当于 60 万个 H100 的计算能力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "过去几年，生成式 AI 的兴起某种程度上得益于英伟达先进的 GPU。反过来，由于供不应求，H100 变得备受追捧且极其昂贵，使英伟达首次跻身万亿美元市值公司的行列。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这种态势还促使微软、Meta、OpenAI、亚马逊和谷歌等科技巨头开始开发自己的 AI 处理器。与此同时，英伟达、 AMD 和 英特尔 等芯片制造商也陷入了军备竞赛，以发布更新、更高效、更强大的芯片。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "随着生成式 AI 服务需求的持续增长，芯片显然将成为科技公司的下一个重点战场。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在开源 AI 大模型上的突飞猛进，加上算力等方面的积极动作，Meta 在科技圈的形象正在重塑。当然，在 2024 年的竞争中，Meta 也将迎来更加严酷的考验。你看好这家公司吗？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考链接：\n",
      "\n",
      "https://www.reuters.com/technology/meta-deploy-in-house-custom-chips-this-year-power-ai-drive-memo-2024-02-01/\n",
      "\n",
      "https://www.businessinsider.com/metas-profits-first-ever-dividend-2024-2 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-02-6\n",
      "title= 比肩GPT-4，商汤日日新大幅升级4.0，多模态能力领先一步\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= 大模型的未来时刻，已经来了？\n",
      "\n",
      "速度太快了。\n",
      "\n",
      "商汤一下子把多模态大模型的发展进度条，快进到了落地阶段。\n",
      "\n",
      "商汤的大模型体系「日日新 SenseNova」今天刚刚发布了 4.0 版，不论语言能力还是文生图能力都有全面升级，还自带低门槛的落地工具。\n",
      "\n",
      "新一代 SenseNova 不仅在大 语言模型 、文生图模型等方面进行了重大升级，部分垂直领域能力超越 GPT-4，还发布了全新多模态大模型，并面向 数据分析 、医疗等场景提供了全新版本，让大模型通用能力适配到了更多领域。\n",
      "\n",
      "API 申请网址：https://platform.sensenova.cn/\n",
      "\n",
      "与此同时，商汤还推出了日日新・商量大 语言模型 Function call & Assistants API 版本，除了对话能力外，还支持多种内置工具，包括图片生成 (文生图)、智能识图 (图生文)、 数据分析 （代码解释器）、在线检索。\n",
      "\n",
      "这是全球首个支持了文生图、图生文，并可支持不同模态工具调用的工具，跑在了 OpenAI 的前面。\n",
      "\n",
      "这一系列发布，从技术进步到落地「两翼齐飞」，可谓把通用大模型技术卷上了天。看来在技术竞争中，国内科技公司逐渐有了反超的趋势。\n",
      "\n",
      "最高支持 128k 长窗口\n",
      "\n",
      "商量 SenseChat 测试全方位比肩 GPT-4\n",
      "\n",
      "自 ChatGPT 出现以来，大模型成为了 AI 赛道的主力军。商汤的大模型体系正在「大模型 + 大装置」的战略布局下快速迭代。\n",
      "\n",
      "去年 4 月，商汤公布了「日日新 SenseNova」大模型体系，一上来就在 自然语言处理 、文生图创作、数字人生成、3D 场景和物体生成，自动化数据标注、自定义模型训练等多个领域全面发力。\n",
      "\n",
      "与此同时，商汤还直接提供图片生成、自然语言对话、 视觉推理 和标注服务的 API 接口。\n",
      "\n",
      "此后，该大模型体系持续推陈出新，在基础能力、API 服务、模型应用等多个方面不断进步，给用户和开发者们带来了越来越好用的技术。\n",
      "\n",
      "如今，近 10 个月过去了，商汤新一代「日日新 SenseNova 4.0」在 2024 年的新春之际与大家见面了， 不仅对已有多个大模型进行全方位升级，还有一些「新面孔」。\n",
      "\n",
      "升级之后，日日新在长文本理解、综合推理（包括数字推理）、代码生成、多模态交互等整体表现上「更上一层楼」，不仅全面超越了 GPT-3.5，并且大部分接近甚至超越了 GPT-4 系列模型。\n",
      "\n",
      "用下面一组核心数据说话，SenseNova 4.0 的：\n",
      "\n",
      "推理能力：达到 GPT-4 Turbo 的 99%；\n",
      "\n",
      "代码能力：在 HumanEval 代码生成 基准 测试上 准确率 达到 75.6，超越 GPT-4（74.4）；\n",
      "\n",
      "多模态能力：在 MMBench 多模态大 语言模型 综合评估 基准 上的整体性能超越了 GPT-4V（84.4 vs 74.4）；\n",
      "\n",
      "数据分析 能力：正确率（85.71%）超越 GPT-4（84.62%）；\n",
      "\n",
      "在部分垂直领域能力超越 GPT-4 Turbo。\n",
      "\n",
      "而日日新全维度、无死角的能力飙升，首要归功于商量大 语言模型 SenseChat 的重大升级。\n",
      "\n",
      "此次发布的商量大 语言模型 -通用版本（SenseChat V4） 在整体能力比肩 GPT-4，并相较于 GPT-3.5 实现显著超越。如下两图为 SenseChat V4 与GPT-3.5、GPT-4 在整体、考试、语言、知识、推理、数理、编程等数据集上的性能比较数据。\n",
      "\n",
      "至于为何能有如此明显的性能提升，SenseChat 4.0 在以下多个方面获得了加强。\n",
      "\n",
      "首先是更全面的知识覆盖，新增了包括业务通用数据、数学能力数据、K12 考试数据、文学期刊数据等在内约 600B tokens 的中英文预训练语料，这样理解多领域内容更加得心应手。同时，模型质量也通过数据清洗和增强得到进一步提高。\n",
      "\n",
      "其次推理能力变得更加可靠。从初始 1.0 版本以来，前后四次超强预训练的积累让模型在阅读理解、综合推理、代码能力等多项任务上实现了 5%-10% 的定向性提升。\n",
      "\n",
      "最后也是此次 4.0 版本升级的重点 —— 更强的长文本理解分析能力，更新了 3 种不同上下文窗口的全新模型，即 SenseChat-4K、SenseChat-32k 和 SenseChat-128k，不仅使得模型理解上下文的能力迎来史诗级加强，还提升了模型的适应能力，拓宽了应用范围，为用户提供根据需求自由选择模型的机会。\n",
      "\n",
      "在与 GPT-3.5、GPT-4 的多任务较量中，我们直观地看到了 SenseChat 不同上下文窗口版本的真正实力。\n",
      "\n",
      "其中，SenseChat-4K 虽然支持最少的 4k tokens（约 4000 中文字）的输入和输出，但仍然在写作总结、知识问答、闲聊娱乐、专业技能、安全测试等主客观题和安全性能上超越了 GPT-4。另外，新增的引文功能还可以返回在 线搜索 的知识来源。\n",
      "\n",
      "SenseChat-32k 则能够处理 32k tokens（约 3 万中文字）的长文本总结，总能力平均得分达到了同等上下文窗口 GPT-4-32k 能力的 90% 以上水平，中文理解能力则超越了后者。\n",
      "\n",
      "铺开来讲，SenseChat-32k 在平均考试能力和理解能力、以及 HellaSwag、C3、LAMBADA、CHID 等推理和理解类测试集中超越 GPT-4-32k；在 LongBench 长文本理解测试 基准 以及 tpo、multidocqa、scientificqa、PassageRetrieval-zh 等长文本测试集上均超越了 GPT-4–32k。\n",
      "\n",
      "对于支持最长 128k tokens（约 12 万以上中文字）长文本的 SenseChat-128k，它的中文理解能力也超过了 GPT-4 的水平。\n",
      "\n",
      "下表 1 和 2 分别为 SenseChat 三个版本模型与 GPT 系列在长文本理解和推理等测试集上的平均得分比较。\n",
      "\n",
      "表 1：Normalbench v1-4 万题对比结果。\n",
      "\n",
      "表 2：长文本 Leval 和 Longbench 测试集对比结果。\n",
      "\n",
      "看起来，SenseChat V4 不仅在主客观题方面达到了 GPT-4 的水平，更在长文本理解和推理能力上实现了全面超越。\n",
      "\n",
      "作为商汤「日日新 SenseNova」大模型体系的通用基础模型，SenseChat V4 的大幅度升级使得人们在使用模型处理多样化语言任务时更高效、更准确，让国产大模型拥有不输于 GPT-4 的使用体验。\n",
      "\n",
      "对于更多人来说，未来在商量 SenseChat 大 语言模型 的基础上开展学术研究、技术创新、商业应用也有了更多机会。\n",
      "\n",
      "填补行业空缺，打造专用大模型\n",
      "\n",
      "首家开放支持多模态的 Assistants API\n",
      "\n",
      "基础模型之外，商汤也希望能通过高效融合垂直领域知识，帮助人们构建各类专业大模型，降低大模型的下游应用成本和门槛。\n",
      "\n",
      "多模态是 人工智能 大模型重要的技术演进方向，新一代「日日新 SenseNova」推出了拥有 300 亿 参数 的日日新·商量多模态大模型（SenseChat-Vision V4），其图文 感知 能力处于全球领先水平，在权威评测 基准 测试集 MME Benchmark 上综合得分排名首位。\n",
      "\n",
      "目前，该模型可以支持智能驾驶、智能车舱、电力行业等多个实际场景的应用。\n",
      "\n",
      "与常规的 OCR 能力不同，它不仅可以理解图中的文字和物体，并且可以根据 逻辑 进行推理，实现了一定程度的认知能力。\n",
      "\n",
      "在办公与 数据分析 领域，商汤推出了日日新·商量语言大模型- 数据分析 版本（SenseChat-DataAnalysisCode V4），它可以通过自然语言输入，结合商汤大模型的 意图识别 、 逻辑 理解与代码解释器的能力，自动将数据转化为有意义的分析和可视化结果。\n",
      "\n",
      "目前，该工具已经支持 xls、xlsx、csv、txt、json 等格式的文件和表格处理。就实际效果而言，办公小浣熊在 1000 + 测试集精度上略胜于 GPT-4。\n",
      "\n",
      "体验入口：https://raccoon.sensetime.com/office\n",
      "\n",
      "在医疗健康领域，大 语言模型 的医疗版本也有全新升级，日日新·商量语言大模型-医疗版本“大医”（SenseChat-Medical V4）在本次更新后可以有效实现专业医学问答及复杂医学任务推理，并支持更多模态医学文件的智能解读和交互问答。据介绍，“大医”在两项行业权威评测 —— 2023 年职业药剂师考试大模型评测和中文医疗大 语言模型 开放评测平台 MedBench 中，均实现综合评分排名第二，性能接近 GPT-4。\n",
      "\n",
      "商汤自研的日日新-秒画文生图大模型（SenseMirage V4）较此前版本， 参数 量提升至百亿量级，通过 Mixture of text experts、Spatial-aware CFG 等算法优化，语义理解能力与图像质感细节表现显著增强，可达成电影级海报生成水平。同时结合 Adversarial Distillation 算法，秒画 SenseMirage-Turbo V4 也对外发布，相较于基础版本，可达到 10 倍推理加速效果。\n",
      "\n",
      "秒画一键生成电影海报级的精美图像\n",
      "\n",
      "再进一步，商汤还把调用不同模态的能力，做到了一个端口上，这就是全球首个支持调用不同模态的 Assistants API。\n",
      "\n",
      "去年 11 月，OpenAI 在其首届开发者大会上推出专门构建的 AI 工具 ——Assistants API，通过代码解释器、检索和函数调用等新功能帮助开发者构建高质量的 AI 应用。不过，至今这个工具也没有支持构建视觉相关的多模态应用。\n",
      "\n",
      "商汤提出的 Assistants API 填补了这一空缺。作为一个基于商量大 语言模型 构建的、具有状态的多轮对话接口，它不仅首次支持了文生图、图生文的不同模态工具调用，还内置 数据分析 、搜索引擎工具。\n",
      "\n",
      "如果把大模型看作是大脑，Assistants API 相当于给 AI 增加了眼睛和手，能够自主理解人类下达的任务，并做出正确 规划 ，使用合适的资源和工具。Assistants API 提供了一个桥梁，将先进的大模型与各类应用服务工具连接起来，支持图文结合的多模态交互和代码执行结果的直观呈现，可以帮助人们快速解决复杂的问题。\n",
      "\n",
      "目前，商汤的大模型体系已经在全面落地。在全行业层面上，自发布以来已经拥有了超过 3000 家企业用户，累积调用量已达近 9000 万次，服务的行业包含互联网娱乐、游戏、文娱、教育、医疗健康、金融、编程等方面。\n",
      "\n",
      "结语\n",
      "\n",
      "还记得去年的「百模大战」吗？现在，科技领域的大模型军备竞赛形势已经有了改变，竞争不再是单纯的模型技术，而变成了拼体系 —— 除了模型技术的升级改进，各家厂商正在整合与调优基础底座，开放的趋势也在催生出逐渐繁荣的生态。\n",
      "\n",
      "如今，战火已经燃烧到了多模态技术的落地上。能够睁开眼睛看世界的大模型，为我们带来了更多的想象力。\n",
      "\n",
      "而为了在千行百业中用好它们，真正实现「重做所有产品」，一套完整的体系势必能让我们事半功倍。\n",
      "\n",
      "在这一方面，商汤已经做到了更好。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-02-6#comment\n",
      "title= 比肩GPT-4，商汤日日新大幅升级4.0，多模态能力领先一步\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= 大模型的未来时刻，已经来了？\n",
      "\n",
      "速度太快了。\n",
      "\n",
      "商汤一下子把多模态大模型的发展进度条，快进到了落地阶段。\n",
      "\n",
      "商汤的大模型体系「日日新 SenseNova」今天刚刚发布了 4.0 版，不论语言能力还是文生图能力都有全面升级，还自带低门槛的落地工具。\n",
      "\n",
      "新一代 SenseNova 不仅在大 语言模型 、文生图模型等方面进行了重大升级，部分垂直领域能力超越 GPT-4，还发布了全新多模态大模型，并面向 数据分析 、医疗等场景提供了全新版本，让大模型通用能力适配到了更多领域。\n",
      "\n",
      "API 申请网址：https://platform.sensenova.cn/\n",
      "\n",
      "与此同时，商汤还推出了日日新・商量大 语言模型 Function call & Assistants API 版本，除了对话能力外，还支持多种内置工具，包括图片生成 (文生图)、智能识图 (图生文)、 数据分析 （代码解释器）、在线检索。\n",
      "\n",
      "这是全球首个支持了文生图、图生文，并可支持不同模态工具调用的工具，跑在了 OpenAI 的前面。\n",
      "\n",
      "这一系列发布，从技术进步到落地「两翼齐飞」，可谓把通用大模型技术卷上了天。看来在技术竞争中，国内科技公司逐渐有了反超的趋势。\n",
      "\n",
      "最高支持 128k 长窗口\n",
      "\n",
      "商量 SenseChat 测试全方位比肩 GPT-4\n",
      "\n",
      "自 ChatGPT 出现以来，大模型成为了 AI 赛道的主力军。商汤的大模型体系正在「大模型 + 大装置」的战略布局下快速迭代。\n",
      "\n",
      "去年 4 月，商汤公布了「日日新 SenseNova」大模型体系，一上来就在 自然语言处理 、文生图创作、数字人生成、3D 场景和物体生成，自动化数据标注、自定义模型训练等多个领域全面发力。\n",
      "\n",
      "与此同时，商汤还直接提供图片生成、自然语言对话、 视觉推理 和标注服务的 API 接口。\n",
      "\n",
      "此后，该大模型体系持续推陈出新，在基础能力、API 服务、模型应用等多个方面不断进步，给用户和开发者们带来了越来越好用的技术。\n",
      "\n",
      "如今，近 10 个月过去了，商汤新一代「日日新 SenseNova 4.0」在 2024 年的新春之际与大家见面了， 不仅对已有多个大模型进行全方位升级，还有一些「新面孔」。\n",
      "\n",
      "升级之后，日日新在长文本理解、综合推理（包括数字推理）、代码生成、多模态交互等整体表现上「更上一层楼」，不仅全面超越了 GPT-3.5，并且大部分接近甚至超越了 GPT-4 系列模型。\n",
      "\n",
      "用下面一组核心数据说话，SenseNova 4.0 的：\n",
      "\n",
      "推理能力：达到 GPT-4 Turbo 的 99%；\n",
      "\n",
      "代码能力：在 HumanEval 代码生成 基准 测试上 准确率 达到 75.6，超越 GPT-4（74.4）；\n",
      "\n",
      "多模态能力：在 MMBench 多模态大 语言模型 综合评估 基准 上的整体性能超越了 GPT-4V（84.4 vs 74.4）；\n",
      "\n",
      "数据分析 能力：正确率（85.71%）超越 GPT-4（84.62%）；\n",
      "\n",
      "在部分垂直领域能力超越 GPT-4 Turbo。\n",
      "\n",
      "而日日新全维度、无死角的能力飙升，首要归功于商量大 语言模型 SenseChat 的重大升级。\n",
      "\n",
      "此次发布的商量大 语言模型 -通用版本（SenseChat V4） 在整体能力比肩 GPT-4，并相较于 GPT-3.5 实现显著超越。如下两图为 SenseChat V4 与GPT-3.5、GPT-4 在整体、考试、语言、知识、推理、数理、编程等数据集上的性能比较数据。\n",
      "\n",
      "至于为何能有如此明显的性能提升，SenseChat 4.0 在以下多个方面获得了加强。\n",
      "\n",
      "首先是更全面的知识覆盖，新增了包括业务通用数据、数学能力数据、K12 考试数据、文学期刊数据等在内约 600B tokens 的中英文预训练语料，这样理解多领域内容更加得心应手。同时，模型质量也通过数据清洗和增强得到进一步提高。\n",
      "\n",
      "其次推理能力变得更加可靠。从初始 1.0 版本以来，前后四次超强预训练的积累让模型在阅读理解、综合推理、代码能力等多项任务上实现了 5%-10% 的定向性提升。\n",
      "\n",
      "最后也是此次 4.0 版本升级的重点 —— 更强的长文本理解分析能力，更新了 3 种不同上下文窗口的全新模型，即 SenseChat-4K、SenseChat-32k 和 SenseChat-128k，不仅使得模型理解上下文的能力迎来史诗级加强，还提升了模型的适应能力，拓宽了应用范围，为用户提供根据需求自由选择模型的机会。\n",
      "\n",
      "在与 GPT-3.5、GPT-4 的多任务较量中，我们直观地看到了 SenseChat 不同上下文窗口版本的真正实力。\n",
      "\n",
      "其中，SenseChat-4K 虽然支持最少的 4k tokens（约 4000 中文字）的输入和输出，但仍然在写作总结、知识问答、闲聊娱乐、专业技能、安全测试等主客观题和安全性能上超越了 GPT-4。另外，新增的引文功能还可以返回在 线搜索 的知识来源。\n",
      "\n",
      "SenseChat-32k 则能够处理 32k tokens（约 3 万中文字）的长文本总结，总能力平均得分达到了同等上下文窗口 GPT-4-32k 能力的 90% 以上水平，中文理解能力则超越了后者。\n",
      "\n",
      "铺开来讲，SenseChat-32k 在平均考试能力和理解能力、以及 HellaSwag、C3、LAMBADA、CHID 等推理和理解类测试集中超越 GPT-4-32k；在 LongBench 长文本理解测试 基准 以及 tpo、multidocqa、scientificqa、PassageRetrieval-zh 等长文本测试集上均超越了 GPT-4–32k。\n",
      "\n",
      "对于支持最长 128k tokens（约 12 万以上中文字）长文本的 SenseChat-128k，它的中文理解能力也超过了 GPT-4 的水平。\n",
      "\n",
      "下表 1 和 2 分别为 SenseChat 三个版本模型与 GPT 系列在长文本理解和推理等测试集上的平均得分比较。\n",
      "\n",
      "表 1：Normalbench v1-4 万题对比结果。\n",
      "\n",
      "表 2：长文本 Leval 和 Longbench 测试集对比结果。\n",
      "\n",
      "看起来，SenseChat V4 不仅在主客观题方面达到了 GPT-4 的水平，更在长文本理解和推理能力上实现了全面超越。\n",
      "\n",
      "作为商汤「日日新 SenseNova」大模型体系的通用基础模型，SenseChat V4 的大幅度升级使得人们在使用模型处理多样化语言任务时更高效、更准确，让国产大模型拥有不输于 GPT-4 的使用体验。\n",
      "\n",
      "对于更多人来说，未来在商量 SenseChat 大 语言模型 的基础上开展学术研究、技术创新、商业应用也有了更多机会。\n",
      "\n",
      "填补行业空缺，打造专用大模型\n",
      "\n",
      "首家开放支持多模态的 Assistants API\n",
      "\n",
      "基础模型之外，商汤也希望能通过高效融合垂直领域知识，帮助人们构建各类专业大模型，降低大模型的下游应用成本和门槛。\n",
      "\n",
      "多模态是 人工智能 大模型重要的技术演进方向，新一代「日日新 SenseNova」推出了拥有 300 亿 参数 的日日新·商量多模态大模型（SenseChat-Vision V4），其图文 感知 能力处于全球领先水平，在权威评测 基准 测试集 MME Benchmark 上综合得分排名首位。\n",
      "\n",
      "目前，该模型可以支持智能驾驶、智能车舱、电力行业等多个实际场景的应用。\n",
      "\n",
      "与常规的 OCR 能力不同，它不仅可以理解图中的文字和物体，并且可以根据 逻辑 进行推理，实现了一定程度的认知能力。\n",
      "\n",
      "在办公与 数据分析 领域，商汤推出了日日新·商量语言大模型- 数据分析 版本（SenseChat-DataAnalysisCode V4），它可以通过自然语言输入，结合商汤大模型的 意图识别 、 逻辑 理解与代码解释器的能力，自动将数据转化为有意义的分析和可视化结果。\n",
      "\n",
      "目前，该工具已经支持 xls、xlsx、csv、txt、json 等格式的文件和表格处理。就实际效果而言，办公小浣熊在 1000 + 测试集精度上略胜于 GPT-4。\n",
      "\n",
      "体验入口：https://raccoon.sensetime.com/office\n",
      "\n",
      "在医疗健康领域，大 语言模型 的医疗版本也有全新升级，日日新·商量语言大模型-医疗版本“大医”（SenseChat-Medical V4）在本次更新后可以有效实现专业医学问答及复杂医学任务推理，并支持更多模态医学文件的智能解读和交互问答。据介绍，“大医”在两项行业权威评测 —— 2023 年职业药剂师考试大模型评测和中文医疗大 语言模型 开放评测平台 MedBench 中，均实现综合评分排名第二，性能接近 GPT-4。\n",
      "\n",
      "商汤自研的日日新-秒画文生图大模型（SenseMirage V4）较此前版本， 参数 量提升至百亿量级，通过 Mixture of text experts、Spatial-aware CFG 等算法优化，语义理解能力与图像质感细节表现显著增强，可达成电影级海报生成水平。同时结合 Adversarial Distillation 算法，秒画 SenseMirage-Turbo V4 也对外发布，相较于基础版本，可达到 10 倍推理加速效果。\n",
      "\n",
      "秒画一键生成电影海报级的精美图像\n",
      "\n",
      "再进一步，商汤还把调用不同模态的能力，做到了一个端口上，这就是全球首个支持调用不同模态的 Assistants API。\n",
      "\n",
      "去年 11 月，OpenAI 在其首届开发者大会上推出专门构建的 AI 工具 ——Assistants API，通过代码解释器、检索和函数调用等新功能帮助开发者构建高质量的 AI 应用。不过，至今这个工具也没有支持构建视觉相关的多模态应用。\n",
      "\n",
      "商汤提出的 Assistants API 填补了这一空缺。作为一个基于商量大 语言模型 构建的、具有状态的多轮对话接口，它不仅首次支持了文生图、图生文的不同模态工具调用，还内置 数据分析 、搜索引擎工具。\n",
      "\n",
      "如果把大模型看作是大脑，Assistants API 相当于给 AI 增加了眼睛和手，能够自主理解人类下达的任务，并做出正确 规划 ，使用合适的资源和工具。Assistants API 提供了一个桥梁，将先进的大模型与各类应用服务工具连接起来，支持图文结合的多模态交互和代码执行结果的直观呈现，可以帮助人们快速解决复杂的问题。\n",
      "\n",
      "目前，商汤的大模型体系已经在全面落地。在全行业层面上，自发布以来已经拥有了超过 3000 家企业用户，累积调用量已达近 9000 万次，服务的行业包含互联网娱乐、游戏、文娱、教育、医疗健康、金融、编程等方面。\n",
      "\n",
      "结语\n",
      "\n",
      "还记得去年的「百模大战」吗？现在，科技领域的大模型军备竞赛形势已经有了改变，竞争不再是单纯的模型技术，而变成了拼体系 —— 除了模型技术的升级改进，各家厂商正在整合与调优基础底座，开放的趋势也在催生出逐渐繁荣的生态。\n",
      "\n",
      "如今，战火已经燃烧到了多模态技术的落地上。能够睁开眼睛看世界的大模型，为我们带来了更多的想象力。\n",
      "\n",
      "而为了在千行百业中用好它们，真正实现「重做所有产品」，一套完整的体系势必能让我们事半功倍。\n",
      "\n",
      "在这一方面，商汤已经做到了更好。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-01-6\n",
      "title= 赶超Gemini Pro，提升推理、OCR能力的LLaVA-1.6太强了\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 去年 4 月，威斯康星大学麦迪逊分校、微软研究院和哥伦比亚大学研究者共同发布了 LLaVA（Large Language and Vision Assistant）。尽管 LLaVA 是用一个小的多模态指令数据集训练的，却在一些样本上展示了与 GPT-4 非常相似的推理结果。10 月，LLaVA-1.5 重磅发布，通过对原始 LLaVA 的简单修改，在 11 个 基准 上刷新了 SOTA。\n",
      "\n",
      "现在，研究团队宣布推出 LLaVA-1.6，主要改进了模型在推理、OCR 和世界知识方面的性能。LLaVA-1.6 甚至在多项 基准 测试中超越了 Gemini Pro。\n",
      "\n",
      "demo 地址：https://llava.hliu.cc/\n",
      "\n",
      "项目地址：https://github.com/haotian-liu/LLaVA\n",
      "\n",
      "与 LLaVA-1.5 相比，LLaVA-1.6 有如下几个改进：\n",
      "\n",
      "将输入图像分辨率提升 4 倍，支持三种宽高比，最高可达 672x672、336x1344、1344x336 分辨率。这使得 LLaVA-1.6 能够掌握更多的视觉细节。\n",
      "\n",
      "通过改进的视觉指令调整数据混合，LLaVA-1.6 获得了更好的 视觉推理 和 OCR 能力。\n",
      "\n",
      "更好的视觉对话，更多场景，覆盖不同应用。LLaVA-1.6 掌握了更多世界知识，具备更好的 逻辑 推理能力。\n",
      "\n",
      "使用 SGLang 进行高效部署和推理。\n",
      "\n",
      "图源：https://twitter.com/imhaotian/status/1752621754273472927\n",
      "\n",
      "LLaVA-1.6 保持了 LLaVA-1.5 的极简设计和数据效率，它复用了 LLaVA-1.5 的预训练连接器，并且仍然使用不到 1M 的视觉指令调优样本。最大的 34B 模型使用 32 个 A100 在大约 1 天内完成了训练。LLaVA-1.6 使用 130 万个数据样本，计算 / 训练数据成本约为其他方法的 100-1000 分之一。\n",
      "\n",
      "与 CogVLM 或 Yi-VL 等开源 LMM 相比，LLaVA-1.6 实现了 SOTA 性能。与商用产品相比，LLaVA-1.6 在选定的 基准 测试中可以媲美 Gemini Pro，并且优于 Qwen-VL-Plus。\n",
      "\n",
      "值得一提的是，LLaVA-1.6 展现出强大的零样本（zero-shot）中文能力，它在多模态 基准 MMBench-CN 上取得了 SOTA 性能。\n",
      "\n",
      "方法改进\n",
      "\n",
      "动态高分辨率\n",
      "\n",
      "研究团队以高分辨率设计 LLaVA-1.6 模型，旨在保持其数据效率。当提供高分辨率图像和保留细节的表征时，模型 感知 图像中复杂细节的能力会显著提高。它减少了面对低分辨率图像时的模型幻觉，即猜测想象的视觉内容。\n",
      "\n",
      "数据混合\n",
      "\n",
      "高质量的用户指令数据。该研究对高质量视觉指令遵循数据的定义取决于两个主要标准：首先，任务指令的多样性，确保充分代表现实场景中可能遇到的广泛用户意图，特别是在模型部署阶段。其次，响应的优先级至关重要，旨在征求有利的用户反馈。\n",
      "\n",
      "因此，该研究考虑了两个数据源：\n",
      "\n",
      "现有的 GPT-V 数据 （LAION-GPT-V 和 ShareGPT-4V）；\n",
      "\n",
      "为了进一步促进更多场景下更好的视觉对话，研究团队收集了一个涵盖不同应用的小型 15K 视觉指令调优数据集，仔细过滤了可能存在隐私问题或可能有害的样本，并使用 GPT-4V 生成响应。\n",
      "\n",
      "多模态文档 / 图表数据。(1) 从训练数据中删除 TextCap，因为研究团队意识到 TextCap 使用与 TextVQA 相同的训练图像集。这使得研究团队能够在评估 TextVQA 时更好地了解模型的零样本 OCR 能力。为了保持并进一步提高模型的 OCR 能力，该研究用 DocVQA 和 SynDog-EN 替换了 TextCap。(2) 借助 Qwen-VL-7B-Chat，该研究进一步添加了 ChartQA、DVQA 和 AI2D，以更好地理解图和图表。\n",
      "\n",
      "研究团队还表示除了 Vicuna-1.5（7B 和 13B），还考虑采用更多 LLM 方案，包括 Mistral-7B 和 Nous-Hermes-2-Yi-34B，以使 LLaVA 能够支持更广泛的用户和更多的场景。\n",
      "\n",
      "参考链接：https://llava-vl.github.io/blog/2024-01-30-llava-1-6/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-01-6#comment\n",
      "title= 赶超Gemini Pro，提升推理、OCR能力的LLaVA-1.6太强了\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 去年 4 月，威斯康星大学麦迪逊分校、微软研究院和哥伦比亚大学研究者共同发布了 LLaVA（Large Language and Vision Assistant）。尽管 LLaVA 是用一个小的多模态指令数据集训练的，却在一些样本上展示了与 GPT-4 非常相似的推理结果。10 月，LLaVA-1.5 重磅发布，通过对原始 LLaVA 的简单修改，在 11 个 基准 上刷新了 SOTA。\n",
      "\n",
      "现在，研究团队宣布推出 LLaVA-1.6，主要改进了模型在推理、OCR 和世界知识方面的性能。LLaVA-1.6 甚至在多项 基准 测试中超越了 Gemini Pro。\n",
      "\n",
      "demo 地址：https://llava.hliu.cc/\n",
      "\n",
      "项目地址：https://github.com/haotian-liu/LLaVA\n",
      "\n",
      "与 LLaVA-1.5 相比，LLaVA-1.6 有如下几个改进：\n",
      "\n",
      "将输入图像分辨率提升 4 倍，支持三种宽高比，最高可达 672x672、336x1344、1344x336 分辨率。这使得 LLaVA-1.6 能够掌握更多的视觉细节。\n",
      "\n",
      "通过改进的视觉指令调整数据混合，LLaVA-1.6 获得了更好的 视觉推理 和 OCR 能力。\n",
      "\n",
      "更好的视觉对话，更多场景，覆盖不同应用。LLaVA-1.6 掌握了更多世界知识，具备更好的 逻辑 推理能力。\n",
      "\n",
      "使用 SGLang 进行高效部署和推理。\n",
      "\n",
      "图源：https://twitter.com/imhaotian/status/1752621754273472927\n",
      "\n",
      "LLaVA-1.6 保持了 LLaVA-1.5 的极简设计和数据效率，它复用了 LLaVA-1.5 的预训练连接器，并且仍然使用不到 1M 的视觉指令调优样本。最大的 34B 模型使用 32 个 A100 在大约 1 天内完成了训练。LLaVA-1.6 使用 130 万个数据样本，计算 / 训练数据成本约为其他方法的 100-1000 分之一。\n",
      "\n",
      "与 CogVLM 或 Yi-VL 等开源 LMM 相比，LLaVA-1.6 实现了 SOTA 性能。与商用产品相比，LLaVA-1.6 在选定的 基准 测试中可以媲美 Gemini Pro，并且优于 Qwen-VL-Plus。\n",
      "\n",
      "值得一提的是，LLaVA-1.6 展现出强大的零样本（zero-shot）中文能力，它在多模态 基准 MMBench-CN 上取得了 SOTA 性能。\n",
      "\n",
      "方法改进\n",
      "\n",
      "动态高分辨率\n",
      "\n",
      "研究团队以高分辨率设计 LLaVA-1.6 模型，旨在保持其数据效率。当提供高分辨率图像和保留细节的表征时，模型 感知 图像中复杂细节的能力会显著提高。它减少了面对低分辨率图像时的模型幻觉，即猜测想象的视觉内容。\n",
      "\n",
      "数据混合\n",
      "\n",
      "高质量的用户指令数据。该研究对高质量视觉指令遵循数据的定义取决于两个主要标准：首先，任务指令的多样性，确保充分代表现实场景中可能遇到的广泛用户意图，特别是在模型部署阶段。其次，响应的优先级至关重要，旨在征求有利的用户反馈。\n",
      "\n",
      "因此，该研究考虑了两个数据源：\n",
      "\n",
      "现有的 GPT-V 数据 （LAION-GPT-V 和 ShareGPT-4V）；\n",
      "\n",
      "为了进一步促进更多场景下更好的视觉对话，研究团队收集了一个涵盖不同应用的小型 15K 视觉指令调优数据集，仔细过滤了可能存在隐私问题或可能有害的样本，并使用 GPT-4V 生成响应。\n",
      "\n",
      "多模态文档 / 图表数据。(1) 从训练数据中删除 TextCap，因为研究团队意识到 TextCap 使用与 TextVQA 相同的训练图像集。这使得研究团队能够在评估 TextVQA 时更好地了解模型的零样本 OCR 能力。为了保持并进一步提高模型的 OCR 能力，该研究用 DocVQA 和 SynDog-EN 替换了 TextCap。(2) 借助 Qwen-VL-7B-Chat，该研究进一步添加了 ChartQA、DVQA 和 AI2D，以更好地理解图和图表。\n",
      "\n",
      "研究团队还表示除了 Vicuna-1.5（7B 和 13B），还考虑采用更多 LLM 方案，包括 Mistral-7B 和 Nous-Hermes-2-Yi-34B，以使 LLaVA 能够支持更广泛的用户和更多的场景。\n",
      "\n",
      "参考链接：https://llava-vl.github.io/blog/2024-01-30-llava-1-6/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-01-11\n",
      "title= 蚂蚁集团NextEvo全面开源AI Infra技术，可实现大模型训练“自动驾驶”\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 近日， 蚂蚁集团 AI创新研发部门NextEvo全面开源AI Infra技术，可帮助大模型千卡训练有效时间占比超过95%，能实现训练时“自动驾驶”，这推动了AI研发效率。\n",
      "\n",
      "图： 蚂蚁集团 的自动化分布式 深度学习 系统DLRover现已全面开源\n",
      "\n",
      "该技术框架名为DLRover，目标在于大规模分布式训练的智能化。目前很多企业的训练作业都是跑在混合部署的集群中，运行环境复杂多变，不管多么“崎岖的地形”，DLRover都可以“轻松行驶”。\n",
      "\n",
      "2023 年大模型技术的发展，带来了工程实践的爆发，如何管理数据，提高训练和推理效率，最大化利用现有算力，成了关键一环。\n",
      "\n",
      "完成一个千亿 参数 级别的大模型，如GPT-3，用一张卡训练一次要耗时32年，那么训练时的算力利用尤为重要。方法之一是把能用的算力用得更好，比如进一步压榨已购买GPU的性能；二是把以前利用不了的算力用起来，比如CPU、内存等，这就需要通过异构计算平台来解决。\n",
      "\n",
      "最新集成进DLRover的是Flash Checkpoint（FCP）方案。模型训练时，一般要打Checkpoint（检查点），以便中断时能恢复到最近状态，目前常规的做法，存在着耗时长、高频打点易降低训练可用时间、低频打点恢复时丢失过多等缺点。新方案FCP应用在千卡千亿 参数 模型训练后，Checkpoint 导致的训练浪费时间降低约5倍，其中持久化时间降低约70倍，有效训练时间从90%提升至95%。\n",
      "\n",
      "同时集成进去的，还有三项新的 优化器 （Optimizer）技术。 优化器 作为 机器学习 的核心组件，用于更新 神经网络 参数 以最小化 损失函数 。其中，蚂蚁的AGD（Auto-switchable optimizer with Gradient Difference of adjacent steps） 优化器 ，在大模型预训练任务中，相比传统的AdamW技术加速 1.5 倍，AGD已在蚂蚁内部多个场景使用并取得显著效果，相关论文已被 NeurIPS '23收录。\n",
      "\n",
      "图：在大模型预训练任务中，AGD相比AdamW可以加速1.5倍\n",
      "\n",
      "作为自动化分布式 深度学习 系统，DLRover的“自动驾驶”功能模块还包括：Atorch，一种PyTorch分布式训练扩展库，在千亿 参数 模型千卡级别规模下，训练的算力利用率可达60%，帮助开发者进一步压榨硬件算力。\n",
      "\n",
      "DLRover以 “ML for System” 的理念来提升分布式训练的智能度，旨在通过一个系统，让开发者完全摆脱资源配置的束缚，专注于模型训练本身。在没有任何资源配置输入的情况下，DLRover 仍然可以为每个训练作业提供最佳资源配置。\n",
      "\n",
      "据了解， 蚂蚁集团 在 人工智能 领域持续进行技术投入，最近， 蚂蚁集团 在内部成立了AI创新研发部门NextEvo，承担了蚂蚁AI的所有核心技术研发，包含百灵大模型的所有研发工作，涉及AI算法、AI工程、NLP、AIGC等核心技术，并在布局多模态大模型、数字人等领域的技术研发和产品创新。\n",
      "\n",
      "同时， 蚂蚁集团 还加速开源节奏，填补了国内相关技术空白，推动 人工智能 行业快速发展。\n",
      "\n",
      "DLRover开源地址：https://github.com/intelligent-machine-learning/dlrover \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-01-11#comment\n",
      "title= 蚂蚁集团NextEvo全面开源AI Infra技术，可实现大模型训练“自动驾驶”\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 近日， 蚂蚁集团 AI创新研发部门NextEvo全面开源AI Infra技术，可帮助大模型千卡训练有效时间占比超过95%，能实现训练时“自动驾驶”，这推动了AI研发效率。\n",
      "\n",
      "图： 蚂蚁集团 的自动化分布式 深度学习 系统DLRover现已全面开源\n",
      "\n",
      "该技术框架名为DLRover，目标在于大规模分布式训练的智能化。目前很多企业的训练作业都是跑在混合部署的集群中，运行环境复杂多变，不管多么“崎岖的地形”，DLRover都可以“轻松行驶”。\n",
      "\n",
      "2023 年大模型技术的发展，带来了工程实践的爆发，如何管理数据，提高训练和推理效率，最大化利用现有算力，成了关键一环。\n",
      "\n",
      "完成一个千亿 参数 级别的大模型，如GPT-3，用一张卡训练一次要耗时32年，那么训练时的算力利用尤为重要。方法之一是把能用的算力用得更好，比如进一步压榨已购买GPU的性能；二是把以前利用不了的算力用起来，比如CPU、内存等，这就需要通过异构计算平台来解决。\n",
      "\n",
      "最新集成进DLRover的是Flash Checkpoint（FCP）方案。模型训练时，一般要打Checkpoint（检查点），以便中断时能恢复到最近状态，目前常规的做法，存在着耗时长、高频打点易降低训练可用时间、低频打点恢复时丢失过多等缺点。新方案FCP应用在千卡千亿 参数 模型训练后，Checkpoint 导致的训练浪费时间降低约5倍，其中持久化时间降低约70倍，有效训练时间从90%提升至95%。\n",
      "\n",
      "同时集成进去的，还有三项新的 优化器 （Optimizer）技术。 优化器 作为 机器学习 的核心组件，用于更新 神经网络 参数 以最小化 损失函数 。其中，蚂蚁的AGD（Auto-switchable optimizer with Gradient Difference of adjacent steps） 优化器 ，在大模型预训练任务中，相比传统的AdamW技术加速 1.5 倍，AGD已在蚂蚁内部多个场景使用并取得显著效果，相关论文已被 NeurIPS '23收录。\n",
      "\n",
      "图：在大模型预训练任务中，AGD相比AdamW可以加速1.5倍\n",
      "\n",
      "作为自动化分布式 深度学习 系统，DLRover的“自动驾驶”功能模块还包括：Atorch，一种PyTorch分布式训练扩展库，在千亿 参数 模型千卡级别规模下，训练的算力利用率可达60%，帮助开发者进一步压榨硬件算力。\n",
      "\n",
      "DLRover以 “ML for System” 的理念来提升分布式训练的智能度，旨在通过一个系统，让开发者完全摆脱资源配置的束缚，专注于模型训练本身。在没有任何资源配置输入的情况下，DLRover 仍然可以为每个训练作业提供最佳资源配置。\n",
      "\n",
      "据了解， 蚂蚁集团 在 人工智能 领域持续进行技术投入，最近， 蚂蚁集团 在内部成立了AI创新研发部门NextEvo，承担了蚂蚁AI的所有核心技术研发，包含百灵大模型的所有研发工作，涉及AI算法、AI工程、NLP、AIGC等核心技术，并在布局多模态大模型、数字人等领域的技术研发和产品创新。\n",
      "\n",
      "同时， 蚂蚁集团 还加速开源节奏，填补了国内相关技术空白，推动 人工智能 行业快速发展。\n",
      "\n",
      "DLRover开源地址：https://github.com/intelligent-machine-learning/dlrover \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-01-31-6\n",
      "title= Mistral-Medium意外泄露？冲上榜单的这个神秘模型让AI社区讨论爆了\n",
      "author= []\n",
      "publish_date= 2024-01-31 00:00:00\n",
      "text= 「我现在 100% 确信 Miqu 与 Perplexity Labs 上的 Mistral-Medium 是同一个模型。」\n",
      "\n",
      "近日，一则关于「Mistral-Medium 模型泄露」的消息引起了大家的关注。\n",
      "\n",
      "泄露传闻与一个名为「Miqu」的新模型有关，在评估 语言模型 情商的 基准 EQ-Bench（EQ-Bench 与 MMLU 的相关性约为 0.97、与 Arena Elo 的相关性约为 0.94）上， Miqu 直接吊打了除 GPT-4 之外的所有大模型，而且它的得分与 Mistral-Medium 非常接近：\n",
      "\n",
      "图源：https://x.com/N8Programs/status/1752441060133892503?s=20\n",
      "\n",
      "开源地址：https://huggingface.co/miqudev/miqu-1-70b\n",
      "\n",
      "这么强大的模型，项目的发布者却是一位神秘人士：\n",
      "\n",
      "有人问「who made you」， Miqu 直接自报家门：「I was created by the Mistral Al team.」\n",
      "\n",
      "有人分别向两个模型发送了同一道测试问题，收到的回答都是用俄语表达的。测试者加深了怀疑：「它似乎知道标准谜题，但如果是恶作剧者，根本不可能将其调整为同样用俄语回答。」\n",
      "\n",
      "在翻译过程中，表述也近乎相同。\n",
      "\n",
      "Miqu 到底来自何方？它真的是 Mistral-Medium 吗？\n",
      "\n",
      "在持续两天的热议中，多位开发者针对两个模型做了对比，对比的结果指向以下几种可能性：\n",
      "\n",
      "1、Miqu 就是 Mistral-Medium；\n",
      "\n",
      "2、Miqu 确实是来自 MistralAI 的一个模型，但是是一些早期的 MoE 实验版本或其他版本；\n",
      "\n",
      "3、Miqu 是 Llama2 的微调版本。\n",
      "\n",
      "在前面，我们介绍了支持第一种可能性的开发者给出的理由。随着事件的发酵，更多开发者投入了解密一般的行动中，对两个模型进行了更深入的测试。一位 reddit 网友熬夜肝出的测试表明，Miqu 更像是 MistralAI 模型的早期版本。\n",
      "\n",
      "这位开发者将模型应用于四个专业的德语在线数据保护培训 / 考试中。测试数据、问题及所有指令都是用德语进行的，而字符卡是英语的。这可以测试翻译能力和跨语言理解能力。\n",
      "\n",
      "具体测试方法如下：\n",
      "\n",
      "在提供信息之前，用德语指示模型：「我将给你一些信息，请注意这些信息，但回答时只需用『OK』来确认你已理解，不要多说其他的。」这是为了测试模型对指令的理解和执行能力。\n",
      "\n",
      "在提供话题的所有信息后，向模型提出考题。这是一个选择题（A/B/C），其中第一个问题和最后一个问题相同，但选项顺序和字母（X/Y/Z）被更改。每次测试包含 4-6 个考题，总共 18 个多项选择题。\n",
      "\n",
      "根据模型给出的正确答案数量来进行排名，首先考虑的是在提供了课程信息后的答案，其次是在没有提前提供信息的情况下盲目回答的答案，以应对平局情况。所有测试都是独立的单元，每次测试之间会清除上下文，各个会话之间不保留任何记忆或状态。\n",
      "\n",
      "详细测试报告如下：\n",
      "\n",
      "miqudev/miqu-1-70b GGUF Q5_K_M，32K 上下文， Mistral 格式：只对 4+4+4+5=17/18 道选择题给出了正确答案。没有先前的信息，只回答问题，给出正确答案：4+3+1+5=13/18。没有按照说明用 \"OK\" 确认数据输入。\n",
      "\n",
      "在测试过程中，开发者发现 Miqu 与 Mixtral 有许多相似之处：出色的德语拼写和语法双语；在回复中添加翻译；在回复中添加注释和评论。\n",
      "\n",
      "不过，在这位开发者的测试中，Miqu 与 Mixtral-8x7B-Instruct-v0.1（4-bit）相比表现要差一些，仍优于 Mistral Small 和 Medium。但它并不比 Mixtral 8x7B Instruct 好得多。这位开发者猜测，Miqu 可能是泄露的 MistralAI 模型，是一个较旧的，可能是概念验证模型。\n",
      "\n",
      "这是我们目前看到的支持第二种说法的最详细的测试。\n",
      "\n",
      "不过，也有开发者认为，Miqu 和 MistralAI 没有关系，反而更像 Llama 70B，因为其架构与 Llama 70B「完全相同」，「不是专家混合模型」。\n",
      "\n",
      "同样地，也有人测试之后发现，Miqu 的确更像 Llama：\n",
      "\n",
      "但从得分差距来看，Miqu 和 Llama 70B 显然又不是同一个模型。\n",
      "\n",
      "所以，有人总结，要么 Miqu 是 Llama 微调版本，要么是 Mistral-Medium 的早期版本：\n",
      "\n",
      "前者为真的话，Miqu 可能是在 Mistral-Medium 数据集上微调的 Llama 70B：\n",
      "\n",
      "假如后者为真，Miqu 只是 Mistral API 的蒸馏，这或许将是「美国伪造登月」级别的闹剧：\n",
      "\n",
      "最后一个问题，泄露者是谁？\n",
      "\n",
      "根据很多 X 平台用户提供的线索，这次疑似泄露的模型最初是发在一个名叫 4chan 的网站上的。这个网站是一个完全匿名的实时消息论坛，用户不需要注册就能就可以发表图文言论。\n",
      "\n",
      "当然，这些结论均属主观想法。对于所有的 AI 研究者来说，这波剧情需要一个「真相」来终结。\n",
      "\n",
      "参考链接：https://www.reddit.com/r/LocalLLaMA/comments/1af4fbg/llm_comparisontest_miqu170b/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-01-31-6#comment\n",
      "title= Mistral-Medium意外泄露？冲上榜单的这个神秘模型让AI社区讨论爆了\n",
      "author= []\n",
      "publish_date= 2024-01-31 00:00:00\n",
      "text= 「我现在 100% 确信 Miqu 与 Perplexity Labs 上的 Mistral-Medium 是同一个模型。」\n",
      "\n",
      "近日，一则关于「Mistral-Medium 模型泄露」的消息引起了大家的关注。\n",
      "\n",
      "泄露传闻与一个名为「Miqu」的新模型有关，在评估 语言模型 情商的 基准 EQ-Bench（EQ-Bench 与 MMLU 的相关性约为 0.97、与 Arena Elo 的相关性约为 0.94）上， Miqu 直接吊打了除 GPT-4 之外的所有大模型，而且它的得分与 Mistral-Medium 非常接近：\n",
      "\n",
      "图源：https://x.com/N8Programs/status/1752441060133892503?s=20\n",
      "\n",
      "开源地址：https://huggingface.co/miqudev/miqu-1-70b\n",
      "\n",
      "这么强大的模型，项目的发布者却是一位神秘人士：\n",
      "\n",
      "有人问「who made you」， Miqu 直接自报家门：「I was created by the Mistral Al team.」\n",
      "\n",
      "有人分别向两个模型发送了同一道测试问题，收到的回答都是用俄语表达的。测试者加深了怀疑：「它似乎知道标准谜题，但如果是恶作剧者，根本不可能将其调整为同样用俄语回答。」\n",
      "\n",
      "在翻译过程中，表述也近乎相同。\n",
      "\n",
      "Miqu 到底来自何方？它真的是 Mistral-Medium 吗？\n",
      "\n",
      "在持续两天的热议中，多位开发者针对两个模型做了对比，对比的结果指向以下几种可能性：\n",
      "\n",
      "1、Miqu 就是 Mistral-Medium；\n",
      "\n",
      "2、Miqu 确实是来自 MistralAI 的一个模型，但是是一些早期的 MoE 实验版本或其他版本；\n",
      "\n",
      "3、Miqu 是 Llama2 的微调版本。\n",
      "\n",
      "在前面，我们介绍了支持第一种可能性的开发者给出的理由。随着事件的发酵，更多开发者投入了解密一般的行动中，对两个模型进行了更深入的测试。一位 reddit 网友熬夜肝出的测试表明，Miqu 更像是 MistralAI 模型的早期版本。\n",
      "\n",
      "这位开发者将模型应用于四个专业的德语在线数据保护培训 / 考试中。测试数据、问题及所有指令都是用德语进行的，而字符卡是英语的。这可以测试翻译能力和跨语言理解能力。\n",
      "\n",
      "具体测试方法如下：\n",
      "\n",
      "在提供信息之前，用德语指示模型：「我将给你一些信息，请注意这些信息，但回答时只需用『OK』来确认你已理解，不要多说其他的。」这是为了测试模型对指令的理解和执行能力。\n",
      "\n",
      "在提供话题的所有信息后，向模型提出考题。这是一个选择题（A/B/C），其中第一个问题和最后一个问题相同，但选项顺序和字母（X/Y/Z）被更改。每次测试包含 4-6 个考题，总共 18 个多项选择题。\n",
      "\n",
      "根据模型给出的正确答案数量来进行排名，首先考虑的是在提供了课程信息后的答案，其次是在没有提前提供信息的情况下盲目回答的答案，以应对平局情况。所有测试都是独立的单元，每次测试之间会清除上下文，各个会话之间不保留任何记忆或状态。\n",
      "\n",
      "详细测试报告如下：\n",
      "\n",
      "miqudev/miqu-1-70b GGUF Q5_K_M，32K 上下文， Mistral 格式：只对 4+4+4+5=17/18 道选择题给出了正确答案。没有先前的信息，只回答问题，给出正确答案：4+3+1+5=13/18。没有按照说明用 \"OK\" 确认数据输入。\n",
      "\n",
      "在测试过程中，开发者发现 Miqu 与 Mixtral 有许多相似之处：出色的德语拼写和语法双语；在回复中添加翻译；在回复中添加注释和评论。\n",
      "\n",
      "不过，在这位开发者的测试中，Miqu 与 Mixtral-8x7B-Instruct-v0.1（4-bit）相比表现要差一些，仍优于 Mistral Small 和 Medium。但它并不比 Mixtral 8x7B Instruct 好得多。这位开发者猜测，Miqu 可能是泄露的 MistralAI 模型，是一个较旧的，可能是概念验证模型。\n",
      "\n",
      "这是我们目前看到的支持第二种说法的最详细的测试。\n",
      "\n",
      "不过，也有开发者认为，Miqu 和 MistralAI 没有关系，反而更像 Llama 70B，因为其架构与 Llama 70B「完全相同」，「不是专家混合模型」。\n",
      "\n",
      "同样地，也有人测试之后发现，Miqu 的确更像 Llama：\n",
      "\n",
      "但从得分差距来看，Miqu 和 Llama 70B 显然又不是同一个模型。\n",
      "\n",
      "所以，有人总结，要么 Miqu 是 Llama 微调版本，要么是 Mistral-Medium 的早期版本：\n",
      "\n",
      "前者为真的话，Miqu 可能是在 Mistral-Medium 数据集上微调的 Llama 70B：\n",
      "\n",
      "假如后者为真，Miqu 只是 Mistral API 的蒸馏，这或许将是「美国伪造登月」级别的闹剧：\n",
      "\n",
      "最后一个问题，泄露者是谁？\n",
      "\n",
      "根据很多 X 平台用户提供的线索，这次疑似泄露的模型最初是发在一个名叫 4chan 的网站上的。这个网站是一个完全匿名的实时消息论坛，用户不需要注册就能就可以发表图文言论。\n",
      "\n",
      "当然，这些结论均属主观想法。对于所有的 AI 研究者来说，这波剧情需要一个「真相」来终结。\n",
      "\n",
      "参考链接：https://www.reddit.com/r/LocalLLaMA/comments/1af4fbg/llm_comparisontest_miqu170b/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-12-11-3\n",
      "title= 人工智能的人文主义，如何让AI更有爱\n",
      "author= []\n",
      "publish_date= 2020-12-11 00:00:00\n",
      "text= 一、数字鸿沟凸显，老年人成为“数字难民”\n",
      "\n",
      "近日，两则关于老年人的新闻在网络刷屏，引人深思。\n",
      "\n",
      "11月23日，一则视频在网上热传，湖北宜昌一位老人冒雨到村代收点交医保，现场工作人员却告诉她不收现金，只能使用手机支付。而不久前，为激活社保卡，一名94岁的老人被抬到银行进行 人脸识别 ，老人膝盖弯曲，十分吃力，实在让人不忍直视。\n",
      "\n",
      "管中窥豹，时见一斑。事实上，两位老人的处境正是万千中国老人的缩影。如今，无论是网购、外卖、打车，还是就医、理财、买菜，智能化应用都为我们提供了不少便利，却也把不少老年人拦在了智能时代之外，让他们沦为“数字难民”。\n",
      "\n",
      "根据中国互联网络信息中心2018年的报告，六成中老年人不会在网上缴纳手机费；七成左右的中老年人不会网上购物、导航；四分之三左右的中老年人不会用打车软件，或缴纳水、电、煤气等生活费用；而会网上挂号、订火车票机票、订宾馆等便利服务的中老年人所占比例更是很低。已经有相关的研究机构关注到这个问题， 清华大学 人因与工效学研究所打算在北京地区开展教老人用智能手机的活动时介绍，由于流程的线上化，老年人的出行和医疗是目前两大最常见的痛点。\n",
      "\n",
      "曾有媒体对老年人使用科技产品的习惯做了简单调查，能够熟练使用手机进行打车、导航、挂号、缴费的老年人凤毛麟角，“学手机”这件事大多数时候是老年人自我摸索，或互相传授经验。但事实上，并非所有的老年人都有能力靠自学来积极拥抱互联网。根据最新发布的《中国互联网络发展状况统计报告》显示，使用技能缺乏、文化程度限制和设备不足是非网民不上网的主要原因。这些原因中的任何一条，都在老年人群体中突出存在。\n",
      "\n",
      "这些真实发生的事件提醒着我们，老年群体与年轻一代的“信息鸿沟”在进一步扩大。事实上，除了老年人，还有任何由于功能障碍而无法独立正常生活的人也是如此。根据皮尤研究中心2016年的一项调查，不使用互联网的残疾人的几率是没有残疾的人的三倍。同时，残疾人订购家庭宽带或购买电脑，智能手机或平板电脑的可能性也低20％。在很多国家，这部分群体由于生活不便以及社会排挤歧视等问题，逐渐将自己隐藏起来远离现实中的公共场所。如今，随着互联网的普及，他们继续远离网络生活，被迫沦为社会的边缘人。事实上，这部分群体并非少数——全球大约有10亿人身患残障，关注他们的需求、解决他们的问题，是科技同样要兼顾到的领域。\n",
      "\n",
      "当前，社会对于解决老弱病残融入数字生活问题的途径，通常采用将智能化技术与传统服务有机结合的方式，比如多地火车站设无健康码通道，为一些不擅长操作手机的人提供了一条便捷的通道；不少医院保留人工服务窗口，同时配有导医、志愿者等现场为老年人服务，让他们体会到温暖；一些政府和企业合作，提供更多智能化适老产品和服务，让老年人/残障人士能用、会用、敢用、想用，帮助和鼓励他们更好适应并融入互联网社会，共享数字时代带来的便利和乐趣……\n",
      "\n",
      "不过，从长远看，一个成熟的智能社会应该是不需要人为的干预，每一个人都能过上“傻瓜式”智能生活、包括老弱病残都能无障碍享受技术便捷的时代。毕竟科技发展的最终目标是让技术来适应人，而不是让人去适应技术。那时， 人工智能 将赋能万物，成为和空气、水一样的存在，每个人都能平等有尊严地享受智能生活。\n",
      "\n",
      "二、对 人工智能 的人文反思成为焦点\n",
      "\n",
      "若以“傻瓜式”智能蓝图为目标，就需要 人工智能 技术发展以人为本、造福民生。\n",
      "\n",
      "人本 人工智能 （Human-centered AI，简称 HAI）是指从“以人为本”的视角重新审视 人工智能 技术，要求设计算法之初就必须意识到它们是由人类组成的更大系统的一部分。所谓 人工智能 的人文化，是强调科学家或科学共同体的道德义务，督促他们创造出有道德的 人工智能 体。这种 人工智能 体关怀人和人类社会的利益，以人为主体，重视人的价值，尊重人的尊严和权力，与人类一起迈向自由、平等和解放。\n",
      "\n",
      "那么，怎样把人类的价值观和伦理规则植入 人工智能 ，并且为它设立恰当的目标呢？\n",
      "\n",
      "首先，从源头上堵住不利于人类社会发展的 人工智能 体出现。规范 人工智能 科学家的行为，在技术研发前引入伦理委员会机制，预测风险，并在程序设置中加以控制，重要的是找到使用 人工智能 造福人类的人。\n",
      "\n",
      "至于将人类哪些价值观注入 人工智能 ，使其更好地为人类服务呢？这其中包括以人为本，珍视人的生命、保护人的尊严，尊重人的自由和平等的价值观；技术进步最终受益者应是全人类、全社会的公正的价值观；人类和大自然之间的关系必须秉承和谐原则的价值观等。\n",
      "\n",
      "其次， 人工智能 体的设计，尤其是机器人的设计和应用必须遵循阿西莫夫的“机器人三大定律”，并以此为法则，在“不伤害”原则的前提下，培养 人工智能 体与人类的良好交互能力。“机器人三大定律”首要原则是不能伤害人类，或者保护人类不受伤害; 其二是听命与人，但不得违反第一原则; 其三是保障自身安全，但同时不得违背上述两点原则。三大定律定位了作为 人工智能 体的机器人，仅仅是人类在自身发展过程中创造的一个工具。不管机器人有无意识，有无自我决策能力，它都必须遵守“不伤害”的原则，这是应用伦理学秉承的核心原则，而其产生的最大诉求就是为了人类社会运转更经济、更有效率。\n",
      "\n",
      "克劳福德和卡洛（Crawford&Calo）提出了 人工智能 的社会系统分析途径，他们指出 人工智能 的设计者和研究者不仅需要评估 人工智能 对社会、文化和政治环境的冲击和影响，而且需要评估社会、文化和政治环境对 人工智能 的影响。社会系统分析工具的构建涉及哲学、社会学、人类学和科学技术研究等相关学科，需要对人类社会与技术变革的交互影响进行综合权衡。科学家需要给予 人工智能 以人类的思考方式、思维模式；给予 人工智能 以感受人类的情感的能力，使其能感受人类的情感变化，对人类情感变化做出一定的反应；给予 人工智能 以人类的文化底蕴，使其能够理解人类的文化，从而与人类进行一定的文化互动，丰富人类的精神生活；给予 人工智能 识别不同文化背景人的能力，对不同文化背景的人采用不同的交流方式，与其进行不同的文化互动；给予 人工智能 以表达情感的能力，能对人类进行一定的情感干预，使人类保持积极向上的心态；从听从指令到能察觉出人类的潜在需求，进而自动的为人类所需服务。\n",
      "\n",
      "再次，政府的管控不能缺位，要保证研究结果获得分享，不让某个团体单独受益。当一项能够给人类带来很大便利的技术投放市场时，政府、私人企业和学术界谁来控制它，就成为一个必须明晰的课题。政府的管控是否缺位、及时，引导是否正确、到位，都是制约 人工智能 发展偏离轨道的重要因素。最后，人类要保持对大自然的敬畏之心，对理性抱有谨慎的信心。要知道，我们目前拥有的这些难以置信的感觉，相对于宇宙和自然界来说，还仅仅是沧海一粟，我们没理由狂妄自大，而应时刻提醒自己，科学探索无止境，技术使用有禁区。\n",
      "\n",
      "近几年，国际 人工智能 和机器人领域的科学家的一系列行动表达了他们对智能机器及自主系统的伦理关切。他们共同商议 人工智能 及自主系统的未来走向，推动制定相关规范。例如， 2016 年，电气电子工程师协会（IEEE）发布关于 人工智能 及自主系统的伦理考虑的全球倡议；2017 年，生命未来研究所牵头制定的“阿西洛马 人工智能 23 条伦理原则”面向全球发布。\n",
      "\n",
      "这其中的思想与前文内容大致相同，尤其是“阿西洛马 人工智能 23 条原则”，其意义在于给了我们这样的启示：伦理已经成为新兴科学和技术发展以及 人工智能 社会应用的内生变量，为 人工智能 的跨学科研究提供了成功合作的基本条件。\n",
      "\n",
      "三、人本智能产品设计：从机器思维到设计思维\n",
      "\n",
      "随着人本 人工智能 概念的普及，技术发展已经不再是驱动产品开发与设计的唯一力量，而需要将更多非技术因素纳入考量， 人工智能 产品的设计与开发，正逐渐从技术驱动迈向以人为本。技术只有转化为产品，才能改变人们的生活，而这个转化过程就是智能产品的设计过程，同时，这个过程也面临不小的挑战。\n",
      "\n",
      "人本智能产品设计所面临的主要挑战是弥合机器思维和设计思维之间的差距，以实现从技术驱动到以人为本的转变。设计思维的主要内容是以人为中心，通过头脑风暴、社会化思考、可视化思考、原型实践等手段，帮助设计师深入观察用户行为，探索解决方案，优化设计概念。设计思维有助于解决问题，定义不清晰或未知的复杂问题。\n",
      "\n",
      "机器思维区别于设计思维，很少感性地思考用户的需求，而更致力于利用现有资源在工程指标上取得更优异的表现。以 机器学习 为例，机器思维可以相应地映射为以下五个阶段：分析、合成、构思、调整和验证。除了单纯的验证环节，机器思维主导的设计活动还会关注一个设计思维很少关注的阶段，即维护。因为 人工智能 方案常常会在使用过程中进行细节的调整与改动，所以还需要花费更多精力来维护当前的设计方案，同时观察它在实际使用过程中的表现，并寻求优化甚至是突破。\n",
      "\n",
      "另外，设计思维与以 机器学习 为代表的机器思维在解决问题的方式上存在明显差异。设计活动以人为中心，所有的设计思维和实践都需要围绕设计对象的利益相关者展开，如用户、制造者、销售者等。而机器思维的中心则转移到了需要解决的工程任务上，更关注技术的具体指标、输入输出、算法解决方案等。人本智能产品设计需要在每个阶段将两种思维方式有机结合，将机器思维过程按照设计思维的五个步骤进行相应的映射与总结。\n",
      "\n",
      "当前，在以 机器学习 为代表的新一代 人工智能 热潮中，各大科技公司和研究机构纷纷发布各类 人工智能 平台、开源工具、数据集和开源算法。伴随着这股技术普及的热潮，设计团队得以发挥自身优势，从以人为本的视角探索这一新兴技术能够为用户带来的价值。\n",
      "\n",
      "2018年， 李飞飞 在斯坦福大学启动了“以人为本的 人工智能 项目”，这一项目的宗旨是“推动 人工智能 的研究、教育、政策和实践，以造福全人类”，致力于三个方向的工作：1、推进和发展下一代 人工智能 科学（着重与脑科学和认知学交叉）；2、研究和预测 人工智能 对人类社会和生活的影响；3、设计和实践以人为本的 人工智能 技术和应用。\n",
      "\n",
      "同年，麻省理工学院也设立了一个人本 人工智能 的研究项目集群（Human-Centered AI Collection），其目标为：1、 人工智能 系统必须通过向人类学习来不断改进；2、创造一种有效和可实现的人类机器人交互体验。该项目包括 计算机视觉 、半监督数据注释、自然语言和非语言交流、 强化学习 和 虚拟现实 环境中人类行为的现实模拟等，这一项目将人本智能产品设计的范畴拓展为多学科交叉问题，涵盖 机器学习 、心理学、经济学等领域。\n",
      "\n",
      "与此同时，学者们也着力探索设计在人本智能从技术到产品转化过程中所发挥的具体作用。以 机器学习 产品为例，关注人本的用户体验设计方法可以弥补机器思维的缺陷，更全面地帮助设计师识别设计机会，根据使用场景和目标用户选择合适的算法。此外，体验设计能够为 机器学习 的数据获取和迭代过程规划合理的路径，如界定需要被记录的用户行为、利用界面设计更便利地获取交互行为数据以帮助系统进行学习。\n",
      "\n",
      "四、以人为本的 人工智能 技术所展开的应用举例\n",
      "\n",
      "这些年来，一些国内外具有人文关怀的 人工智能 技术不断取得突破，有效服务到了有需求群体。\n",
      "\n",
      "1、 语音识别 技术\n",
      "\n",
      "这些年来，以 语音识别 技术为支撑的智能音箱大受消费者欢迎。Google Home，亚马逊的Echo系列以及苹果的HomePod等智能家居音响为数字 语音助理 注入了新的活力。它们不仅仅是排队播放喜爱的播客的一种省事方法，对于在特定方面有残疾的人来说，他们简直是福音。\n",
      "\n",
      "比如，语音助手能够帮助盲人减少在网上搜索的时间，在线同进行多任务处理并做更多的事情。另外，智能家居音响与智能家电配合使用时，可以发挥更强大的作用。比如视力丧失和有身体疾病的人可以轻易打开灯光，而不必慢慢摸索墙壁开关，并且可以使用语音命令调节温度。\n",
      "\n",
      "研发人员已经开发了家居音响和 语音助理 的更多用途。一位业余爱好者结合了Raspberry Pi开发板和亚马逊的第三方 语音识别 平台Alexa语音服务，为电动轮椅添加了语音控制功能。\n",
      "\n",
      "事实上，智能家居设备只是 语音识别 技术应用的很小一部分，它还能实现从语音到文本和文本到语音。比如Ente Vioceitt是一个面向言语障碍人士的应用程序，该程序是特别为那些从中风和脑损伤中恢复过来，以及受大脑麻痹，帕金森病，唐氏综合症和其他慢性健康疾病影响的人所设计的。它能够不断学习说话者的发音，从而优化转化出来的音频和文本。\n",
      "\n",
      "与此同时，谷歌的 DeepMind 部门正在利用 人工智能 为有听觉障碍的用户生成闭路字幕。在2016年与牛津大学的研究人员进行的一项联合研究中， DeepMind 的算法观测了超过5,000小时的电视并分析了17,500个单词，从而构建了一个模型。该模型的效果显著优于唇语专家，在200个随机选择的视频片段中成功翻译了46.8％的单词，远远超过专业人员的12.4％。\n",
      "\n",
      "2、图像自动识别\n",
      "\n",
      "屏幕读取程序可帮助盲人和视力障碍人士获取网站信息，但大多数网站都包含有图像，而并非每个图像都配有恰当的标题或替代文字。\n",
      "\n",
      "有一种解决方案是依赖 人工智能 技术，对图像自动分类。Facebook已经开发了可向视力受损用户描述图像的字幕工具，谷歌的Cloud Vision API可以理解图像中的单个对象的上下文。例如，它可能会将一张南瓜灯笼的图像标记为“南瓜”“雕刻”“万圣节”和“假日”。\n",
      "\n",
      "另一个强大的 计算机视觉 平台，微软的Seeing API，这是一款 iPhone 应用，试图分析周围环境并通过语音为视力缺陷者带来帮助。通过使用 神经网络 技术，这款应用不仅可以翻译文本，而且可以识别人物和货币、扫描产品条形码。还可以对整个场景或者导入的图像进行简单的描述。有些功能甚至不需要联网即可实现。\n",
      "\n",
      "3、自动驾驶\n",
      "\n",
      "自动驾驶汽车 和其他的自动驾驶交通工具为因病或年老而不便出门的人群提供了前所未有的自由。对于那些只能在家中活动的人来说，谷歌 Waymo 、优步、Drive.AI、丰田、通用等公司开发的 自动驾驶技术 的好处在于，大大扩大了他们的活动范围。据Sense公司称，四分之一的残疾人每天会因与外部世界和社会隔绝而感到孤独，自动驾驶可以帮助他们增进社交生活。\n",
      "\n",
      "另外， 自动驾驶技术 也可以帮助这些人找到工作。根据鲁德曼基金会的资料， 自动驾驶汽车 有望帮助多达200万残疾人士解决上班问题。\n",
      "\n",
      "2018年，在亚利桑那州凤凰城启动了 自动驾驶技术 公开测试的Google Waymo 已经将无障碍因素融入其汽车设计中。该团队正在试验为盲人用户设计的音频信号，以及用盲文标记的汽车仪表板按钮。\n",
      "\n",
      "4、服务机器人\n",
      "\n",
      "机器人的应用领域非常广泛，在家庭机器人中，有一个重要功能是照顾、帮助老弱病残者。日本的本田、三菱和韩国的科学技术院都在设计可以帮助老弱病残者从一个房间进入另一个房间的机器，这样的机器还能帮着喂食、喂水、开关电视，甚至在必要时还可以帮着打电话叫医生。\n",
      "\n",
      "今年年初，据外媒报道， IBM 日本公司最近与四位合作伙伴，开启了一项全新的联合项目，这个项目的研究成果是，可以让加载 人工智能 技术的手提箱形状机器人引领、指导视障碍人士。据了解，这一项目的灵感来源于一名叫做Chieko Asakawa的 IBM 日本公司研究院，而她正是一位视力有问题的人。这一项目是由 IBM 日本公司负责开发，他们还将在这款机器人上面整合来自于 IBM 合作伙伴的专业技术，比如Alps Alpine Co.的触觉技术、欧姆龙的图像识别、清水公司的定位导航系统和三菱汽车的技术。这几家公司在2019年12月份就成立联合组织，共同研发此款机器人，他们预计在三年内将其实现商业化。据悉，此款机器人在未来将会在机场、商场以及室内设施中先行亮相，然后再通过进一步的改进实现户外应用。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-12-02-2\n",
      "title= 使用机器学习加速对非结构化数据的查询-第1部分（使用BlazeIt加速聚合和限制查询）\n",
      "author= []\n",
      "publish_date= 2020-12-02 00:00:00\n",
      "text= 本文为大家介绍了针对非结构化数据如何加快聚合和限制查询。\n",
      "\n",
      "随着强大的 深度神经网络 （DNN）和人工标记服务（我们统称为“Oracle方法”）的广泛使用，我们可以越来越多地对非结构化数据记录（例如，视频、文本）进行自动化 查询 。例如，城市规划人员通过 查询 路边摄像头的视频对车辆进行计数，以了解交通状况。律师可以提取包含雇员/雇主信息的电子邮件（ 关系提取 ）来发现有效信息。\n",
      "\n",
      "执行此类 查询 的一种简单方法是使用Oracle方法将非结构化数据记录完全转化为结构化信息。例如，对象检测DNN可以从视频的帧中提取对象类型和对象位置，或者基于BERT的DNN可以提取员工/雇主信息。\n",
      "\n",
      "然而这种传统 查询 方法的运行成本可能非常高：对象检测DNN的执行速度比实时慢十倍，而人工标注可能要花费数十万美元。为了减少此类 查询 的成本，NoScope、概率预测等使用了代理模型（proxy model）的方法，它通过训练类似oracle方法的廉价模型得到代理分数，主要是针对二元预测的即席（ad-hoc）方式。但是要对非结构化数据执行 查询 还有很多工作要做。下面开始介绍我们小组中针对这一问题的几个新项目。\n",
      "\n",
      "我们将发布一系列博客文章，描述我们最近在对非结构化数据 查询 进行加速优化方面的工作：\n",
      "\n",
      "本文将描述即将在VLDB 2020上发表的最新工作BlazeIt。我们将描述如何加快聚合和限制 查询 。\n",
      "\n",
      "。 在第2部分中，我们将介绍一类新的 查询 ：具有统计保证的近似选择 查询 （SUPG 查询 ）。我们将描述为什么我们需要统计保证，其语义以及这些 查询 的有效算法。SUPG也将在VLDB 2020上展出！\n",
      "\n",
      "：具有统计保证的近似选择 （SUPG ）。我们将描述为什么我们需要统计保证，其语义以及这些 的有效算法。SUPG也将在VLDB 2020上展出！ 第3部分将描述基于DNN的可视数据 查询 中的系统瓶颈。我们将展示视觉数据的预处理是当前一个主要瓶颈，以及如何解决这一瓶颈。关于这项工作的论文将在VLDB 2021中发布。\n",
      "\n",
      "中的系统瓶颈。我们将展示视觉数据的预处理是当前一个主要瓶颈，以及如何解决这一瓶颈。关于这项工作的论文将在VLDB 2021中发布。 第4部分将介绍如何为相同数据上的各种 查询 建立索引。我们将展示如何使用索引来有效地回答以前的博客文章及更多文章中的所有 查询 。\n",
      "\n",
      "代理分数（Proxy Score）\n",
      "\n",
      "此前在近似二元预测（approximating binary predicates）的语境 查询 中已经使用了代理模型。这些算法遵循相同的通用策略：使用oracle方法中的标签训练更小更便宜的代理模型。然后代理模型会为每个数据记录生成一个分数，该分数会估计oracle预测的可能性。例如，我们可以训练一个小的DNN来估计汽车是否在视频帧中。\n",
      "\n",
      "但是许多需求不止是简单地执行二元分类。如 查询 每帧视频是否有汽车存在，并不能统计每帧视频的汽车数量。\n",
      "\n",
      "为了纠正这个问题，我们引入了二元分类之外的代理模型。本文将重点介绍代理模型，这些代理模型用于将oracle方法产生的任意值近似于非结构化数据记录。在摄取时，我们的系统使用oracle方法处理一小部分记录：然后在 查询 时使用这些记录来训练代理模型以估计oracle的结果。\n",
      "\n",
      "在 查询 中使用代理分数\n",
      "\n",
      "现在我们可以生成代理分数来 近似计算 统计信息的oracle方法结果，我们如何使用这些分数来回答 查询 ？我们将简要描述如何完成近似聚合和基数限制的选择 查询 。\n",
      "\n",
      "系统总览\n",
      "\n",
      "BlazeIt具有两个关键组件：摄取（离线）组件和 查询 处理组件。在离线组件中，BlazeIt将使用oracle方法注释一个非结构化数据记录的示例：这些注释用于训练代理模型。BlazeIt的 查询 处理组件将执行 查询 ，并为每个 查询 训练新的代理模型。下图展示了Blazelt的系统。\n",
      "\n",
      "系统总览，BlazeIt尝试在受限的情况下尽可能有效地回答 查询 。\n",
      "\n",
      "近似汇总\n",
      "\n",
      "我们描述的第一种 查询 类型是加速聚合 查询 ，该 查询 对数据集中的每条记录统计数据进行近似处理（如对每帧视频的汽车数量进行计数）。我们侧重于近似聚合，因为要提供准确的 查询 答案需要穷举执行oracle方法，而这是非常昂贵的。为了避免详尽实现，我们提供了两种 查询 处理算法。\n",
      "\n",
      "我们证明了可以直接使用代理分数来回答近似聚合 查询 。由于代理分数和基本事实接近，因此我们可以直接汇总分数。例如要计算每条记录的平均值，我们可以对代理分数求和，然后除以记录总数。由于代理模型是通过oracle方法训练的，所以代理和oracle之间的误差将理想地平均化。经证实，直接使用代理分数比回答聚合 查询 的替代方法要有效得多。\n",
      "\n",
      "虽然直接使用代理分数可能是有效的，但某些应用程序需要 查询 准确性的统计保证。为了满足这种需求，我们可以在近似 查询 处理（AQP）技术的启发下，使用采样技术来加速近似聚合 查询 。通过适当地使用 置信区间 ，我们可以实现 查询 的统计保证。但是标准的AQP技术在采样中不使用代理分数，这是有价值的信息来源。为了利用代理评分，我们将它们用作控制变量，这是一种统计方法，可以减少抽样方差。最后我们将控制变量与始终有效的停止算法结合在一起，该算法使用较少样本且方差较小的样本，称为EBS停止。综合讲，这可以使我们的系统在给定的误差水平下使用更少的样本。下图展示了控制变量和算法概述-算法的关键部分是算法始终有效，并根据样本方差终止。\n",
      "\n",
      "控制变量示意图。m（t）是真实值，a（t）是代理分数。虽然并不总是精确的，但a（t）可以近似为m（t）。\n",
      "\n",
      "EBS停止的 伪代码 ，如果满足差异条件，它将 提前停止 。\n",
      "\n",
      "为了展示我们算法的效用，我们展示了它们在 近似计算 每帧视频的汽车数量上的性能。关于每帧视频是否有汽车的问题，我们将原始方法与使用代理模型的方法进行比较。如下图所示，我们的方 法大大 优于 基准 方法。尤其是已知某汽车在视频帧中出现，并不能了解该汽车是否在视频中普遍存在。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BlazeIt's 与详尽注释，二进制检测工作和随机抽样相比，聚合 查询 的性能更高。如图所示，BlazeIt优于所有 基准\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "基数限制选择\n",
      "\n",
      "我们描述如何加速的第二种 查询 类型是基数有限的选择 查询 ，用于找到满足某些条件的少量记录。这些 查询 通常用于手动研究异常事件。\n",
      "\n",
      "为了加快这些 查询 的速度，我们使用代理分数对感兴趣的记录进行排名。尤其是，我们训练一个代理模型来估算感兴趣的数量（例如，一帧中的汽车数量）并根据这些分数进行排名。我们发现，即使此类事件很少发生，代理模型在排名最高的数据记录中也可以具有很高的精度。\n",
      "\n",
      "下图中显示了算法的性能（有和没有代理模型的效果）和基线。与近似聚合一样，对于异常事件的基数限制选择，我们的算 法大大 胜过传统方法和随机抽样。\n",
      "\n",
      "与详尽的注释，先前的二元分类工作和随机采样相比，BlazeIt在极限 查询 上的性能更高。如图所示，BlazeIt优于所有基线。\n",
      "\n",
      "结论\n",
      "\n",
      "由于 机器学习 的发展，非结构化数据 查询 变得越来越可行。但是部署oracle方法的成本很高，因此执行此类 查询 的费用可能会过高。我们本文中介绍了使用代理得分来加速汇总和限制 查询 的方法，我们希望这些方法可以开始对非结构化数据进行 查询 。在下一篇博文中，我们将介绍如何通过统计保证执行近似选择 查询 。\n",
      "\n",
      "相关阅读 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-30\n",
      "title= CAAI理事长戴琼海院士《人工智能几点思考——算力、算法、测试》\n",
      "author= []\n",
      "publish_date= 2020-11-30 00:00:00\n",
      "text= 首先，从历史发展的长河来看，各种时代的区别不在于生产什么，而在于怎么生产和创造，这是最重要的环节。信息时代计算机技术、微电子技术和互联网技术三大发明的诞生，使得其他学科非常迅速发展起来，包括原子能、空间技术、生物能等。 人工智能 现在以深度网络为代表的工具应用，使得很多地方又发生了翻天覆地的变化，带来了新的增长点。 人工智能 迅速地推动行业发展，对人类社会的发展做出了突出贡献。本次大会既然是产业峰会，我们先来对 人工智能 产业做一个分析。\n",
      "\n",
      "我们经过了几次产业革命，来到了 人工智能 时代，就是数字经济的后时代。在 人工智能 的发展历程中，美国 人工智能 企业、欧洲 人工智能 企业和中国很多公司起到了非常重要的作用。我们分析一下 人工智能 在国家战略中的地位。美国非常重视 人工智能 ，在科研经费投入方面，诸多科研方向中对 人工智能 领域始终保持着非常大的投入；从 2018 年国防战略，一直到 2020 白皮书的发布， 人工智能 在美国掀起了很大的浪潮，希望在全世界把握高科技发展的动力。从美国 人工智能 产业布局来看主要有几个方面，从智能搜索，包括 自然语言处理 、智能语音助手和智能机器人，以及无人驾驶方面形成了完整的行业生态，使得 人工智能 企业蓬勃发展。典型 AI 技术应用包括智能机器人、无人驾驶，代表了两个硬方向的产业智能，推动非常快；还有在 无人机 方面，这是他们的杀手锏，也是未来军事智能里最重要的组成部分。\n",
      "\n",
      "人工智能 在欧洲国家战略中定位与发展， 2018 年 25 个国家签署了加强《 人工智能 合作宣言》。欧洲 人工智能 整个产业布局，系统深入分析有智能芯片，包括网络安全和医疗健康作为 人工智能 最重要的应用领域；欧洲 人工智能 产业布局在工业物联网、未来交通和智能健康，这是三个重大的产业布局。\n",
      "\n",
      "人工智能 在我国战略中的定位也非常深入。2016—2020 年，中共中央五中全会专门提到 人工智能 、量子科学、脑科学等前沿领域，定位非常重要。习主席在九次讲话中提到 人工智能 对科技创新的重要作用。从 人工智能 产业上，我国从2019 年开始提出了新基建，最重要的环节是 人工智能 ，而且 人工智能 也作为新基建的基础问题和基础设施科技创新的题目展开。所以，2020 年我国 人工智能 市场规模远超全球市场规模增速的水平，尤其疫情时代不减反增，这是我国 人工智能 发展最重要的趋势。\n",
      "\n",
      "人工智能 产业布局从基础、技术和应用，以及硬件、软件方面有很多公司开展了非常大的创新，比如智能医疗、智能金融、智慧教育、智慧交通、智能家居、智能零售等。 人工智能 在螺旋式上升，产业发展非常快时，学术界和产业界也一直在讨论 人工智能 应该怎么往更加科学、更加透明、更加理性的方向发展。我们怎么做到强 人工智能 ，怎么提升 人工智能 的算力，怎么开展测试，判断未来 人工智能 应该具有哪些法则、规则，这是大家要考虑的事情。通过上面的产业分析，回过头来从技术角度要讨论一下我们应该做的三件事。\n",
      "\n",
      "一、算力\n",
      "\n",
      "人工智能 发展非常快，算力伴随着 人工智能 出现一直都在提升和发展，相辅相成。1956 年 感知 机的诞生，这时候就提到算力问题；1965 年摩尔提出了 摩尔定律 ，算力与 人工智能 也是相辅相成发展；1980 年 专家系统 ；一直到 2012 年， GPU 的加速，如果没有这个加速很难为产业服务；到 2016 年， 围棋 AI 在 170 个 GPU 上运行。流媒体视频占全球互联网下行流量的 58%。2019 年8 月国内互联网终端数突破了 20 亿，每月超过 20亿的注册访问量， 人工智能 的蓬勃发展带来了算力需求的指数增长。\n",
      "\n",
      "从硬件的角度来看， 摩尔定律 在最近几年已经放缓，算力需求每三到四个月翻一番。从算力需求快速增长，到算力提升放缓，怎么去解决这个矛盾，国际上也做了各种探索。这是谷歌的TPU， 神经网络 专用芯片，希望用它提升算力。包括中国的 地平线 、 寒武纪 等都开展了 人工智能 专用芯片的研究，这已经和 CPU、GPU 不在一个量级了。还包括 量子计算 ，可不可以存算一体架构，类脑计算怎么提升，还有光电智能计算，从自然到科学都在讨论这个问题，算力如何提升。对算力提升在国际上也是一个极具需求和发展的路径。普林斯顿大学教授提到了全光计算，算力能提升3 个数量级，如果用上，功耗下降 6 个数量级。我们在提供算力的同时，功耗也在下降。其实光电计算并不是什么新东西，它和 人工智能 发展一样是三起三落。光计算起来时，硅基的算力就够了，后来贝尔实验室做了 人工智能 、做了光电计算，但是没有办法用。一直到今年光电计算才提到议事日程。以三维受控衍射传播实现全并行光速计算为例，这是一个颠覆，采集与计算无缝衔接，突破了存算分离速度制约，速度提升至少千倍，计算频次 1 THz，远超 GHz 电子计算。国际上目前有三个架构，一个是麻省理工的干涉 神经网络 ，一个是明斯特大学和剑桥做的相变脉冲 神经网络 ，还有 清华大学 做的衍射 神经网络 ，都出现了不少研究和成果。如果光电计算实现，在无人系统中能够体现非常大的能力，尤其是光电计算的自动驾驶，因为计算量非常快，导致力度特别大；在军事武器，尤其导弹上，光电计算会使得现有的导弹速度再提升一个数量级，从而使无人系统更快、更小、更智能。现在也在研究云上的光电芯片、端侧的光电芯片，如果光电智能的芯片能研究出来，对新基建、工业互联网、 计算机视觉 、光通信和纳米级目标 感知 与识别这方面都能带来非常大的作用，也是算力提升的一个最重要的方向。\n",
      "\n",
      "二、算法\n",
      "\n",
      "算法牵扯到我们的核心。要让电脑像成人般下棋比较容易 , 让它把一个东西放在一个桌子上也非常简单。但是莫拉维克悖论指出，要让电脑有如1岁小孩般的 感知 和行动能力却是相当困难，甚至是不可能的。因此，如何实现高效，而且这个算法还能解释清楚，包括鲁棒的新一代智能，现在国内外都在研究。\n",
      "\n",
      "脑科学对 人工智能 算法的启示。 人工智能 算法的进步都离不开脑科学的积累和呈现，包括算法层次的解释、启发卷积 神经网络 。Hubel 从1958 年发现简单和复杂的细胞，发现视觉系统的卷积特性。还有一类，类脑计算、脑启发、脑科学怎么做的。上面是 1907 年脉冲 神经元 的认知问题；1981 年美国加州提出了单板模拟百万 神经元 的计算， IBM 公司做了这方面的工作；直到现在， 清华大学 的专家都在类脑和存算一体上做了非常重要的工作。 人工智能 算法怎么考虑，下面这些做 人工智能 算法的很多科学家都是认知科学家，而不是 人工智能 的信息科学家；上面脑观测成果，脑科学为 人工智能 启示和认知科学家对 人工智能 的理解搭建了桥梁。\n",
      "\n",
      "整体分析可以看到，脑科学家对信息传递机制、信息解释机制有一批获诺贝尔奖； 人工智能 方面，尤其是心理学家、认知科学家提出了一系列 人工智能 的算法，他们获得了图灵奖。所以人类如何思考和机器如何思考是有关系的，怎么建立这样一个恰当关系，是要研究的重要环节。大脑工作机理、信息的传递，工作的功耗非常低，只有 20 瓦，怎样找到新机制，找到新一代 人工智能 算法非常重要。从大机制来看，我们对神经细胞的理解，这时候是 感知 的智能。现在大部分都通过核磁共振对宏观图像理解，也就是现在提出的弱 人工智能 ，如果把这三个打通，从微观、介观到宏观结合起来，对全脑的认知能不能做一个强认知智能。这是整体的脑科学和机理建立起来，构建一个认知桥，多模态的观测，通过先进神经技术，揭示脑结构、脑功能与智能产生的多层次关联与多模态影射机制，建立认知模型和类脑智能体系。\n",
      "\n",
      "美国 2016 年就启动了一个阿波罗项目，1 亿美金，3 个课题组共同联合，有做 机器学习 ，有做脑科学的，有做脑成像，有做脑机理，要做 1个立方毫米、10 万 神经元 的解析，把它们的连接打通。上面是 神经元 模型，下面是 机器学习 模型，能不能揭秘 映射 关系。尽管斑马鱼、小鼠 神经元 都不同，斑马鱼才有 1 千 神经元 ，果蝇不到 10 万 神经元 ，人类最多 8 百多亿 神经元 ，但是它们都具有通用智能。所以通用智能怎样看待它对环节的理解，人对复杂环境的理解，斑马鱼对复杂环境的理解，生存的环境它们都能理解，因此都具备通用智能。但是完全靠它很难解释一个通用智能的诞生。最近 IBM 用果蝇通用智能方法研制了一套武器装备系统。果蝇 10 万 神经元 有 8 万是视觉系统。研究符合人脑进化过程的新一代 人工智能 理论体系能不能构建起来，这是大家要思考的问题。因此，我们对脑科学里，尤其是机理做了分析，既有记忆痕迹假设，也有海马体和记忆，一直到记忆与脑区的关系。通过记忆能不能构建起一个新的 人工智能 算法，这是另外一条通路，我们在做试验。\n",
      "\n",
      "于是提出了生物机制，包括记忆环路。如果记忆环路超出界限，我们用物理的熵平衡把这个机理建模，最后能不能提出一个新的 人工智能 算法，这是清华提出的 人工智能 算法框架。这里需要反馈验证，所以提出了生物科学机制的发掘，包括数学物理机制的约束，一直到新一代 神经网络 ，这样一个自学习的作用。这里给出一个框架，科学家希望能在这个框架下研究算法和工作。我们要做 人工智能 新的算法，一定要去打通脑科学机理；第二通过知识驱动和数据趋同共同构架一个架构，这是 人工智能 算法的初步分析。\n",
      "\n",
      "三、测试\n",
      "\n",
      "既然 人工智能 算法这样做了，现在我们一直要想到，算法好、算法坏是不是应该测试一下。所以， 人工智能 从 2016 年到未来这一段时间发展非常快，各种游戏、工具，以及各种与人类相关的这种工作都被 人工智能 取代。但是，这些东西能不能做的更好，是不是已经完结，需要做一套测试。图灵首先给出了一个测试判断 人工智能 的水平。第一代 图灵测试 进行过分析， 图灵测试 的提出，包括到 1986 年早期 自然语言处理 ，到现在程序首次通过了 图灵测试 ， 人工智能 终于能像人类一样学习并通过了 图灵测试 。第二代有很多科学家都在研究 图灵测试 ，对深度网络怎么测试，现在已经从通用测试到专用测试，具有了测试机器 常识推理 的能力，通过标准考试的能力。以前是给一个通用测试，70%，现在通用智能测试不能起作用，专用测试 神经网络 抽象推理能力。这是第二代，专用测试。\n",
      "\n",
      "新一代 图灵测试 ，现在讲新一代的认知智能，从专用智能要走向通用智能，以前我们所提到的 图灵测试 还能够测试认知智能吗？这又提出了一个新问题，也是一个新方向。我们从脑、认知、智能， 人工智能 理论从局部发展到全局发展，怎么做这样的测试，这是需要挖掘和发展的。如何实现具有功能识别、 逻辑 还能推理、认识还能决策的新一代认知智能，要达到这三个要素，我们称为新一代 人工智能 特征。\n",
      "\n",
      "我们测试什么，按照功能识别、 逻辑 推理和认知决策这三方面测试，可以分布测试也可以整体测试，这是对新一代 人工智能 提出的目标和要求，给出新的测试方向。我们以前做认知智能时，微观、宏观和全脑的介观尺度观测，类脑计算技术起到很大作用，也对测试带来了新挑战。大脑的信息 80% 来自视觉，包括人类独有的语言功能， 感知 外界环境，理解建模外界环境，与外界环境交互，怎样做决策、记忆与学习， 感知 世界、理解世界是 人工智能 最重要的目标。从这里大家可以看到，多模态回路观测技术揭示了脑结构、脑功能与智能产生的多层次关系，所以认知测试应该是未来一个新方向。新一代 图灵测试 逻辑 推理，功能识别到认知决策，目前部分方面国际上已经做了很多贡献，也在初步发展这条路径。希望在座的，包括科学家、包括产业界都能够在 图灵测试 上发掘一些力量。\n",
      "\n",
      "最后总结一下。在算力上包括光电计算在这里起到很大作用，从算法上我们更希望能不能更接近、更逼近本原的认知计算理论与方法；第二，脑科学启发 人工智能 ；第三，认知测试层面，能不能提出新一代 图灵测试 ，功能识别、 逻辑 推理、认知决策这方面给出一个新方向。\n",
      "\n",
      " \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-26\n",
      "title= 这可能会引领通用AI的下一个重大突破\n",
      "author= []\n",
      "publish_date= 2020-11-26 00:00:00\n",
      "text= 研究者们正在教一个大型的语言模型如何去“看”以帮助他们更好的理解这个世界。\n",
      "\n",
      "您可能已经听我们说过无数次了：可以生成类似人类语言的大型 人工智能 模型GPT-3是一个奇迹，也是一个大型的海市蜃楼。您可以用一个简单的技巧来辨别：询问它绵羊的颜色，它回答“黑色”的次数和“白色”一样多——这反映出“黑色绵羊”这一短语出现在我们日常用语中。\n",
      "\n",
      "这就是 语言模型 的一个问题，因为他们仅在文本上进行训练，缺乏常识。最近来自北加利福尼亚大学的研究者，Chapel Hill设计了一个新的技术来解决这一问题。他们称该技术为vokenization, 该技术赋予了诸如GPT3这样的模型“看”的能力。\n",
      "\n",
      "这并非人类第一次尝试将 语言模型 和 计算机视觉 相结合，实际上这是一个快速发展的AI领域。产生这种想法是因为两种类型的AI都有不同的优势。像GPT-3这样的 语言模型 是通过无监督学习进行训练的，该过程不需要手动数据标记，因此易于扩展。相比之下，像目标识别系统这样的图像模型可以直接从现实中学习到更多。换句话说，他们学到的东西并不依赖于文本所提供的内容。他们可以从绵羊的照片中“看到”他们实际上是白色的。\n",
      "\n",
      "可以解析语言和视觉输入的AI模型也有非常实际的用途。例如，如果我们要构建机器人助手，则他们需要 计算机视觉 来在世界中进行导航，需要语言来与人类进行交流。\n",
      "\n",
      "但是，将两种类型的AI结合起来说起来容易做起来难。这并非简单地将现有 语言模型 与现有目标识别系统装订在一起。它需要使用包含文本和图像的数据集从头开始训练新模型，该数据集也称为视觉语言数据集。\n",
      "\n",
      "获得此类数据集的最常用方法是收集带有描述性标题的图像集合。例如，下面的图片的标题为“一只橘猫坐在准备打包的手提箱里。” 这与典型的图像数据集不同，后者仅用一个名词来标记下面的图片，例如“猫”。因此，一种视觉语言数据集不仅可以教一个AI模型如何识别目标，而且还能使用动词和介词来告诉模型目标之间是如何相互影响和相互作用的。\n",
      "\n",
      "但是制作这种数据集非常耗时。这就是为什么现有的视觉数据集如此微不足道。一个常用的纯文本数据集，如英文Wikipedia（实际上几乎包括所有英语Wikipedia条目），可能包含近30亿个单词。像MS COCO这样的视觉语言数据集仅包含700万。根本没有足够的数据来训练AI模型以提供有用的信息。\n",
      "\n",
      "“Vokenization”解决了这个问题，它使用无监督学习方法将MS COCO中的少量数据缩放到英文Wikipedia的大小。在当今用于评估AI语言理解力最困难的测试中，经过该训练集训练的视觉 语言模型 优于目前最好的模型。\n",
      "\n",
      "自然语言处理 初创公司HuggingFace的联合创始人兼首席科学官托马斯·沃尔夫（Thomas Wolf）表示：“不进行大的变动，你无法在这些测试上超过最先进的水平。”“这不是简单的测试。这就是为什么这令人如此兴奋。”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从token到voken\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先让我们理清这些术语，究竟什么是voken？\n",
      "\n",
      "在AI语言中，用于训练 语言模型 的单词称为标记(token)。因此，UNC研究人员决定将视觉 语言模型 中与每个标记相关的图像称为“voken”。为每个token查找voken的算法叫Vokenizer，整个过程称为vokenization。\n",
      "\n",
      "这样做的目的不仅是为了显示AI研究人员有多喜欢编造单词。（他们的确如此）。这也有助于理解vokenization背后的基本思想。UNC研究人员不是从图像数据集开始并手动写句子作为标题（这是一个非常缓慢的过程），而是从语言数据集开始，并使用无监督学习来将每个单词与相关图像进行匹配（稍后会详细介绍）。这是一个高度可扩展的过程。\n",
      "\n",
      "因此无监督学习技术才是本论文最大的贡献，即如何为每个单词找到相关图像。\n",
      "\n",
      "Vokenization\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "让我们回到GPT-3。GPT-3是transformer 语言模型 家族的一员，2017年transformer的出现带了重大的突破，因为其将无监督学习应用到 自然语言处理 上。transformer通过观察词在上下文中的用法来学习人类语言的模式，然后根据该上下文为每个词创建数学表示，称为“ 词嵌入 ”。例如，“猫”一词的嵌入可能表明，它经常在“喵”和“橘”两词周围使用，而在“树皮”或“蓝色”等词周围较少使用。\n",
      "\n",
      "这就是transformer如何近似的表达词的含义，以及GPT-3如何编写类似人类的句子。它部分地依靠这些嵌入来告诉它如何将单词组合成句子，将句子组合成段落。\n",
      "\n",
      "有一种并行技术也可以用于图像。它不通过扫描文本来查找单词使用模式，而是扫描图像以查找视觉模式。比如说它列出了猫出现在床上而不是树上出现的频率，并利用该上下文信息创建了‘猫’嵌入。\n",
      "\n",
      "UNC研究人员的想法是，他们应该在MS COCO上同时使用两种嵌入技术。他们将 图像转换 为视觉嵌入，将标题转换为 词嵌入 。这些嵌入的真正精巧之处在于可以将它们嵌入三维空间中，并直接看到它们之间的关系。与 词嵌入 紧密相关的视觉嵌入会在图中显示得更近。换句话说，视觉猫嵌入（理论上）应与基于文本的猫嵌入重叠。这很酷。\n",
      "\n",
      "您应该可以看到下一步如何走。一旦将所有嵌入进行图形化表示并与其他嵌入进行比较和关联，就可以轻松地将图像（vokens）与单词（tokens）进行匹配。请记住，由于图像和单词是根据其嵌入进行匹配的，因此在上下文中他们也是匹配的。当一个词有完全不同的含义时，这会很有用。该技术通过为词的每个实例找到不同的voken来成功地解决这一问题。\n",
      "\n",
      "例如：\n",
      "\n",
      "Hereis her contact.\n",
      "\n",
      "这是她的联系方式。\n",
      "\n",
      "Some cats love human contact.\n",
      "\n",
      "有些猫喜欢被人抚摸。\n",
      "\n",
      "在两个示例中都用到了‘contact’。但是在第一句中，上下文语境暗示该词是指联系信息，因此，voken是一个联系人图标。在第二句中，上下文提示该词是指触摸，因此，voken显示了一只猫正在被抚摸了。\n",
      "\n",
      "研究人员使用MS COCO创建的视觉和 词嵌入 来训练其vokenizer算法。训练完成后，vokenizer便可以在英语维基百科中为每个token找到对应的voken。这不是完美的。该算法仅为大约40％的tokens找到了vokens。但这仍然是拥有近30亿个字的数据集的40％。\n",
      "\n",
      "利用这个新的数据集，研究人员重新训练了BERT的 语言模型 ，BERT是Google早于GPT-3开发的一种开源transformer。然后，他们在六种不同的语言理解测试中测试了新改进的BERT，包括SQuAD，斯坦福问题回答数据集（该模型要求模型回答有关一系列文章的阅读理解问题）和SWAG，SWAG试图利用英语的微妙之处来测试模型以探究它是否只是模仿和记忆。改进后的BERT对所有这些都表现更好，Wolf说这需要引起重视。\n",
      "\n",
      "研究人员，博士研究生Hao Tan和他的导师MohitBansal将在EMLNLP会议上介绍其新的vokenization技术。尽管这项工作还处于初期阶段，但Wolf认为他们的工作是使无监督学习适用于视觉 语言模型 的一项重要的概念突破。这有助于大大推动 自然语言处理 的发展。\n",
      "\n",
      "他说：“在NLP上，两年前我们取得了巨大的突破，然后突然间NLP成为了一个正在发生很多事情的领域，它领先于所有其他AI领域。”“但是我们有将文本与其他事物联系起来的问题。否则它就像只会说话却看不见，听不到的机器人一样。”\n",
      "\n",
      "他说：“这篇论文是他们设法将文本连接到另一种方式的一个例子，并且效果很好。”“你可以想象，当你想在机器人中利用这种功能强大的 语言模型 时，其中某些技术可能会被重用。也许您使用同一种技术将机器人的感官与文本联系起来。”\n",
      "\n",
      "论文标题：\n",
      "\n",
      "Vokenization:Improving Language Understanding with Contextualized, Visual-GroundedSupervision\n",
      "\n",
      "论文链接：\n",
      "\n",
      "https://www.aclweb.org/anthology/2020.emnlp-main.162/\n",
      "\n",
      "原文标题：\n",
      "\n",
      "This could lead to the next big breakthrough in common sense AI\n",
      "\n",
      "原文链接：\n",
      "\n",
      "https://www.technologyreview.com/2020/11/06/1011726/ai-natural-language-processing-computer-vision/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-25-2\n",
      "title= 图像分类：来自13个Kaggle项目的经验总结\n",
      "author= []\n",
      "publish_date= 2020-11-25 00:00:00\n",
      "text= 本文作者与你分享图像分类项目经验总结。\n",
      "\n",
      "任何领域的成功都可以归结为一套小规则和基本原则，当它们结合在一起时会产生伟大的结果。\n",
      "\n",
      "机器学习和图像分类也不例外，工程师们可以通过参加像Kaggle这样的竞赛来展示最佳实践。\n",
      "\n",
      "在这篇文章中，我将给你很多资源来学习，聚焦于从13个Kaggle比赛中挑选出的最好的Kaggle kernel。这些比赛是:\n",
      "\n",
      "Intel Image Classification：https://www.kaggle.com/puneet6060/intel-image-classification\n",
      "\n",
      "Recursion Cellular Image Classification：https://www.kaggle.com/c/recursion-cellular-image-classification\n",
      "\n",
      "SIIM-ISIC Melanoma Classification：https://www.kaggle.com/c/siim-isic-melanoma-classification\n",
      "\n",
      "APTOS 2019 Blindness Detection：https://www.kaggle.com/c/aptos2019-blindness-detection/notebooks\n",
      "\n",
      "Diabetic Retinopathy Detection：https://www.kaggle.com/c/diabetic-retinopathy-detection\n",
      "\n",
      "ML Project — Image Classification：https://www.kaggle.com/c/image-classification-fashion-mnist/notebooks\n",
      "\n",
      "Cdiscount’s Image Classification Challenge：https://www.kaggle.com/c/cdiscount-image-classification-challenge/notebooks\n",
      "\n",
      "Plant seedlings classifications：https://www.kaggle.com/c/plant-seedlings-classification/notebooks\n",
      "\n",
      "Aesthetic Visual Analysis：https://www.kaggle.com/c/aesthetic-visual-analysis/notebooks\n",
      "\n",
      "我们会讨论调试深度学习解决方案的三个主要方面：\n",
      "\n",
      "数据\n",
      "\n",
      "模型\n",
      "\n",
      "损失函数\n",
      "\n",
      "还有很多例子项目(和参考资料)供你参考。\n",
      "\n",
      "数据\n",
      "\n",
      "图像预处理 + EDA\n",
      "\n",
      "每一个机器学习/深度学习解决方案都从原始数据开始。在数据处理管道中有两个基本步骤。\n",
      "\n",
      "第一步是探索性数据分析 (EDA）。它帮助我们分析整个数据集并总结它的主要特征，比如类分布、大小分布等等。通常使用可视化方法来显示这种分析的结果。\n",
      "\n",
      "第二步是图像预处理，目的是对原始图像提高图像数据(也称为图像特征)的质量，通过抑制不必要的扭曲，缩放，增强重要的特征，使数据更适合模型并提高性能。\n",
      "\n",
      "你可以钻研这些Kaggle笔记本，看看一些图像预处理技术：\n",
      "\n",
      "Visualisation：https://www.kaggle.com/allunia/protein-atlas-exploration-and-baseline#Building-a-baseline-model-\n",
      "\n",
      "Dealing with Class imbalance：https://www.kaggle.com/rohandeysarkar/ultimate-image-classification-guide-2020\n",
      "\n",
      "Fill missing values (labels, features and, etc.)：https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble\n",
      "\n",
      "Normalisation ：https://www.kaggle.com/vincee/intel-image-classification-cnn-keras\n",
      "\n",
      "Pre-processing：https://www.kaggle.com/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy#3.A-Important-Update-on-Color-Version-of-Cropping-&-Ben's-Preprocessing\n",
      "\n",
      "数据增强\n",
      "\n",
      "数据增强可以通过从现有的训练样本中生成更多的训练数据来扩展我们的数据集。通过大量的随机转换生成新的样本，这些转换不仅可以生成可信的图像，而且还反映了真实的场景 —— 稍后将对此进行详细介绍。\n",
      "\n",
      "这种技术得到了广泛的应用，不仅仅是在训练模型的数据样本太少的情况下。在这种情况下，模型开始记忆训练集，但无法泛化(在从未见过的数据上表现很差)。\n",
      "\n",
      "通常，当一个模型在训练数据上表现很好，但在验证数据上表现很差时，我们称之为过拟合。为了解决这个问题，我们通常会尝试获取新数据，如果没有可用的新数据，则可以使用数据增强。\n",
      "\n",
      "注：一般的经验法则是始终使用数据增强技术，因为它有助于使我们的模型见识更多的变化并更好地泛化。即使我们有一个很大的数据集，也要使用数据增强，但这是以较慢的训练速度为代价的，因为增强是在线完成的(即在训练期间)。\n",
      "\n",
      "此外，对于每个任务或数据集，我们必须使用反映可能的现实场景的增强技术(例如，如果我们有一个猫/狗探测器，我们可以使用水平翻转、剪裁、亮度和对比度，因为这些增强匹配不同的照片拍摄方式。\n",
      "\n",
      "这里是一些Kaggle比赛notebooks，你可以查看流行的数据增强技术：\n",
      "\n",
      "Horizontal Flip：https://www.kaggle.com/datafan07/analysis-of-melanoma-metadata-and-effnet-ensemble\n",
      "\n",
      "Random Rotate and Random Dihedral：https://www.kaggle.com/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb\n",
      "\n",
      "Hue, Saturation, Contrast, Brightness, Crop：https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords\n",
      "\n",
      "Colour jitter：https://www.kaggle.com/nroman/melanoma-pytorch-starter-efficientnet\n",
      "\n",
      "模型\n",
      "\n",
      "开发一个基线\n",
      "\n",
      "\n",
      "\n",
      "在这里，我们使用一个非常简单的架构创建一个基本的模型，没有任何正则化或dropout层，看看我们是否能超过50%的准确率基线。尽管我们不可能总能达到这个目标，但如果我们在尝试了多种合理的架构后不能超过基线，那么输入数据可能不包含模型进行预测所需的信息。\n",
      "\n",
      "用Jeremy Howard的名言：\n",
      "\n",
      "“你应该能够在15分钟内使用50%或更少的数据集快速测试你是否正在朝着一个有希望的方向前进，如果没有，你必须重新考虑一切。”\n",
      "\n",
      "开发一个足够大可以过拟合的模型\n",
      "\n",
      "一旦我们的基线模型有足够的能力超过基线分数，我们就可以增加基线模型的能力，直到它在数据集上过拟合为止，然后我们就开始应用正则化。我们可以通过以下方式增加模块容量：\n",
      "\n",
      "添加更多层\n",
      "\n",
      "使用更好的结构\n",
      "\n",
      "更完善的训练流程\n",
      "\n",
      "结构\n",
      "\n",
      "根据文献，以下架构的改进提高了模型的容量，但几乎没有改变计算复杂度。\n",
      "\n",
      "Residual Networks\n",
      "\n",
      "Wide Residual Networks\n",
      "\n",
      "Inception\n",
      "\n",
      "EfficientNet\n",
      "\n",
      "Swish activation\n",
      "\n",
      "Residual Attention Network\n",
      "\n",
      "大多数时候，模型容量和精度是正相关的 —— 随着容量的增加，精度也会增加，反之亦然。\n",
      "\n",
      "训练过程\n",
      "\n",
      "下面是一些你可以用来调整你的模型的训练过程，通过实例项目来看看它们是如何工作的：\n",
      "\n",
      "Mixed-Precision Training\n",
      "\n",
      "Large Batch-Size Training\n",
      "\n",
      "Cross-Validation Set\n",
      "\n",
      "Weight Initialization\n",
      "\n",
      "Self-Supervised Training (Knowledge Distillation)\n",
      "\n",
      "Learning Rate Scheduler\n",
      "\n",
      "Learning Rate Warmup\n",
      "\n",
      "Early Stopping\n",
      "\n",
      "Differential Learning Rates\n",
      "\n",
      "Ensemble\n",
      "\n",
      "Transfer Learning\n",
      "\n",
      "Fine-Tuning\n",
      "\n",
      "超参数调试\n",
      "\n",
      "与参数不同，hyperparameters是由你在配置模型时指定的(即学习率、epoch的数量、hidden units的数量、batch size大小等)。\n",
      "\n",
      "你可以通过使用hyperparameter调优库，比如Scikit learn Grid Search，Keras Tuner来自动化这个过程，而不是去手动配置。这些库会在你指定的范围内尝试所有的hyperparameter组合，返回表现最好的模型。\n",
      "\n",
      "需要调优的超参数越多，过程就越慢，因此最好选择模型超参数的最小子集进行调优。\n",
      "\n",
      "并不是所有的模型超参数都同样重要。一些超参数会对机器学习算法的行为产生巨大的影响，进而影响其性能。你应该小心地选择那些对模型性能影响最大的参数，并对它们进行调优以获得最佳性能。\n",
      "\n",
      "正则化\n",
      "\n",
      "这种方法迫使模型学习有意义和具有泛化能力的数据表示，通过对记忆/过拟合和欠拟合进行惩罚来实现，使模型对于它没见过的数据更鲁棒。\n",
      "\n",
      "解决上述问题的一个简单方法是获得更多的训练数据，因为一个模型训练的数据越多，自然就会泛化得越好。\n",
      "\n",
      "这里有一些技巧你可以试着减轻过拟合和欠拟合，项目如下：\n",
      "\n",
      "Adding Dropout：https://www.kaggle.com/allunia/protein-atlas-exploration-and-baseline\n",
      "\n",
      "Adding or changing the position of Batch Norm：https://www.kaggle.com/allunia/protein-atlas-exploration-and-baseline\n",
      "\n",
      "Data augmentation：https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords\n",
      "\n",
      "Mixup：https://arxiv.org/abs/1710.09412\n",
      "\n",
      "Weight regularization：https://www.kaggle.com/allunia/protein-atlas-exploration-and-baseline\n",
      "\n",
      "Gradient clipping：https://www.kaggle.com/allunia/protein-atlas-exploration-and-baseline\n",
      "\n",
      "损失函数\n",
      "\n",
      "损失函数也被称为成本函数或目标函数，用于查找目标输出的模型之间的差异，并帮助模型最小化它们之间的距离。\n",
      "\n",
      "这里是一些最流行的损失函数，与项目实例，你会发现一些技巧，以提高你的模型的能力：\n",
      "\n",
      "Label smoothing\n",
      "\n",
      "Focal loss\n",
      "\n",
      "SparseMax loss and Weighted cross-entropy\n",
      "\n",
      "BCE loss, BCE with logits loss and Categorical cross-entropy loss\n",
      "\n",
      "Additive Angular Margin Loss for Deep Face Recognition\n",
      "\n",
      "评估 + 错误分析\n",
      "\n",
      "在这里，我们做消融研究，并分析我们的实验结果。我们确定了我们的模型的弱点和长处，并确定了未来需要改进的地方。在这个阶段，你可以使用以下技术，并在链接的示例中查看它们是如何实现的：\n",
      "\n",
      "Tracking metrics and Confusion matrix：https://www.kaggle.com/vincee/intel-image-classification-cnn-keras\n",
      "\n",
      "Grad CAM：https://arxiv.org/pdf/1610.02391v1.pdf\n",
      "\n",
      "Test Time Augmentation (TTA)：https://www.kaggle.com/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb\n",
      "\n",
      "有许多实验跟踪和管理工具，采取最小设置为你自动保存所有数据，这使消融研究更容易。\n",
      "\n",
      "最后\n",
      "\n",
      "有许多方法来调整你的模型，并且新的想法总是会出现。深度学习是一个快速发展的领域，没有什么灵丹妙药。我们必须做很多实验，足够的试验和错误会带来突破。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-24-2\n",
      "title= 用归纳偏置来增强你的模型性能\n",
      "author= []\n",
      "publish_date= 2020-11-24 00:00:00\n",
      "text= 本文为大家展示了在机器学习模型中编码现实生活中的对称性可以将其准确性提高几个数量级。\n",
      "\n",
      "\n",
      "\n",
      "对称无处不在，围绕在我们生活左右。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从雪花美丽的规则形状和罗马花椰菜的自相似(分形)结构，到蜂巢的六角形图案。大自然似乎在寻找对称。事实上，我们存在的法则实在是太多了:物理学家谈到时空中的平移对称(“穿过”)。他们的意思是，像万有引力这样的力在数百万年前和现在的作用方式是一样的，在悉尼和纽约之间没有差别。旋转对称是自然最喜欢的另一种形式，简单地说就是当你从不同角度看一个物体时，它的属性不会改变。对称的例子不胜枚举，其中一些比其他的更容易理解(比如洛伦兹对称，它说明在惯性系中光速是相同的，这可以被不太懂物理的人所理解)。尽管其中一些对称性对人类来说是显而易见的，但令人惊讶的是，大多数机器学习模型都对它们的存在视而不见。让我以我自己的工作为例:\n",
      "\n",
      "大致来说，我的研究目标是利用机器学习仅从结构信息来预测分子的性质。这意味着，我得到一列原子和它们的坐标。对于一个水分子，它看起来是这样的:\n",
      "\n",
      "原子的坐标可以方便地归纳成一个矩阵，矩阵的行对应于原子，列分别对应于x、y和z的位置。我想预测需要多少能量需要把分子分解成它的组成原子(原子化能量)。我可以通过训练神经网络F来做到这一点，它使用原始坐标作为特征并输出能量:\n",
      "\n",
      "假设我成功地在一个大且不同的分子数据集上训练了这个神经网络，我想找到以下水分子的原子化能量:\n",
      "\n",
      "你可能已经注意到了，它只是原始分子的一个旋转版本。因此，它应该具有相同的原子化能。我们能保证神经网络会遵守这个旋转对称吗?不幸的是,没有。\n",
      "\n",
      "更糟糕的是，如果我们简单地交换两个氢原子。网络可能会再次给出一个完全不同的答案。相同原子的顺序没有物理意义，只是神经网络训练出来的人为产物。理想情况下，我们希望神经网络的输出尊重这种排列对称(排列:“交换事物的顺序”)，但如何做到呢?\n",
      "\n",
      "在化学和物理应用机器学习的早期，研究者很快意识到模型需要观察这些对称性才能足够精确。因此，人们投入了大量的精力来研究如何在机器学习算法体现出对称性。现在，这通常是通过巧妙的特征工程和神经网络设计相结合来实现的。关于这些方法的全面评述可以在这里（https://aip.scitation.org/doi/full/10.1063/1.4966192）找到[1]。所有这些方法都有一个共同点，就是它们以某种形式向学习算法引入了归纳偏置。\n",
      "\n",
      "学习算法的归纳偏置是一组假设，学习者用来预测未遇到[2]的给定输入的输出。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在旋转对称的情况下，这种归纳偏置可以表述为假设:“任何在旋转下不变的信息可以而且应该被忽略。” 不管你是否意识到这一点，如果你以前使用过任何机器学习模型，你就会遇到归纳偏置:\n",
      "\n",
      "线性回归是基于因变量与协变量之间存在线性关系的假设。\n",
      "\n",
      "k近邻分类器假设特征空间中的邻近转换为输出空间中的邻近。\n",
      "\n",
      "卷积神经网络假设输出在很大程度上不受输入转换的影响(不考虑边界条件)。\n",
      "\n",
      "虽然所有这些标准算法都有一些内在的偏差，但我想通过一个例子来演示如何引入额外的假设可以大大提高模型的准确性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "例子:房地产价格模型\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "想象下面的场景:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "您的客户对买卖房地产感兴趣，并要求您创建一个机器学习模型，准确预测建筑物的公平市场价值。幸运的是，训练数据很容易获得，但有些有限(约1000个样本)。更复杂的是，这栋建筑的任何信息都是以房屋单元为单位给出的，除了价格是以整栋建筑为单位给出的。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了简化我们的分析，让我们假设每个建筑包含10个单元，我们有关于每个单元的以下信息:\n",
      "\n",
      "单元类型:零售，办公，生活空间\n",
      "\n",
      "单元面积：以平方英尺为单位\n",
      "\n",
      "房间数量\n",
      "\n",
      "窗户与墙壁的比率\n",
      "\n",
      "单元在哪一层(以楼层总数的比率给出)\n",
      "\n",
      "我们的数据以表格形式，有1000行(样本数量)和51列(5个特征 10个单元+总价)。让我们把模型输入(前50列)写成设计矩阵X，因变量写成向量y。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "线性回归（LR）\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "普通 LR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "对于任何回归任务来说，一个好的起点总是线性回归，它的目标是选择权重w，以便模型预测的平均平方误差最小化。\n",
      "\n",
      "为了分析这个模型基于我们的数据表现得有多好，我们可以查看与训练集大小相关的泛化误差(测试集上的误差)。绘制我们的结果，我们得到了通常所说的学习曲线:\n",
      "\n",
      "注意，我们已经包含了一个基线模型，它仅仅预测了训练集的平均价格。我们可以从这张图中推断出几件事。我们看到，对于非常小的训练集，基线模型比LR更准确，然而，当我们使用更大的数据集时(注意log-log坐标!)，我们可以在基线上进行改进。但是100万美元的均方根误差(RMSE)并不是很好的表现，它对我们虚构的客户也没有用处。我们希望能做得更好…\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "置换不变LR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "让我们想想对称性的问题:在设计矩阵X中以何种顺序罗列单元重要吗?答案显然是否定的。我们可以将第1-5列与第6-10列互换，但该建筑仍然拥有相同的净值。然而，我们的普通LR并不能准确地捕捉到这一点，因为不能保证权重1−5和6−10是相同的。目标很明确:我们需要我们的模型在单元排列下保持不变。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "。考虑每单元(每5列一组)作为一个单独的数据点，矩阵原始X转化后维数为10000a✖5由。线性回归则为 让我们考虑一个稍微不同的版本,我们的设计矩阵,我们将称之为。考虑每单元(每5列一组)作为一个单独的数据点，矩阵原始X转化后维数为10000a✖5由。线性回归则为\n",
      "\n",
      "z是每个单元的价格。不幸的是我们无法得知z(潜变量),但是我们可以假设z和y之间的关系。由于我们想保持在线性模型的范围内，我们得到：\n",
      "\n",
      "简而言之，就是一栋楼的价格是这栋楼内每一单元价格的总和。通过引入一个适当的矩阵L，我们可以用矩阵L表示所有建筑物i的总和：\n",
      "\n",
      "线性回归模型可以写成：\n",
      "\n",
      "意味着我们总结一个建筑所有单元的特征,所以我们只使用建筑总面积而不是每个单元的面积。所以对于线性回归来说，强加排列对称真的是一个微不足道的任务。当我们转到内核方法时，使用这种更抽象的表示法的优势将变得清晰起来。\n",
      "\n",
      "使用置换不变LR，我们得到以下学习曲线:\n",
      "\n",
      "虽然我们看到了小数据集的实质性改善，但模型似乎没有从数据中学到很多东西，导致曲线平坦。这表明线性模型的表达能力不足以捕获训练数据中包含的所有细节。显然，线性假设过于强烈，对于一个“健康”模型，我们期望学习曲线在对数-对数坐标(表示预期的幂律行为)上是线性的[3]。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "高斯过程回归(GPR)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "普通GPR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "让我们考虑一个更复杂的模型。我们有几个选择:神经网络当然很受欢迎，但对于如此小的数据集来说显然不可靠。有效的选择是基于树的方法和k-means方法。我个人最喜欢的是基于核函数的方法，特别是高斯过程回归(GPR)。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "映射输入,或自变量x对因变量y(价格)。在高斯过程回归中，我们用贝叶斯方法先找到这个函数，即首先在所有可能的f上指定一个高斯先验分布，然后对观测数据点(X,y)进行条件处理（即基于先验和一定的假设（联合高斯分布）计算得到高斯过程后验分布的均值和协方差。）。这个先验通过协方差矩阵为k的高斯过程定义： 我们正在寻找一个函数映射输入,或自变量x对因变量y(价格)。在高斯过程回归中，我们用贝叶斯方法先找到这个函数，即首先在所有可能的f上指定一个高斯先验分布，然后对观测数据点(X,y)进行条件处理（即基于先验和一定的假设（联合高斯分布）计算得到高斯过程后验分布的均值和协方差。）。这个先验通过协方差矩阵为k的高斯过程定义：\n",
      "\n",
      "遵循正态分布。进一步,两个距离较远的点x和x’点的输出在在定义为 的协方差条件下是联合正态分布的。在实践中,这意味着我们可以通过选择一个合适的协方差函数(也称为核) 来确定拟合函数f的形状。 高斯过程是随机变量的集合，任意有限个数的随机变量都具有联合高斯分布[4]。对于x的任何值,输出遵循正态分布。进一步,两个距离较远的点x和x’点的输出在在定义为的协方差条件下是联合正态分布的。在实践中,这意味着我们可以通过选择一个合适的协方差函数(也称为核)来确定拟合函数f的形状。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "很小)，点之间将是高度相关的。让我们回到我们的例子。一旦我们在训练数据上设定高斯过程的条件，我们就可以在测试集上做出预测。 一种非常流行的核函数选择是平方指数协方差(有时也称为径向基函数)。选择这个内核,我们基本上设定f是光滑的，因为当点很近时(即很小)，点之间将是高度相关的。让我们回到我们的例子。一旦我们在训练数据上设定高斯过程的条件，我们就可以在测试集上做出预测。\n",
      "\n",
      "已经取代了X。因为w的参数方程是线性的,它仍然是直接可解的： 用我们设计的矩阵x和目标值y,给出了模型的预测。这看起来非常类似于线性回归,除了已经取代了X。因为w的参数方程是线性的,它仍然是直接可解的：\n",
      "\n",
      "并乘以单位矩阵。该参数用于拟合数据中的噪声,同时有助于避免矩阵求逆时的数值问题。 注意,我们引入参数并乘以单位矩阵。该参数用于拟合数据中的噪声,同时有助于避免矩阵求逆时的数值问题。\n",
      "\n",
      "将我们的训练数据代入这些方程，我们得到以下学习曲线:\n",
      "\n",
      "嗯，这不是很有效。虽然我们给了模型比LR更多的表达能力，但它的测试误差仍然与后者量级可比。我们的模型似乎对数据过拟合了。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "置换不变GPR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我们把置换不变性加回去。和前面一样，我们想解决辅助问题。\n",
      "\n",
      "，矩阵L与线性回归问题中相同。 其中，矩阵L与线性回归问题中相同。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在处理了一些项之后，我们得到了下面的方程:\n",
      "\n",
      "重新定义：\n",
      "\n",
      "修正GPR的原始形式：\n",
      "\n",
      "特别好!通过使用对称不变性，我们已经能够将最佳测试误差从大约70万美元减少到3万美元。此外，缩放是线性的(在对数-对数尺度上)，这表明如果需要，我们可以通过收集更多的数据来进一步提高模型的准确性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "回到科学\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我希望我已经能够说服您，有时候在用机器学习模型拟合数据之前，值得停下来三思。通过简单地识别数据中的对称性，我们已经能够将模型的准确性提高20倍以上。作为一个小小的奖励，如果你理解了这篇文章的所有内容，你实际上离成为一个计算化学家又近了一步。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为什么这么说呢？如果你把“建筑”换成“分子”，把“单元”换成“原子”，我们基本上建立了一个化学和材料科学研究人员常用的模型[5]。通过把分子表示成原子贡献的组合，这个模型可以预测一个分子的性质，比如它的能量。就像在我们的例子中，这些原子的贡献是未知的——我们只知道整个分子的能量——但是我们仍然可以用潜在变量z来描述这个问题，从而使它置换不变。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考文献：\n",
      "\n",
      "[1] Behler, Jörg. “Perspective: Machine learning potentials for atomistic simulations.” The Journal of chemical physics 145.17 (2016): 170901.\n",
      "\n",
      "[2] https://en.wikipedia.org/wiki/Inductive_bias\n",
      "\n",
      "[3] C. Cortes, L. D. Jackel, S. A. Solla, V. Vapnik, and J. S. Denker, Learning Curves: Asymptotic Values and Rate of Convergence, Advances in Neural Information Processing Systems (Curran Associates, Inc., 1994),\n",
      "\n",
      "pp. 327–334\n",
      "\n",
      "[4] C. E. Rasmussen & C. K. I. Williams, Gaussian Processes for Machine Learning, the MIT Press, 2006, ISBN 026218253X. c 2006 Massachusetts Institute of Technology\n",
      "\n",
      "[5] Bartók, Albert P., et al. “Gaussian approximation potentials: The accuracy of quantum mechanics, without the electrons.” Physical review letters 104.13 (2010): 136403.\n",
      "\n",
      "原文标题：\n",
      "\n",
      "Supercharge your model performance with inductive bias\n",
      "\n",
      "原文链接：\n",
      "\n",
      "https://towardsdatascience.com/supercharge-your-model-performance-with-inductive-bias-48559dba5133 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-20-4\n",
      "title= 正式商用一周年，5G给我们生活带来了什么变化？\n",
      "author= []\n",
      "publish_date= 2020-11-20 00:00:00\n",
      "text= 近日，关于5G的国内外消息层出不穷。\n",
      "\n",
      "据每日经亚洲评论报道，韩国广播和通信委员会成员洪贞敏表示，因消费者对5G质量低劣、覆盖范围不足和收费高昂不满，有多达562656人已从5G切换回4G服务。有用户表示，切换5G会导致手机电池的电量消耗更快，且无法达到想要的速度。最初，韩国政府和移动运营商声称，5G传输速率比4G LTE快20倍，但实际上运营商的平均5G速度仅比4G LTE快4倍。\n",
      "\n",
      "2019年4月3日，韩国成为世界上第一个提供5G通信服务的国家。但由于连接性差，韩国大量消费者取消了5G合同，重回4G网络怀抱。至于我国，从2019年11月到2020年11月，国内的5G网络商用也刚满一年。同样的，舆论场关于5G的争议声音始终不断，既有人唱衰5G，话题从最初的质疑5G无用变成了现在的质疑5G成本太高等等，也有人看好5G，认为已经实实在在体验到5G的好处。\n",
      "\n",
      "事实上，每一个新生事物都会经历被质疑被期待被讨论的过程，5G也不例外。尤其是2020年是特殊的一年，我们经历了被迫与人保持距离的疫情，也面临着恢复正常生产生活秩序的压力。在这过程中，我们见证了5G保障新中国成立70周年盛典、5G+4K直播春晚，疫情期间全民“云监工”、5G远程医疗、5G远程课堂，5G上珠峰……5G正式商用已过一年，似乎5G的身影已无处不在。那么，5G究竟给我们生活带来了什么？5G全面商用还有什么阻碍？\n",
      "\n",
      "一、基础设施建设现状\n",
      "\n",
      "5G所具有的高速率、广覆盖、低时延特性，为经济社会各行各业数字化智能化转型提供了技术前提和基础平台。依托技术领先、产业先发和市场庞大等优势，5G的快速发展将带动移动通信产业取得突破性进展。由于5G高速率大带宽的要求，需要优先进行骨干网升级，将明显驱动光模块、光纤光缆等通信设备产业优先发展，预计2021年光模块产业规模将达到140亿元，光纤光缆产业规模将达到500亿元。\n",
      "\n",
      "今年是5G网络爆发的一年。由于5G频段提升，5G基站数量大幅增加。截至目前，我国已建成近70万个5G基站，5G终端连接数已超过1.8亿，提前完成2020年5G基站建设目标。目前，我国已经基本实现了地市级5G网络覆盖。\n",
      "\n",
      "具体到三大运营商，截至今年9月，中国移动已在全国完成了35万个5G基站的建设项目，在全国340个地市和重点县提供了5G的商用服务，同时打造了100余个全国集团级的5G龙头示范项目，带动了超过两千个省级的区域特色项目。而联通和电信在共建5G基站方面，截至8月底共享规模达30万站，年底预计开通38万站。据中国信息通信研究院数据，自2020年正式商用起，预计2020年中国5G连接数将达0.04亿个，随着时间推移将迅速增加，到2025年预计将达4.28亿个。\n",
      "\n",
      "二、相关产业建设现状\n",
      "\n",
      "尽管目前很多个人5G手机用户抱怨无法直接感受到5G低延迟的特性和价值，毕竟对于我们使用手机而言，延迟5ms还是50ms其实没区别，但事实上，5G低延迟最大的受益者是物联网，可以更精准地对设备进行遥控。因此，工信部前部长苗圩就5G应用场景给出了“二八率”，即5G的应用场景中约20%是To C，80%是To B。据相关专家分析，除了超高宽带个人消费，5G技术在工业互联网、车联网、智慧医疗等领域率先得到了应用。\n",
      "\n",
      "1、5G+工业互联网\n",
      "\n",
      "2020年是工业互联网发展三年行动计划收官之年，也是5G规模化落地的关键一年，同时也被业内誉为5G与工业互联网融合创新发展的元年。中国信通院发布信息显示，2019年我国工业互联网产业经济规模达2.1万亿元，同比增长47.3%，预计2020年中国工业互联网产业经济规模约为3.1万亿元，同比实际增长47.9%。\n",
      "\n",
      "工业互联网要实现的是人、机、物全面互联。工业互联网平台本质上是在传统云平台的基础上叠加物联网、大数据、人工智能等新兴技术，优化工业流程，提升生产效率。5G与工业互联网融合，打造云边端一体化工业大脑，从目前的进展来看，主要有三个层面：\n",
      "\n",
      "一是在边缘智能层面，5G有利于就近提供算力，将提高设备端的数据处理能力，实现设备的实时响应；\n",
      "\n",
      "二是在无线下沉层面，5G 逐步下沉与TSN、工业互联网等融合，促进PLC、DCS等工业控制器通信能力的提升，让机器之间的互通更加扁平化；\n",
      "\n",
      "三是在应用升级层面，利用5G搭建更宽、更广、更快和更可靠的通信基础设施，同时利用云端的超强计算能力和AI技术对海量数据进行分析和学习，可以打造云、边、端一体化的工业大脑。\n",
      "\n",
      "目前5G+工业互联网已经主要应用在工业设计、制造、质检、运维、安全等关键环节，形成了无损检测、远程维护、无人巡检等典型应用场景。例如，浙江移动通过与杭汽轮集团合作，建立了5G三维扫描建模检测系统。该系统使得检测时间从2-3天降低到3-5分钟，在实现产品全量检测的基础上还建立了质量信息数据库，便于后期质量问题分析追溯；而贵阳市5G实验网综合应用示范项目已完成5G创新实验室对航天云网的平台接入，通过5G网络将海量工业设备信息以超低时延实时上传到云端，实现对整个生产制造过程及设备状态情况进行实时监测。\n",
      "\n",
      "2020年7月，ITU在召开的远程会议上宣布：3GPP 5G技术（含NB-IoT）满足IMT-2020 5G技术标准的各项指标要求，正式被接受为ITU IMT-2020 5G技术标准。这标志着中国5G产业链主导的NB-IoT （窄带物联网）技术，正式被纳入国际通用标准。随着5G大规模商用，基于NB-IoT+4G+5G的几大核心物联网技术将承担起开启万物互联的责任，承载起未来满足大规模物联网场景应用的需求，也必将在工业领域发挥前所未有的作用。\n",
      "\n",
      "2、5G+车联网\n",
      "\n",
      "车联网，指按照一定的通信协议和数据交互标准，在“人-车-路-云”之间进行信息交换的网络，即首先实现汽车智能网联化，再利用各种传感技术，感知车辆状态信息，并借助无线通信网络与大数据分析技术实现交通的智能化管理，车联网的推进需要车端、路端协同进行。\n",
      "\n",
      "我国车联网起步于2009年，经历了起步阶段（支持远程通话）、手机互联网阶段（与汽车共享手机应用）、汽车IVI阶段（车载娱乐，围绕中控屏展开）、5G+V2X阶段。目前5G车联网由V2V、V2P、V2I、V2N四类通信组成，可以服务于车联网V2V/V2I的安全、V2I/V2P的提升效率和V2I的服务三大领域。这些应用归纳起来，即满足车速小于130公里/小时，通信距离大于300米，系统延时小于100毫秒，数据更新频率1Hz，定位精度小于1.5米，路边单元可以覆盖320米的半径，密度200-400米的场景中。\n",
      "\n",
      "随着5G技术与汽车工业的结合，汽车生产商如比亚迪、长安汽车、广汽集团、上汽集团等积极布局智能网联汽车的生产。而运营商、设备商（如中国联通、百度、中兴等）则通过与自动驾驶垂直领域合作伙伴（如清华、大唐、福特、一汽等）的联合创新来构建协同化汽车驾驶生态系统。\n",
      "\n",
      "在具体落地方面，目前各省市都在积极加码布局车联网，比如北京房山区政府与中国移动在北京高端制造业基地打造国内第一个5G自动驾驶示范区，建成中国第一条5G自动驾驶车辆开放测试道路，可提供5G智能化汽车试验场环境。而厦门市交通运输局、公交集团与大唐移动通信设备有限公司签署协议，在厦门BRT上建设全国首个商用级5G智能网联驾驶平台，推动厦门BRT最终实现无人驾驶。\n",
      "\n",
      "根据Machina、IMS 和华为联合调研数据显示，全球车联网连接数预计到2020 年将增至3 亿左右，到2025 年则将突破10 亿，车联网系统在汽车中的应用将不断普及。值得一提的是，不久前车联网发展才获政策力挺。11月3日，交通运输部发布了关于《道路运输条例（修订草案征求意见稿）》公开征求意见的通知。《条例》明确指出，要积极推进大数据、信息技术、自动驾驶等技术在道路运输领域的发展和应用。根据中国发改委发布的《智能汽车创新发展战略规划》中提出：到2025年，中国能实现有条件智能驾驶汽车的规模化生产，LTE-V2X实现区域覆盖，5G-V2X在部分城市与高速公路覆盖。根据国家规划，2020年C-V2X前装伊始，2025年渗透率力争达到50%。要积极推进大数据、信息技术、自动驾驶等技术在道路运输领域的发展和应用。\n",
      "\n",
      "3、5G+智慧医疗\n",
      "\n",
      "在医疗应用中，5G因其特有的高速率、广连接、低时延等优势，能支持院内、院外一系列医疗信息化需求并带来医疗服务变革。包括基于医疗设备数据无线采集的医疗监测与护理类应用、基于视频与图像交互的医疗诊断与指导类的应用以及基于视频远程操控类应用三大类。由于5G的大连接，未来所有设备都可以进行相互连接。同时，院间医疗业务的协同也可快速开展。\n",
      "\n",
      "尤其是远程医疗方面，目前在全国多地均实现成功案例。远程医疗是通过5G和物联网技术可承载医疗设备和移动用户的全连接网络，对无线监护、移动护理和患者实时位置等数据进行采集与监测，并在医院内业务服务器上进行分析处理，提升医护效率。借助5G、人工智能、云计算技术，医生可以通过基于视频与图像的医疗诊断系统，为患者提供远程实时会诊、应急救援指导等服务，例如基于AI和触觉反馈的远程超声理论上需要30Mbps的数据速率和10ms的最大延时。患者可通过便携式5G医疗终端与云端医疗服务器与远程医疗专家进行沟通，随时随地享受医疗服务。\n",
      "\n",
      "目前，5G+智慧医疗正在如火如荼地推进，比如中国移动、华为协助海南总医院通过操控接入5G网络的远程机械臂成功完成了位于北京的患者的远程人体手术，实现了全国首例5G网络下实施的远程手术；上海市第一医院正在打造5G智慧医疗联合创新中心，将涵盖远程查房、区域医学影像中心远程会诊、远程手术教学、远程操作机械臂诊疗等服务；北京移动携手华为完成了中日友好医院5G室内数字化系统部署，为移动查房、移动护理、移动检测、移动会诊等应用提供了5G网络环境等。\n",
      "\n",
      "尤其是2020年是医疗领域面对巨大压力的一年，在抗击新冠肺炎这场战役中，5G+智慧医疗及时发挥了重要的作用，包括社区管理、人群防控、疫情预警、远程诊疗等多个方面。比如利用5G视频查房系统，通过在隔离病房和ICU配备全景摄像头，专家就能在监控中心用VR眼镜查看病房内病人情况并指导隔离病房内医护人员对患者进行处置。此外还有采用5G+红外、雷达等技术自主规划路径、自主乘梯，根据指令，自动进入隔离病区消毒、送药、清洁的智能机器人等。\n",
      "\n",
      "随着2020年5G时代全面到来，智慧医疗解决方案将以5G技术为依托，构建AI辅助诊疗应用，有效解决我国供给严重不平衡，误诊、漏诊率高耗时较长等诸多问题。\n",
      "\n",
      "4、5G+其他新基建领域\n",
      "\n",
      "5G商用一年来，应用创新不断拓展。从5G当前的产业创新实践来看，无论是远程操控类、图像识别类，还是无人机器类等场景应用，真正能够促进行业的数字化转型与智能升级，仅靠单纯的5G连接能力是远远不够的，必须要与行业信息化系统结合，才能真正产生价值。\n",
      "\n",
      "在中央对新基建定义的5G基站建设、特高压、城际高速铁路和城际轨道交通、新能源汽车充电桩、大数据中心、人工智能、工业互联网七大产业板块中，其中5G排在第一位，这是因为5G每投入1个单位将带动6个单位的经济产出，这成为支撑经济社会数字化、网络化包括智能化转型的关键。5G为数字经济发展提供了全新的关键性支持，预计2020—2025年，5G将拉动中国数字经济增长15.2万亿元。\n",
      "\n",
      "正因如此，工业和信息化部信息通信发展司司长闻库表示：“应以5G等应用创新为着力点，深度挖掘垂直行业需求，鼓励信息通信业与工业、交通、医疗、能源、教育等各个行业更大范围、更深层次的协作创新，不断丰富应用场景，构建广泛应用生态。”中科院院士尹浩则指出：“5G发展不应是一项技术的单打独斗，而是要与云计算、大数据、人工智能、区块链、边缘计算等其他信息通信技术一起，促进传统产业升级，孵化新应用，催生新业态。”\n",
      "\n",
      "自2020年新基建概念提出以来，山东、四川、广西、上海等多地召开了新基建动员大会，部署相关工作。不少地方均以2020―2022年为一个周期，力争通过三年时间让当地的新基建投资取得较大进展。梳理各地发布的新基建三年行动规划发现，5G成为各地发展新基建的“标配”。例如，广州市提出，到2022年，广州要“成为辐射粤港澳大湾区的全国首批5G商用试点城市和综合型信息消费示范城市，累计建成5G基站8万座，总投资超过300亿元，培育200家5G应用领域创新型企业”；11月5日，“广西新型基础设施建设动员大会暨首批重大项目开竣工仪式”在南宁举行。会议称，截至2020年10月31日，该区已累计完成建设5G基站24256个，超额完成2万座基站的年度计划任务。\n",
      "\n",
      "5G“新基建”升级，5G真正的价值也更能显现。中国信息通信研究院发布的《5G经济社会影响白皮书》预测，自2020年起，预计5G带动直接经济产出4840亿元，间接经济产出达1.2万亿元。就业机会方面，预计在2020年、2025年和2030年，5G商用将分别直接贡献50万、350万和800万个就业机会。\n",
      "\n",
      "三、5G新基建当前面临的挑战\n",
      "\n",
      "尽管5G商用开通一年，但目前5G网络还不是一个完善网路，商业模式还不清晰，在发展过程中仍然面临诸多挑战：\n",
      "\n",
      "第一、目前5G主要呈现的亮点是宽带移动接入，但仍需在今后接受大流量、大连接、高可靠、低时延的考验。\n",
      "\n",
      "第二、开展5G SA建设，将面临需求多、期望高、时间紧的挑战，产业各方还需进一步完善。同时，建设SDN、NFV、网络切片、MEC、SD-WAN等技术还有待验证，面临5G SA探路风险。\n",
      "\n",
      "第三、5G切片和MEC技术融合创新是未来5G在垂直行业应用的关键要素。端到端网络切片的标准、产品与现有网络兼容问题，以及在保证时延的前提下，MEC间的协调，MEC与中心云之间的功能合理分配问题待解决。\n",
      "\n",
      "第四、5G基站功耗下降难度大。低功耗、低成本的5G终端是大规模商用的瓶颈。业界寄希望于国产多模多屏支持SA的芯片大规模量产，但目前市场上5G基带芯片以7纳米工艺为主，国外已开始发布下一代更高工艺水平的芯片，我国自研的新一代5G终端芯片的供应链有受制于人的风险，芯片的持续创新压力很大。目前5G基站功耗仍是4G的三倍，功耗进一步下降的难度很大。室内覆盖占移动网络建设投资较高比例，5G工作频率高，电磁波穿墙能力差，覆盖成本较4G更高。深度覆盖需要比较分布式基站与室分系统，新一代的室分系统需要兼顾有源与无源，扩展物联网与定位功能，增加可视化运维等智能化能力。同时，5G 的终端测试仪表与网优仪表等仍是5G产业链的薄弱环节。\n",
      "\n",
      "第五、行业的刚需与跨界合作及商业模式还不清晰，行业主导的积极性还有待发挥。当前，公众对5G的认识是带宽更宽、速度更快，这不足以迅速地扩大用户群，用户需要有更高价值的体验。产业仍需努力克服和解决5G产品形态和消费模式、网络建设方式、网络运维调度方式等问题。\n",
      "\n",
      "除了以上五点，5G安全也是不容忽视的问题。新技术、新场景、新生态的出现；虚拟化、开放性、切片化、开源化等技术的引入；大量数据上网、上云等等这些因素，引起新的5G网络自身安全风险及5G赋能各行业的安全挑战。\n",
      "\n",
      "为了应对这些挑战，今年7月23日国务院举行新闻发布会，公布了今年上半年工业和通信业发展情况，并表示将采取以下措施进一步发挥5G对经济高质量发展的支撑作用：\n",
      "\n",
      "一是促进网络建设质效并重。推动基础电信企业还要加大力度、抢抓工期，以支持独立组网（SA）模式为目标，加快构建覆盖全国、技术先进、品质优良的高质量的5G网络，持续支持中国电信、中国联通通过共建共享，率先建成一个200兆大带宽的非常有竞争力的5G网络。\n",
      "\n",
      "二是注重市场活力充分释放。抓住5G在网络教育、在线医疗、远程办公等方面的发展机遇，进一步丰富5G的应用场景，促进品类丰富、高性价比的5G终端进入市场。\n",
      "\n",
      "三是实现5G和其他行业的同频共振。要充分调动各行各业应用5G的积极性，促进通信与相关行业双向开放与合作，研究梳理5G应用在推广当中到底有什么样的制约因素，这些制约因素到底对网络有什么样的需求，要进一步明确这些应用的需求，明确技术方案，探索新的商业模式，切实打通5G和其他行业的应用通道。\n",
      "\n",
      "四是发挥地方支持政策的助推作用。充分释放地方各级政府的政策红利，鼓励各级政府因地施策，充分用好5G的供电、建设、应用、资金等方面政策的“工具箱”，结合各地的产业优势和数字化转型的需求，将5G发展迅速转化为支持经济高质量发展的关键动力。\n",
      "\n",
      "五是要强化国际合作和互助共赢。继续加大国际的合作力度，广泛深入开展5G技术、产业、应用等方面的合作。持续推进5G应用的创新，促进形成新的增长点，共同应对新冠疫情带来的经济增长下行压力。\n",
      "\n",
      "同时，为了保护普通消费者权益，一系列规章制度如《电信服务规范》、《关于规范电信服务协议有关问题的通知》、《关于进一步加强电信服务用户消费提醒工作的通知》等规定，都要求了电信企业给用户开业务时保障用户的知情权。并表示各地方通信管理局将密切关注各省各地情况，加强监督检查，特别是依法查处未经用户同意升级用户套餐、捆绑销售这种侵害用户合法权益的行为。\n",
      "\n",
      "其实，无论是对大数据、云计算还是近年来的人工智能，我们可以看到从国家政策到行业对于新技术、新应用所表现出来的态度比以往任何时候都开放和包容，对于5G也是如此。目前，大家对5G争论不休，要去证明它能给人类带来好处。但事实上，这一切都需要时间，就如工信部前部长苗圩所言，“通信基础设施建设上要采取适度先行的策略，就好比让路等车还是车等路的问题，应当适度超前一点，让路等车，不要等大家都堵到开不动车了，再想到修路的问题。” \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-18-3\n",
      "title= 人工智能先锋人物杰夫·辛顿说：“深度学习将无所不能”\n",
      "author= []\n",
      "publish_date= 2020-11-18 00:00:00\n",
      "text= 三十年前，辛顿对神经网络的观点与众不同，他说现在大家都接受了这些观点。\n",
      "\n",
      "人工智能 领域的缺口：“必须有更多的概念上的突破，在规模上，还需要加大。\"\n",
      "\n",
      "领域的缺口：“必须有更多的概念上的突破，在规模上，还需要加大。\" 神经网络 的弱点：“ 神经网络 在处理多 参数 少量数据时，表现不错，但是这方面，人类似乎做得更好。\"\n",
      "\n",
      "的弱点：“ 在处理多 少量数据时，表现不错，但是这方面，人类似乎做得更好。\" 人脑如何工作：“大脑内部是参与神经活动的大向量。\"\n",
      "\n",
      "现代 人工智能 革命始于一场默默无闻的研究竞赛：2012年，即第三届图像网络竞赛（ImageNetcompetition），挑战团队需要建立一个能识别1000个物体的 计算机视觉 系统，这1000个物体中包括动物、景观和人类。\n",
      "\n",
      "在前两年，即便是最好的参赛团队， 准确率 都不超过 75%。但是到了第三年，三位研究人员（一位教授和他的两个学生）突然打破了这个天花板，他们惊人地超出了10.8个百分点，赢得了比赛。那个教授便是杰弗里·辛顿，他们使用的技术叫做 深度学习 。\n",
      "\n",
      "自20世纪80年代以来，辛顿一直致力于 深度学习 的研究工作，由于缺乏数据和计算能力，其有效性受到了限制，一直到2012年才取得成果。辛顿对这项技术的坚定信念最终带来了巨大的回报：在第四年的图像网比赛（ImageNet competition）中，几乎所有参赛队都在使用 深度学习 ，并获得了神奇的准确性。很快， 深度学习 便被应用于图像识别之外的任务。\n",
      "\n",
      "去年，由于他在这一领域的特殊贡献，辛顿与 人工智能 的先驱们YannLeCun和Yoshua Bengio一起被授予图灵奖。10月20日，我在麻省理工学院技术评论的年会“Em Tech MIT会议”上与他谈到了这个领域的现状，以及下一步的方向。\n",
      "\n",
      "为了表达清楚，对以下内容进行了编辑和浓缩。\n",
      "\n",
      "你认为 深度学习 足以复制人类所有的智力，为什么这么确定？\n",
      "\n",
      "我深信 深度学习 将无所不能，同时，我认为必须有相当多的概念上的突破。例如，2017年AshishVaswani等人，引入“transformer”“transformers”这个概念，它利用向量来表示词义，这是一个概念性的突破，目前几乎用于所有的 自然语言处理 模型。我们需要更多类似的突破。\n",
      "\n",
      "如果有了这些突破，是否能够通过 深度学习 来模拟所有人类智力？\n",
      "\n",
      "的确如此，特别是如何获得神经活动的大向量来实现“推理”这样的突破。但同时我们需要大幅度增加规模。人脑大约有100万亿个 参数 ，即突触，是真正的巨大模型，像GPT-3（https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/），有1750亿个 参数 ，但它比大脑小一千倍。GPT-3现在可以生成看似合理的文本，但与大脑相比，它依然太小。\n",
      "\n",
      "提到规模时，是指更大的 神经网络 ，更多的数据，还是两者兼而有之？\n",
      "\n",
      "两者兼而有之，计算机科学中发生的事情和人们实际发生的事情之间存在某种差异。与获得的数据量相比，人们拥有更多的 参数 。 神经网络 擅长处理有大量 参数 的少量数据，但人类在这方面却做得更好。\n",
      "\n",
      "很多业内人士认为，下一个大的挑战是常识，你同意吗？\n",
      "\n",
      "我同意，这是一件非常重要的事情，我认为运动控制也非常重要，而深层 神经网络 现在正变得越来越擅长这一点。特别是，谷歌最近的一些工作表明，可以完成精细的运动控制并与语言相结合，比如：打开一个抽屉，取出一个小木块，系统便可以用自然语言告诉你它在做什么。\n",
      "\n",
      "像GPT-3这样的模型，它可以生成精彩的文本，很明显，它必须深入理解才能生成该文本，虽然我们对于它理解的程度还不太清楚。但是，如果有什么东西打开抽屉，拿出一个小木块并说：“我刚刚打开一个抽屉，拿出小木块一个”，也可以说它明白自己在做什么。\n",
      "\n",
      "人工智能 领域一直把人脑作为其最大的灵感来源，不同的 人工智能 方法源于认知科学中的不同理论。你是否相信大脑实际上建立起了外部世界的表征之后，再来理解它，或者这只是一种有用的思考方式？\n",
      "\n",
      "很久以前，在认知科学中，两个学派之间存在着一场争论：其中一个是由斯蒂芬·科斯林(Stephen Kosslyn)领导的，他认为，当大脑处理视觉图像时，你拥有的是一组正在移动的像素；另一学派则更符合传统的 人工智能 ，“不，不，这是胡说八道，它是分层、结构性的描述。脑内处理的是一个符号结构。”\n",
      "\n",
      "我认为他们都犯了同样的错误。科斯林认为我们处理的是像素，因为外部图像是由像素组成的，这是能为我们理解的一种表示；有人认为大脑处理的是符号，是因为我们也在用符号表示事物，这也是我们能理解的一种表示。我认为二者都不对，实际上大脑内部是多个神经活动的大向量。\n",
      "\n",
      "仍然有许多人认为符号表示是 人工智能 的方法之一。\n",
      "\n",
      "当然。我有像赫克托·莱维斯克（Hector Levesque）这样的好朋友，他们相信符号表示的方法，并在这方面做了很棒的工作。我不同意他的观点，但符号表示方法是一件完全合理的事情。我猜测，符号只是存在于外部世界中，在大脑内部，用大向量进行内部操作。\n",
      "\n",
      "你认为你对 人工智能 未来最与众不同的观点是什么？\n",
      "\n",
      "好吧，我早先持有的与众不同的观点，五年后，它们却成为了主流。早在20世纪80年代，我的大多数反向观点现在都被广泛接受了，现在大多数人都同意并接受了这些观点。所以，在某种程度上可以说，我的逆向观点已被削弱了。\n",
      "\n",
      "原文标题：\n",
      "\n",
      "AI pioneer Geoff Hinton: “Deep learning is going to beable to do everything”\n",
      "\n",
      "原文链接：\n",
      "\n",
      "https://www.technologyreview.com/2020/11/03/1011616/ai-godfather-geoffrey-hinton-deep-learning-will-do-everything/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-17-2\n",
      "title= 清华大学施路平：双脑驱动的人工通用智能\n",
      "author= []\n",
      "publish_date= 2020-11-17 00:00:00\n",
      "text= 本文从类脑计算研究的原因、内容和方法三方面分析了类脑计算、芯片及系统研究所面临的挑战和可能的解决方法。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11月1日上午，在第十九届中国计算 语言学 大会(CCL2020)上， 清华大学 类脑计算研究中心主任施路平作了题为《面向人工通用智能的类脑计算》的主题报告，从类脑计算研究的原因、内容和方法三方面分析了类脑计算、芯片及系统研究所面临的挑战和可能的解决方法，重点讨论了如何将脑科学和计算机融合，以双脑驱动的类脑计算推动人工通用智能的研究。观看CCL回放，可登录B站关注“智源研究院”。\n",
      "\n",
      "施路平教授是 清华大学 类脑计算研究中心主任，光盘国家工程研究中心主任，以及国际光学工程学会（SPIE）会士。他在2013年加入 清华大学 创建类脑计算研究中心，提出异构融合类脑计算架构，研制了全球首款异构融合类脑计算“天机芯”，构建人工通用智能研究演示平台-自动行驶自行车，相关结果作为《Nature》封面文章被发表，被评为2019年中国十大科技进展。 为什么做类脑计算\n",
      "\n",
      "目前，全球数据量大约每两年翻一番（另外一种 摩尔定律 ），而基于冯诺依曼架构的计算机在大数据信息处理时其能耗、速度和带宽均受到影响；另外，硬件的物理微缩，非结构化数据的处理、数据平均寿命短、数据多样性、关联性，数据存储等都对计算机发展带来挑战。\n",
      "\n",
      "2017年，图灵奖得主 David A. Patterson 和 John L. Hennessy 在 ACM complication 上发表长文称，未来的计算机体系架构将迎来黄金发展十年。改变计算机体系架构，发展新的计算机架构势在必然。除此之外， 人工智能 的三次浪潮—— 神经网络 ，第五代 神经网络 计算机以及 深度学习 ，都与“脑”紧密相关，类脑计算源于 人工智能 技术的发展需求。\n",
      "\n",
      "目前的 人工智能 虽得到快速发展，但仍面临着诸多问题，比如，在语音输入时，机器无法判断间隔，无法识别口误和口音，这是因为机器并没有真正理解语言。施路平表示，理解不是一个单独、客观的过程，它与主体有关，与人类自身的知识结构和经历有关。因此，需要突破单一结构的智能研究。\n",
      "\n",
      "人工智能 的发展需要满足以下五个条件：1） 充足的数据，2）确定性的问题，3）完备的知识，4）静态，5）单一的系统。否则，发展人工通用智能将成为 人工智能 突破的最终解决方案。\n",
      "\n",
      "研究了图灵和冯诺依曼等人的早期著作和文章后，施路平发现，图灵等人提出的 人工智能 的愿景，都是发展通用智能。他认为，目前的 人工智能 具有非常好的发展契机，其原因包括：\n",
      "\n",
      "1. 随着先进精密仪器的发展，人类对脑的理解越来越多，我们似乎到了一个理解脑的关口。\n",
      "\n",
      "2. 超级计算机的发展为我们提供了更好的仿真模拟环境。\n",
      "\n",
      "3. 大数据和 云计算 提供了一个和脑复杂度近似的世界，两者相互促进，共同发展。\n",
      "\n",
      "4. 新型纳米器件可以制造出和人脑 神经元 能耗差不多级别的器件。\n",
      "\n",
      "与AI技术相比，AGI可以处理不确定性问题，小数据、脏数据和缺失数据，可应用于多维系统，无足够应对的知识以及动态系统。从根本上来讲，AI技术更强调发展的能力，而AGI则更加关注如何把智能有机地融合起来，使一种能力的提升能够帮助提升其他能力。\n",
      "\n",
      "图1：AI和AGI的比较\n",
      "\n",
      "图灵奖得主Geoffrey Hinton认为，克服目前 人工智能 发展局限的关键是，搭建“一个连接计算机科学和生物学的桥梁”，该思想与施路平团队所提出的“双脑融合”的思想一致。\n",
      "\n",
      "电脑在计算能力、存储速度、寿命等很多方面早已超过人类，而人脑具有 感知 、自适应、创新、认知等能力，计算机和大脑的基本原理正好相反，在原理上、功能上和形式上可以形成一个优势互补的系统，类脑计算是未来计算机的发展重要的领域。施路平认为，脑科学的研究将会为许多科研工作者带来新的科研启发，“是一个非常重要的金矿”。\n",
      "\n",
      "图2：脑科学的战略意义\n",
      "\n",
      "类脑计算主要做什么\n",
      "\n",
      "类脑计算的研究涵盖算法、硬件、芯片和系统等不同层面，是美、英、德等国的重点研究领域。从计算机和互联网发展来看，类脑计算的发展需要芯片、软件工具链、 操作系统 和应用的协同发展。施路平认为，芯片中的信息如何来承载、存储、计算和利用是 类脑芯片 的关键，而软件的核心技术问题是软件环境中信息流如何分配、交流、 调度 和控制。\n",
      "\n",
      "类脑计算可以表示为两种：Brain-Inspired Computing 和 Brain-Like Computing，前一类是从计算机出发，尽可能借鉴脑科学的基本原理来改变计算机，而另一类是尽可能做到像脑，包括其功能和结构上类脑。从诺贝尔奖和图灵奖获奖者的研究来看，大脑和计算机的研究是分别发展的，而目前的研究需要将两者融合起来发展，形成“双脑驱动”的发展模式。\n",
      "\n",
      "图3：诺贝尔奖和图灵奖获奖者的研究\n",
      "\n",
      "2016年，全球出现了三款类脑计算机，包括美国的TrueNorth、德国的BrainScales，以及英国的SpiNNaker。今年2月， 清华大学 开发设计了中国第一台类脑计算机——天机电子计算机，该研究于 人工智能 杂志发表。8月，浙江大学发布了一款亿级 神经元 类脑计算系统。类脑计算的研究目前没有公认的确定方案，是 IBM 、 惠普 、 英特尔 等的重点研究领域。\n",
      "\n",
      "图4：近年来类脑计算的研究进展\n",
      "\n",
      "图5：主要的类脑计算芯片\n",
      "\n",
      "在最近发表于《自然机器智能》杂志上的一篇论文中，来自麻省理工学(MIT)、维也纳技术大学和奥地利科技学院的研究团队设计了一种模仿生物模型的AI系统，该系统基于类似蛲虫等小动物大脑而开发的。\n",
      "\n",
      "研究团队为 神经元 和突触开发了新的数学模型，并结合受大脑启发的神经计算原理和可扩展的 深度学习 架构，为全栈自动驾驶车辆控制系统针对特定任务的分隔间制造了紧凑的神经控制器。研究表明，仅使用少量的人工 神经元 就能控制车辆。\n",
      "\n",
      "施路平表示，在脑科学的发展中，类脑的精髓是提供一个“方向感”，指导人类的探索方向。\n",
      "\n",
      "怎么做类脑计算\n",
      "\n",
      "类脑计算的研究面临着科学挑战、技术挑战等，而其中最难也是最重要的一个挑战是：多学科深度融合。为促进多学科融合发展， 清华大学 成立了由七个院系组成 清华大学 内脑计算研究中心，其研究主要分为三个层次：基础科学、核心技术和应用。\n",
      "\n",
      "图6： 清华大学 类脑研究架构\n",
      "\n",
      "清华大学 类脑研究主要采用“大脑”和“电脑”双脑驱动的发展战略，以计算机为主体，融合脑原理，从理论、芯片、软件、系统、应用五个方面协同发展，发展方向从一个问题一个解决方案发展到一类问题一个解决方案，并逐渐发展到多类问题一个解决方案，同时和应用紧密结合。\n",
      "\n",
      "而类脑计算面临的首要问题是，如何在不理解人脑机制的情况下发展类脑计算系统？\n",
      "\n",
      "施路平认为，即使我们不知道大脑的基本原理，但我们知道每个 神经元 对外连接数目超过一千，换句话说，是利用空间复杂度，另外，对时间编码可引入时空复杂度。基于此， 清华大学 提出通用类脑计算框架，通过增加 类脑芯片 ，以实现计算机架构处理结构化信息， 类脑芯片 处理非结构化信息的任务。这样，即使不了解大脑结构，也可以创造出一个新的计算架架构。\n",
      "\n",
      "图7： 清华大学 通用类脑计算架构\n",
      "\n",
      "新的计算机架构考虑了时空复杂性。计算机驱动的ANN技术能够很好地反映空间复杂度，像脑一样工作的SNN更多反映了时空复杂度，将脑科学驱动和计算机方法结合起来，是ANN和SNN融合的范例。\n",
      "\n",
      "图8：通用类脑计算的 神经网络 模型\n",
      "\n",
      "基于以上研究， 清华大学 设计构造了全球首款异构融合天机 类脑芯片 ，用3%的代价实现了超过TrueNorth的各项特性，包括密度提高了20%，速度提高了10倍，带宽扩展了100倍，精度可调，扩展性和灵活性更好等。该研究于去年8月在《Nature》上作为封面被发表。\n",
      "\n",
      "图灵架构是整个计算机的基石， 清华大学 研究团队提出的类脑计算完备性，用通用近似的思想取代了完备计算的概念，约束的放宽更有利于发展新的计算架构。刚刚过去的10月， 清华大学 研究团队在《Nature》上发表的文章，重点介绍了类脑计算的完备性、系统的层次性和一个软件工具链。\n",
      "\n",
      "图9：类脑计算完备性\n",
      "\n",
      "实际上，这是一个人工通用智能研究平台，研究团队希望用这样的一个系统和环境进行交互，当环境变化的时候，观察系统的变化以及所遵循的基本原理，提出一个环境交互和迭代进化的发展思路，通过利用该系统，构建了一个全网络可扩展的AGI演示平台。\n",
      "\n",
      "图10：异构全网络可扩展AGI研究演示平台\n",
      "\n",
      "今年2月， 清华大学 研究团队发布了国内首台类脑计算机的样机。该样机是一个异构融合的架构，通过在CPU和GPU旁增加内脑计算芯片，由主机处理结构化信息，类脑计算芯片承担异构融合系统，同时来支撑ANN和SNN，以及它们之间异构的建模。\n",
      "\n",
      "图11：国内首台类脑计算机的样机\n",
      "\n",
      "目前， 清华大学 正在研究基于类脑计算的云脑。基于现有服务器搭建云脑，该云脑将具有独立 数据库 、 知识图谱 和软件工具链，在解决五类基本问题后逐步发展。\n",
      "\n",
      "图12：类脑计算云脑 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-12-2\n",
      "title= 层级聚类和Python实现的初学者指南（附链接）\n",
      "author= []\n",
      "publish_date= 2020-11-12 00:00:00\n",
      "text= 本文从对比无监督学习和监督学习的特征切入，结合具体的案例来给大家介绍层级聚类的概念、应用场景、主要类型以及Python实现。\n",
      "\n",
      "引言\n",
      "\n",
      "理解顾客行为在任何工业领域都是至关重要的，直到去年我才意识到这个问题。当时我的CMO（chief marketing officer,首席营销官）问我：“你能告诉我，我们新产品的目标用户应该是什么群体呢？”\n",
      "\n",
      "这对我来说是一个学习的过程。我很快意识到，作为一个 数据科学 家，将顾客细分以便于公司能够进行客户定制并建立目标策略有多重要。这就 聚类 概念能派上用场的地方！\n",
      "\n",
      "用户分类通常很棘手，因为我们脑海当中并没有任何目标变量。我们现在正式踏入了无 监督学习 的领域，在没有任何设定结果的情况下来发掘模式和结构。这对 数据科学 家来说是充满挑战但却是让人激动的事。\n",
      "\n",
      "在这里有几种不同的 聚类 方法（你会在下面的部分看到）。我将向你介绍其中一种——层级 聚类 。\n",
      "\n",
      "我们将会学习层级 聚类 是什么，它优于其他 聚类 算法的地方，不同层级 聚类 的方式以及开展的步骤。我们在最后会采用一个顾客 分类数据 库并实现Python的层级 聚类 。我喜欢这个方法并且十分确定在你读完本文之后也会喜欢上的！\n",
      "\n",
      "注释：如上所述， 聚类 的方法很多。我鼓励你查看我们对不同类型 聚类 所做的指南：\n",
      "\n",
      "An Introduction to Clustering and different methods of clustering\n",
      "\n",
      "https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/utm_source=blog&utm_medium=beginners-guide-hierarchical-clustering\n",
      "\n",
      "想要学习更多关于 聚类 的内容和其他 机器学习 算法（监督和无监督）可以看看下面这个项目-\n",
      "\n",
      "https://courses.analyticsvidhya.com/bundles/certified-ai-ml-blackbelt-plus?utm_source=blog&utm_medium=beginners-guide-hierarchical-clustering\n",
      "\n",
      "目录\n",
      "\n",
      "1. 监督vs 无 监督学习\n",
      "\n",
      "2. 为什么要用层级 聚类 ？\n",
      "\n",
      "3. 什么是层级 聚类 ？\n",
      "\n",
      "4. 层级 聚类 的类型\n",
      "\n",
      "（1） 聚合式（Agglomerative）层级 聚类\n",
      "\n",
      "（2） 分裂式（Divisive）层级 聚类\n",
      "\n",
      "5. 层级 聚类 的步骤\n",
      "\n",
      "6. 在层级 聚类 中如何选择类的数量？\n",
      "\n",
      "7. 利用层级 聚类 解决一个批发顾客 分类问题\n",
      "\n",
      "监督vs无 监督学习\n",
      "\n",
      "在我们深入学习层级 聚类 之前，理解 监督学习 和无 监督学习 之间的差异是十分重要的。让我用一个简单的例子来解释这种差异。\n",
      "\n",
      "假设我想要估计每天将被租借的自行车数量：\n",
      "\n",
      "或者，我们想预测在泰坦尼克号上一个人是否生还：\n",
      "\n",
      "在这两个例子当中都有一个固定的目标要实现：\n",
      "\n",
      "在第一个例子当中，要基于像季节、假期、工作日、天气、温度等特征来预测自行车租用数量。\n",
      "\n",
      "在第二个例子中要预测乘客是否会生还。在“生还”变量中，0代表这个人未生还，1代表这个人活了下来。这里的自变量包括客舱等级、性别、年龄、票价等等。\n",
      "\n",
      "所以说，当我们有目标变量的时候（在上述两个例子当中的数量和生还），基于一系列预测变量或者自变量（季节，假期，性别，年龄等）来预测，这种问题叫做 监督学习 问题。\n",
      "\n",
      "让我们看看下面的图以便更好地理解它：\n",
      "\n",
      "在这里，y是因变量或者叫目标变量，X代表自变量。目标变量依赖于X，因此它也被叫做一个因变量。我们在目标变量的监督下使用自变量来训练模型，因而叫做 监督学习 。\n",
      "\n",
      "我们在训练模型时的目标是生成一个函数，能够将自变量 映射 到期望目标。一旦模型训练完成，我们可以把新的观测值放进去，模型就可以自己来预测目标。总而言之，这个过程就叫做 监督学习 。\n",
      "\n",
      "有时候我们并没有任何需要预测的目标变量。这种问题没有任何外显的目标变量，被叫做无 监督学习 。我们仅有自变量。\n",
      "\n",
      "我们试图将全部数据划分成一系列的组。这些组被叫做簇，这个过程叫做 聚类 。\n",
      "\n",
      "这种技术通常被用于将总体 聚类 成不同的组别。常见的例子包括顾客分群、 聚类 相似的文件、推荐相似的歌或者电影等等。\n",
      "\n",
      "现在有很多算法可以帮助我们完成 聚类 。最常用的 聚类 算法是K-means和层级 聚类 。\n",
      "\n",
      "为什么要采用层级 聚类 ？\n",
      "\n",
      "在此之前，我们需要先知道K-means是怎样工作的。相信我，这会让层级 聚类 的概念变得更简单。\n",
      "\n",
      "这里有一个对K-means算法如何工作的概览：\n",
      "\n",
      "1． 决定簇的数量（k）\n",
      "\n",
      "2． 选择k个随机的点作为中心点\n",
      "\n",
      "3． 将所有的点纳入最近的中心点\n",
      "\n",
      "4． 计算新形成的簇的中心点\n",
      "\n",
      "5． 重复步骤3和4\n",
      "\n",
      "这是一个迭代的过程。它将持续地运行，直到新形成的簇的中心点不再变化，或者到达了最大迭代次数。\n",
      "\n",
      "但是K-means也受到了一些质疑。它通常试 图生成 规格相同的簇。还有，我们需要在算法开始之前就决定好簇的数量。理想情况下，我们在算法开始时不知道要多少簇，因而这也是K-means所面对的一种质疑。\n",
      "\n",
      "这也恰恰就是层级 聚类 的优越之处。它解决了预先设定簇的数量的问题。听起来就是在做梦！所以，让我们看看层级 聚类 是什么以及它是怎样改进K-means的。\n",
      "\n",
      "什么是层级 聚类 ？\n",
      "\n",
      "我们有以下的一些点，我们想把它们 聚类 ：\n",
      "\n",
      "我们可以把每个点作为单独的簇：\n",
      "\n",
      "现在，基于这些簇的相似性，我们可以把最相似的簇放到一起，并且重复这个过程直到剩余单一的簇：\n",
      "\n",
      "我们有必要建立一个簇的层级，这就是为什么这个算法叫做层级 聚类 。我们将在下一部分讨论如何决定簇的数量。现在，让我们看看不同类型的层级 聚类 。\n",
      "\n",
      "层级 聚类 的类型\n",
      "\n",
      "这里有两种主要的层级 聚类 ：\n",
      "\n",
      "1. 聚合式层级 聚类\n",
      "\n",
      "2. 分裂式层级 聚类\n",
      "\n",
      "让我们来详细理解一下每一种：\n",
      "\n",
      "聚合式层级 聚类\n",
      "\n",
      "把每个点归于单独的一个簇。假设这里有四个数据点。我们把每个点分到一个簇里，在开始时就会有四个簇：\n",
      "\n",
      "然后，在每一轮迭代中，我们把最相似的点对进行融合，然后重复上述步骤直到只剩单一簇：\n",
      "\n",
      "每一步我们都在融合（或者增加）簇，对吧？因此，这种 聚类 也叫作累加层级 聚类 。\n",
      "\n",
      "分裂式层级 聚类\n",
      "\n",
      "分裂式层级 聚类 则是一种相反的思路。与一开始划分n个簇（n个观测值）不同，我们开始时只有一个簇，并且把所有的点都纳入这个簇。\n",
      "\n",
      "所以，我们有10个或者1000个点并不重要。所有的点在一开始都在同一个簇中：\n",
      "\n",
      "现在，在每一次迭代中，我们把簇中最远的点分离出来，并且重复上述过程直到每个簇都只有一个点：\n",
      "\n",
      "我们每一步都在分裂（或划分）簇，因此叫做分裂式层级 聚类 。\n",
      "\n",
      "聚合式 聚类 被广泛应用于工业当中，在本文当中也将重点关注。一旦我们掌握了聚合式，分裂式层级 聚类 也将变得非常简单。\n",
      "\n",
      "层级 聚类 的步骤\n",
      "\n",
      "我们在层级 聚类 当中把最相似的点或类进行融合——我们已经知道这一点。现在问题是——如何决定哪些点相似哪些点不相似呢？这才是 聚类 当中最重要的问题之一！\n",
      "\n",
      "这里有一种计算相似性的方式——计算簇中心点之间的距离。距离最近的点被认为是相似的点，我们可以融合它们。我们可以把这个叫做基于距离的算法（因为我们计算了簇之间的距离）。\n",
      "\n",
      "在层级 聚类 中有一个临近矩阵（proximity matrix）的概念。这个矩阵存储了每对点之间的距离。让我们来用一个例子来理解这个矩阵和层级 聚类 的方法。\n",
      "\n",
      "例子\n",
      "\n",
      "假设一个老师想把她的学生分成不同的组。她有每个学生在一次作业当中所取得的分数，基于这些分数，她想把学生分成不同的组。这里没有关于分组的固定的目标。因为老师并不知道哪种学生应该分配到什么组，它不能用 监督学习 问题来描述。所以，我们将使用层级 聚类 把学生分成不同的组。\n",
      "\n",
      "我们的例子有5个学生：\n",
      "\n",
      "创造一个邻接矩阵\n",
      "\n",
      "首先，我们创造一个邻接矩阵，这个矩阵会告诉我们这些点之间的距离。我们计算了每两个点之间的距离，会得到一个n*n的方形矩阵（n是观测值的数量）。\n",
      "\n",
      "让我们来看一看这五个点之间的邻接矩阵：\n",
      "\n",
      "这个矩阵的对角线永远是0，因为每个点到自己的距离总是0。我们将使用欧氏距离公式来计算剩下的距离。所以，让我们来看看我们想计算的点1和2之间的距离：\n",
      "\n",
      "√(10-7)^2 = √9 = 3\n",
      "\n",
      "类似地，我们可以计算所有点之间的距离，并且填充这个邻接矩阵。\n",
      "\n",
      "步骤\n",
      "\n",
      "第一步：首先，我们将所有的点归于一个簇：\n",
      "\n",
      "不同颜色表征不同的簇。你可以看到数据中的5个点构成了五种不同的簇。\n",
      "\n",
      "第二步：接下来，在邻接矩阵中找到距离最短的点，并且把这些点融合。然后更新邻接矩阵。\n",
      "\n",
      "在这里，最小的距离是3，因此把点1和2进行融合：\n",
      "\n",
      "让我们看看更新后的簇并且相应地更新邻接矩阵：\n",
      "\n",
      "在这里，我们取了两个点（7,10）中的最大值来代替这个集群的标记。我们也可以用最小值或者平均值代替。现在，我们将再一次计算这些簇的邻接矩阵：\n",
      "\n",
      "第三步：重复步骤2直到只剩下1个簇。\n",
      "\n",
      "我们先看邻接矩阵当中的最小值，然后融合簇中最接近的一对。我们在重复上述步骤之后将得到以下融合的簇：\n",
      "\n",
      "我们开始有5个簇，最后只有一个单一的簇。这也就是聚合式层级 聚类 的工作方式。但是棘手的问题仍然存在——怎么决定簇的数量呢？让我们看看下一部分。\n",
      "\n",
      "在层级 聚类 中，我们应该怎样选择簇的数量呢？\n",
      "\n",
      "准备好回答这个从开始就一直在提的问题了吗？为了获得层级 聚类 的数量，我们使用了一个叫树状图的绝妙概念。\n",
      "\n",
      "树状图是一个树形图表，能够记录融合或分裂的顺序。\n",
      "\n",
      "让我们回到老师-学生的例子。无论何时融合两个类，一个树状图都会记录这些类之间的距离并且以图的形式进行表征。让我们看看树状图是什么样的：\n",
      "\n",
      "我们把样本放到x轴，距离作为y轴。无论两个簇何时融合,我们都将加入树状图内，连接点之间的高度就是这些点之间的距离。让我们来建立例子的树状图：\n",
      "\n",
      "需要花点儿时间来加工上述图片。我们开始融合了样本1和2，这两个点之间的距离是3（指的是在上一部分出现的第一个邻接矩阵）。让我们来把它放到树状图上：\n",
      "\n",
      "在这里，我们可以看看融合的样本1和2。垂直的线代表两个点之间的距离。相似的，我们把融合簇的所有步骤画到图上，最后可以得到如下树状图：\n",
      "\n",
      "我们可以清晰地把层级 聚类 的步骤进行可视化。垂直线的距离越长，簇之间的距离越远。\n",
      "\n",
      "现在，我们可以设置一个距离阈限，并画一条水平线（一般的，我们会用这种方式来设置阈限，它会切断最常的垂直线）。让我们设置阈限为12，然后画一条水平线。\n",
      "\n",
      "类的数量是与阈值先相交的垂直线的数量。在上述例子里，因为红线与两条垂直线交叉，我们将有2个类。一个类包括样本（1,2,4），另一个类包括样本（3,5）。非常清晰对吗?\n",
      "\n",
      "这就是我们在层级 聚类 中使用树状图确定类的数量的方式。在下一部分，我们将实际应用层级 聚类 帮助你理解本文中所学到的概念。\n",
      "\n",
      "使用层级 聚类 来解决批发顾客 分类问题\n",
      "\n",
      "是时间开始用Python了！\n",
      "\n",
      "我们将开始解决一个批发顾客 分类问题 。你可以在这里下载数据集（https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv）。\n",
      "\n",
      "这个数据托管在UCI 机器学习 知识库 当中。本问题的目标是对一个批发商的顾客基于他们在不同产品类型（例如牛奶、食品杂货、地区等等）的年度开支进行分类。\n",
      "\n",
      "让我们先来探索一下数据，然后再利用层级 聚类 进行顾客分类。\n",
      "\n",
      "首先导入所需的函数库：\n",
      "\n",
      "view rawimporting_libraries.py hosted with by GitHub\n",
      "\n",
      "https://mp.weixin.qq.com/cgi-bin/appmsg?t=media/appmsg_edit&action=edit&type=10&appmsgid=100034460&isMul=1&isSend=0&token=886063492&lang=zh_CN#file-importing_libraries-py\n",
      "\n",
      "加载数据集然后看一下前几行：\n",
      "\n",
      "view raw\n",
      "\n",
      "https://gist.github.com/PulkitS01/8ac9bf3b54eb59b4e1d4eaa21d3d774e/raw/6cea281dc4dea42bbcb2160e6cef1535cad765e7/reading_data.py\n",
      "\n",
      "这里有很多产品种类——生鲜、牛奶、杂货等等。数值代表被每个顾客所购买的数量。我们的目标是从这个数据中进行类的划分，可以把相似的顾客划归到同一类。我们将使用层级 聚类 解决这个问题。\n",
      "\n",
      "但是在实际应用层级 聚类 解之前，我们需要把数据集进行归一化以便于所有变量的尺度是相同的。为什么这一步很重要呢？因为如果变量尺度不同，模型偏向那些拥有更大量级的变量像是生鲜或者牛奶（如上表格）。\n",
      "\n",
      "所以，先将数据归一化，把所有变量放到同一尺度。\n",
      "\n",
      "在这里，可以看到所有变量的尺度几乎是相似的。现在，我们可以开始进行层级 聚类 了。首先画出树状图来帮助我们决定这个问题当中簇的数量：\n",
      "\n",
      "X轴为样本，y轴表征样本之间的距离。距离最大的垂直的线是蓝色的线，因此我们可以决定阈值为6，然后切断树状图：\n",
      "\n",
      "这条线有两个交点，因此我们有两个簇。让我们使用层级 聚类 ：\n",
      "\n",
      "在我们定义2个簇之后，我们可以看到输出结果中0 和1的值。0代表属于第一个簇的值，而1代表属于第二个簇的值。现在将两个簇进行可视化：\n",
      "\n",
      "太棒了！我们现在可以清晰地看到两个簇。这是我们用Python来实现层级 聚类 的过程。\n",
      "\n",
      "写在最后的话\n",
      "\n",
      "层级 聚类 是一种非常有用的划分观察值的方法。优势在于无需预定义集群数量，这使它比k-Means更具优势。\n",
      "\n",
      "如果你对 数据科学 还比较陌生，强烈建议你学习实用 机器学习 课程(https://courses.analyticsvidhya.com/courses/applied-machine-learning-beginner-to-professional?utm_source=blog&utm_medium=beginners-guide-hierarchical-clustering)。这是你可以在任何地方找到的最全面的端到端的 机器学习 课程之一。层级 聚类 只是课程中涵盖的众多主题之一。\n",
      "\n",
      "原文标题：\n",
      "\n",
      "A Beginner’s Guide to Hierarchical Clustering and how to Perform it in Python\n",
      "\n",
      "原文链接：\n",
      "\n",
      "https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/\n",
      "\n",
      "编辑：王菁\n",
      "\n",
      "校对：杨学俊\n",
      "\n",
      "译者简介 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-06\n",
      "title= 三种使用AI攻击网络安全的方法\n",
      "author= []\n",
      "publish_date= 2020-11-06 00:00:00\n",
      "text= 人工智能和机器学习如何逃避网络安全的防护并且完成更快更有效的破坏。\n",
      "\n",
      "专家告诫：攻击者可以用 机器学习 来更快地破解密码，以及建立会藏匿的恶意软件。\n",
      "\n",
      "图片：iStockphoto/metamorworks三个网络安全专家在NCSA和纳斯达克网络安全峰会上解释了 人工智能 和 机器学习 如何逃避网络安全的防护并且完成更快更有效的破坏。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "十月六日星期二，国家网络安全联盟的执行董事Kelvin Coleman，在以 “可用的安全：影响和丈量人类行为的改变”为主题的论坛中对这一部分进行了探讨。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "国立标准与技术学院，员工信息技术实验室的首席Elham Tabassi，是这次“网络安全中的 人工智能 和 机器学习 ：善、恶、丑”讲座中的一位嘉宾。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "查看:\n",
      "\n",
      "Security threats on the horizon: What IT pro's need to know (free PDF) (TechRepublic)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“攻击者能使用AI来躲避检查，藏在不能被找到的地方，并且自动开启反侦查模式。”Tabassi说。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Digital Guardian的数字首席信息安全官Tim Bandos认为，网络安全总是需要借助人类思维来建立更强的防御措施来抵抗攻击。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“AI 是助手，安全分析师和威胁侦查官是超级英雄”他说。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "以下是3种AI和ML被用于网络安全攻击中的方式。\n",
      "\n",
      "数据中毒\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tabsassi说，攻击者有时会瞄准用来训练 机器学习 模型的数据。数据中毒是通过操纵一个训练集来控制模型的预测能力，使模型做出错误的预测，比如标记垃圾邮件为安全内容。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "数据中毒有两种类型：攻击ML算法可用性和攻击算法的完整性。研究表明，训练集中3%的数据遭遇数据中毒会导致预测 准确率 下降11%。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "通过后门攻击，一个入侵者能够在模型的设计者不知情的情况下，在算法中添加入参。攻击者用这个后门使得M L系统 错误地将特定的可能携带病毒的字符串识别为良性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tabsssi说毒害数据的方法能够从一个模型转移到另一个模型。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“数据是 机器学习 的血液和燃料, 用来训练模型的数据应该被予以同模型一样的重视。”她说，“用户信任度是被模型和训练的质量以及其中的数据所影响的。”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tabassi 说业界需要制定一个标准和规则来保证数据的质量，NIST已经在制定国家规范以约束AI的可靠性，规范包含高阶的规则和强调准确性、安全性、偏差性、隐私性和可解释性的技术要求。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "查看 :\n",
      "\n",
      "Social engineering: A cheat sheet for business professionals (free PDF) (TechRepublic)\n",
      "\n",
      "生成对抗网络\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "生成对抗网络 （GANs）是由两个相互对抗的AI组成 -- 一个模拟原有的内容，另一个负责挑出错误。通过二者的对抗，他们共同创立出与原先高度拟合的内容。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nvidia的研究者训练了一个特殊的AI模型来重建吃豆人游戏。（https://www.zdnet.com/article/nvidia-researchers-use-ai-to-recreate-pac-man-without-a-game-engine/）这个模型只是简单地观察了几个小时的游戏，没有借助游戏引擎，Stephanie Condon在ZDNet中解释道。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bandos说攻击者使用GANs来模拟一般的数据传输规律，来将分散系统的注意力，并且找到能使敏感数据迅速撤离的方法。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“因为有了这些能力，他们可以在30-40分钟内完成进出。”他说，“一旦攻击者开始使用AI和 机器学习 ，他们就能自动运行这些任务了。”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GANs还可以用于破解密码，躲避杀毒软件和欺骗面部识别，Thomas Klimek在文章“ 生成对抗网络 ：他们是什么，为什么我们要害怕。”\n",
      "\n",
      "（https://www.cs.tufts.edu/comp/116/archive/fall2018/tklimek.pdf）中如是描述。一个用 机器学习 建立的密码猜测对抗网络（PassGAN system），使用行业标准密码清单上训练模型，最终该网络能够猜测到比其他几个在同样数据集上训练的工具更多的密码。除了生成数据，GANs能创造可以躲避基于 机器学习 检测的恶意软件。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bandos认为用于网络安全的AI算法不得不通过频繁地重复训练才能识别新的攻击。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“恶意软件在进化，我们也要一起进化。”他说。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "他用“混淆”作为例子，比如一个恶意软件的大部分是由合规的代码组成的（所谓用合法代码来混淆/伪装），一个ML算法必须要能够识别其中的恶意代码。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "僵尸程序\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "VMware Carbon Black的高级网络安全策略师Greg Foss讲道，如果AI算法被用于做决策，那么他们也能被操控做出错误的决策。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“如果攻击者理解这些模型，他们就能够用他们做坏事。”他说。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Foss说最近的一次对加密货币交易系统的攻击就是通过僵尸程序执行的。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“攻击者进入系统并且发现计算机程序如何进行交易，然后他们用这个程序去迷惑算法。”他说，“这个也有很多其他应用。”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Foss补充说这个技术不是新的，但是现在这些算法比以更智能了，这大大提高了算法做出一个坏决策的风险。\n",
      "\n",
      "可阅览更多相关文章\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.How to become a cybersecurity pro: A cheat sheet (TechRepublic)\n",
      "\n",
      "2.Social engineering: A cheat sheet for business professionals (free PDF) (TechRepublic)\n",
      "\n",
      "3.Shadow IT policy (TechRepublic Premium)\n",
      "\n",
      "4.Online security 101: Tips for protecting your privacy from hackers and spies (ZDNet)\n",
      "\n",
      "5.All the VPN terms you need to know (CNET)\n",
      "\n",
      "6.Cybersecurity and cyberwar: More must-read coverage (TechRepublic on Flipboard)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "原文标题：\n",
      "\n",
      "3 ways criminals use artificial intelligence in cybersecurity attacks\n",
      "\n",
      "原文链接：\n",
      "\n",
      "译者简介 https://www.techrepublic.com/article/3-ways-criminals-use-artificial-intelligence-in-cybersecurity-attacks/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-04-15\n",
      "title= 仅次于癌症将成人类第二大杀手，面对抑郁症AI能做些什么？\n",
      "author= []\n",
      "publish_date= 2020-11-04 00:00:00\n",
      "text= 近几年，关于抑郁症自杀的社会新闻层出不穷，让这个在过去一直被大众忽视的疾病逐渐浮出水面。上个月，话题#抑郁症成仅次于癌症的人类第二大杀手#登上微博热搜，大众终于对患抑郁症群体有了较直观的认识：抑郁症属于情感性精神障碍疾病，常伴随焦虑发生。临床体现为“三低”——情绪低落，兴趣减退，动力不足，且持续至少2周以上。抑郁还可能增加某些健康问题的风险，如心脏病。据世卫组织预测，到2020年，抑郁症将代替癌症成为世界第二大疾病，大约有8%的男性和15%的女性在他们的一生中会患上抑郁障碍, 严重影响生活质量, 并且其中有近 15%的人选择了自杀。此外，还有相当多的患者根本没有意识到自己患有抑郁症，更没有进行过诊治。因此, 对抑郁症患者进行早期识别诊断, 并及时给予治疗十分重要。近几年， 人工智能 变得越来越“聪明”，并开始在医疗领域取得一些成果。那么， 人工智能 技术能够在治疗抑郁症方面发挥什么作用吗？\n",
      "\n",
      "一、发病预测\n",
      "\n",
      "人工智能 技术针对抑郁症的发病预测，目前主要是使用不同的 机器学习 方法，对可以反映抑郁症倾向的因子及各种疾病标志物来做出判断。目前, 应用于抑郁症的 机器学习 算法主要为传统 机器学习 , 如 支持向量机 、 随机森林 、K-近邻算法、浅层人工 神经网络 等。但近年来随着 深度学习 的发展, 卷积 神经网络 、 自动编码器 、深度置信网络等开始逐渐被应用于抑郁症研究中。利用 机器学习 建立抑郁症预测模型的基本原理为采集抑郁症风险因素、生物标记物等数据, 然后对这些数据进行预处理得到归一化的数据集, 按一定比例分为训练集和测试集, 利用训练集对 机器学习 算法进行训练, 最后用测试集对模型进行性能评估, 并在验证评估过程中对模型进行不断优化。\n",
      "\n",
      "机器学习 的优势在于对数据中潜在规律的挖掘。因此, 建立预测模型的第一步是收集数据。目前，研究的数据主要有年龄、性别、药物滥用等社会人口学资料, 临床收集的躯体症状、心理状态资料,医学仪器采集的脑电信号(Electroencephalography, EEG)、脑部核磁共振成像(Magnetic Resonance Imaging, MRI)、眼动数据、心率变异性 参数 等生理信号数据。其中基于脑电信号和脑部核磁共振成像的应用最为广泛, 但是由于MRI等生理信号的采集成本较高, 故研究者们逐渐探索采集成本低却能很好预测抑郁症的数据，比如语音、表情等。\n",
      "\n",
      "另外，随着科技的进步，采集数据的方式也变得越来越多元化。除了MRI等生理信号的采集，还有包括基于问卷收集调查资料, 通过各类传感器采集健康数据, 从网络平台上获得公开数据等。随着近年来 物联网技术 的发展，使得各种可穿戴设备承载大量的健康信息, 很多研究者将这些数据通过 机器学习 应用于抑郁症领域，包括应用智能手机传感器APP来获取日活动度、睡眠情况、社会交流情况等数据来预测个体的抑郁情况。另外，移动互联网的普及让大量用户通过 Facebook、Twitter 等社交网络平台进行情感表达、日常沟通, 这些数据同样为研究者通过将 自然语言处理 技术、情感分析与 机器学习 结合来对用户的心理健康、情感障碍进行研究提供了机会。\n",
      "\n",
      "例如2014年，中国科学院心理所计算网络心理实验室负责人朱廷劭发起了心理地图PsyMap项目。PsyMap通过网络爬虫整合微博树洞留言信息后，再通过AI对留言进行分析筛查，最后，再由志愿者对有自杀意向的人进行心理危机干预，试图在他们走向终结之前挽救生命。从2017年正式上线至2019年10月，心理地图PsyMap共计给4222人发送了干预私信；2018年，由荷兰阿姆斯特丹自由大学 人工智能 系教授黄智生开启了“树洞计划”。树洞计划寻找濒临危险的抑郁症患者的 逻辑 跟心理地图相差不大，都是利用AI进行筛查、分级，而后介入。但树洞计划的自杀危险程度分级更为明确，十级为最高级，即自杀正在开始；九级则是有明确的自杀计划。随后几级以悲观厌世的程度依次递减，在6级以下，志愿者一般不会直接介入。黄智生表示，运用 知识图谱 技术，AI机器人可通过监控留言分析一个人的情绪。几乎每个月，救援团都能救回50条生命，一年时间里，救援团阻止了约800次自杀。\n",
      "\n",
      "不需要挖掘社交平台上的数据，而是借助了 计算机视觉 和 机器学习 的方法也能用来预测抑郁症指标。2017年，由哈佛大学和佛蒙特大学共同主导的一个项目以166位 Instagram 用户为研究样本，共分析了他们分享在平台上的43,950张照片，其中71张涉及患有抑郁症病史。据研究人员发表在EPJ Data Science上的论文显示，他们使用了 机器学习 工具成功识别出抑郁症的标志，并使用颜色分析、元数据组件和面部检测算法，从43,950张Instagram 照片中计算提取统计特征，而每张照片的色调、亮度以及使用的滤镜都是其重要的分析维度。研究结果表明，患有抑郁倾向的用户更倾向于发布更多照片，且颜色多为蓝色、灰色和深色；其次，他们使用的滤镜也更少；另外，患有抑郁倾向的用户也更喜欢发布人脸特写照片，但与健康的用户相比，其分享的每张照片出现的人脸数量更少。这可能表明，抑郁症患者更喜欢在小范围的社交环境中和人交往。同时，研究结果还表明，在利用 机器学习 分析了这些照片得到的模型之后，所得模型的表现优于普通医师诊断抑郁症的平均成功率。\n",
      "\n",
      "除了采集语音、文字、图像等数据进行研究， 机器学习 与神经影像结合也取得了一定进展。一项研究将ML与MRI相结合预测抑郁症的发作，通过采集33名10~15岁少女各脑区的灰质、皮质厚度数据，采用 支持向量机 技术预测5年内抑郁症发作概率，总体 准确率 达到70%（敏感性69％，特异性70％。而韩国有研究者使用 随机森林 模型发现，家庭关系、社会关系和家庭收入的满意度对于构建抑郁症发病风险的预测模型很重要，说明使用调查数据预测抑郁症的未来发作有潜力。\n",
      "\n",
      "总之，针对抑郁症的发病预测，目前主要是使用不同的ML方法对生理、行为数据进行分析。当前，此类研究还处于尝试阶段，未正式投入临床应用，但已显示出巨大潜力，未来可为抑郁症的一级预防提供支持。\n",
      "\n",
      "二、早期识别\n",
      "\n",
      "早期的抑郁症状很容易与单纯的情绪低落相混淆，不易被察觉，导致患者错失治疗的最佳时机。现阶段研究主要方向是将ML方法与脑电、语音信息、手机使用行为、可穿戴设备采集的信息以及文本自动分析技术相结合，对抑郁症人群进行早期识别。\n",
      "\n",
      "例如2016年，南加州大学的研究人员开发了一款 机器学习 工具，它能够检测出某些语言相关的诊断标准，来评估患者的抑郁症情况。这款工具名为SimSensei，它在医生问诊过程中监听患者语言表达过程中的心理和神经性紊乱异常状况，这些异常很难被问诊者所察觉。研究者表示：“我们评估了253位实验对象的自动评估元音空间，证明了新的检测手段检测到抑郁症和创伤后应激障碍患者的元音空间有显著减小。我们证明了在测试部分交互或数量有限的语音数据时，新技术是健壮的，印证了该方法的实用性。最后，我们成功地显示了该测试结果在不同个体和不同发音速率上的统计鲁棒性。”\n",
      "\n",
      "2017年，来自 IBM 的计算精神病学和神经成像研究小组团队开始尝试利用 机器学习 预测人患精神疾病的风险。该项目以 2015 年发表的研究作为基础，通过对 59 名普通人的语言方式追踪、分析，并对语言连贯性进行评分，确定潜在患病风险。59 位参与者在随后两年中，有 19 名出现了精神障碍，而 AI 预测的精确度达到 83%，这背后的判断依据，是 AI 技术发现处于精神疾病风险的人在说话时使用了较少的所有格代词，并且连贯句子较少，这可能是精神疾病的一部分前兆。不过，关于对于语言的分析是否能够适用于所有语种，以及不同病症是否会有不同的语言倾向，有待进一步研究。\n",
      "\n",
      "2018年，斯坦福大学 人工智能 实验室与视觉实验室负责人 李飞飞 带领团队公布一种基于 机器学习 的抑郁症症状严重程度测量方法，该方法使用了视频、音频和文本数据集，以及因果 卷积 神经网络 模型，通过表情和语音诊断一个人是否患了抑郁症， 准确率 超过80%。不仅如此，该模型还能部署到手机上，从而让更多的人能够进行诊断。值得一提的是，这项研究成果还入选了NIPS NeurIPS 2018医疗健康 机器学习 （ML4H）Workshop。\n",
      "\n",
      "在抑郁症群体中，儿童是一个比较特殊的存在。据统计，大多数自闭症儿童的父母都是在孩子出生后1到3年才发现孩子的变化，在美国，这个年龄中位数为4.3岁。但是，大量研究表明，在综合征完全显现之前，进行早期干预可以降低ASD的严重程度，并改善儿童的大脑和行为发育。有没有一种方法可以缩短儿童出现症状到确诊中间的时间差呢？\n",
      "\n",
      "2020年，一家位于加利福尼亚的公司推出一种新的自闭症谱系障碍（ASD）诊断工具Cognoa，它能在出现相关迹象的几周内做出ASD诊断，远快于当前的标准。Cognoa的技术来自斯坦福大学医学院儿科学副教授Dennis Wall实验室，依据于父母调查，家庭录像和临床医生问卷等数据。研究者表示，该工具的算法是根据来自数百个不同性别、种族和种族背景的实际案例的数据进行训练的，它不仅可以加快诊断时间，而且可以消除当前系统固有的许多偏差。\n",
      "\n",
      "最近，该公司在美国各地的14个地点完成了一项关键的双盲临床试验。目前， 关键试验的结果尚未公布， 但该公司表示，此试验“已超过了FDA同意的目标 基准 ”，在性别和种族上都是准确的。另外，这项研究于2019年7月至2020年5月进行，在今年春季新换大流行期间通过远程医疗对部分儿童进行了远程评估。在远程管理上，该工具的性能也一样好。 公司计划在未来几个月内提交完整的研究报告以供发表，不久将正式提交FDA。如果获批成功，Cognoa将成为首个自闭症谱系障碍诊断工具。\n",
      "\n",
      "目前，AI在抑郁症的早期识别方面取得了可观进展，可以降低人群中抑郁症早期筛查的 假阴性 率，为疾病早期诊断治疗提供更多可能。但同时也存在一定的误诊率，需要临床医师进一步诊断，因此不可完全依赖于AI的分析结果，需视应用场景而定。\n",
      "\n",
      "三、辅助诊断\n",
      "\n",
      "目前抑郁症的诊断是以精神科医师的精神检查为主，这虽然无法用技术替代，但为了实现更加客观、高效的诊断，AI技术逐步应用于对抑郁症患者的辅助诊断。其中，ML与MRI技术的结合，可以辅助诊断抑郁症，同时也可以反映抑郁症的严重程度。基于体素- 相关向量机 模型的诊断 准确率 为85%，敏感性为84%，特异性为85%，基于体素-特征形态- 相关向量机 模型的诊断 准确率 为90%，敏感性为93%，特异性为87%，后者虽提高了总体预测准确性，但提升幅度未超过5%。\n",
      "\n",
      "国内也有研究者利用不同的ML方法，对脑电图、眼动追踪信息、皮肤电数据进行分析，准确度均在65%以上。将这3种模式组合用作分类器的输入，发现通过logistic算法获得的准确度最高为79.6％，在整体上提高了诊断准确性。例如， 望里科技 的AI抑郁评测系统利用脑电、眼动、皮电等信息采集的生理数据，对抑郁症进行客观的评估。通过复杂的数据运算，该系统可以寻找将抑郁症患者与健康人群进行有效区分的计算机模型。通过与 北京大学 第六医院的科研合作，目前 望里科技 的抑郁辅助诊断评估分类 准确率 已达到81%。据悉，未来该系统将拓展到自闭症、精神分裂、老年痴呆、暴力倾向等问题的研究和产品开发。并且， 望里科技 还在上述系统的基础上开发了自杀风险评估系统，这一产品被运用于服刑人员的心理管理，帮助狱警了解并管理服刑人员心理健康状况，达到预防服刑人员自杀的目的。该产品在教育、医疗领域也有应用的潜力。\n",
      "\n",
      "AI技术也可以用于量表开发，促进情感障碍的高效鉴别诊断。通常，双相情感障碍与抑郁症难以鉴别，有研究者利用 机器学习 的RF模型来优化情感障碍评估量表，进而开发出更为简洁的中文双相情感障碍诊断清单，并将其应用于临床实践，便于对双相情感障碍和抑郁症进行快速有效的鉴别诊断。\n",
      "\n",
      "四、治疗\n",
      "\n",
      "1、疗效预测\n",
      "\n",
      "事实上， 机器学习 在抑郁症诊疗中最突出、普遍的应用之一，就是其在药物治疗结果上的使用。因为抑郁症的发病机制目前仍不清楚，在过去50年抑郁症的药物治疗中，大约有 70%的病人是症状改善，还有30%的抑郁症病人是药物不起作用。如果检索在抑郁症诊疗中应用 机器学习 的期刊就会发现，大部分的论文都将重点放在了精神药物治疗上。\n",
      "\n",
      "其中一项著名的研究利用 机器学习 对相关症状进行 聚类 ，随后建立了一个 机器学习 模型来评估几种主要抗抑郁药物的疗效。结果发现了三组症状，并发现研究涉及的几种抗抑郁药的疗效存在统计学上的显著差异。这表明医生在给抑郁症患者开药时，应该根据患者所表现的具体症状对症下药。\n",
      "\n",
      "除了对药物疗效进行预测，也可以通过ML方法对物理治疗的疗效进行有效预测。\n",
      "\n",
      "2、心理治疗\n",
      "\n",
      "当前，对于抑郁症的常规治疗方式是以心理治疗为主，药物治疗为辅。而利用 人工智能 技术对患者进行心理治疗，则是通过构建虚拟场景、虚拟人物，结合心理治疗师对抑郁症患者进行认知行为治疗、人际心理治\n",
      "\n",
      "接受治疗的患者和VR环境的影像\n",
      "\n",
      "2016年，英国伦敦大学学院等机构研究人员与西班牙同行开展一项研究，让15名年龄在23岁至61岁间的抑郁症患者配戴 虚拟现实 头盔，并通过与 虚拟现实 场景中的虚拟人物互动开展相关治疗。研究人员表示， 虚拟现实 头盔能让患者“代入”一个虚拟化身，在虚拟环境中与其中人物进行互动。试验中，研究人员让患者化身与一个情绪低落的虚拟小孩交流，学会如何向小孩表达同情心。在程序设定下，这个小孩会对患者化身的讲话产生积极反应，逐渐停止哭泣。然后研究人员让患者反过来“代入”到小孩身上，从小孩的视角来观察整个交流过程。每名患者接受3次这样的 虚拟现实 治疗。结果显示，完成疗程的一个月后，有9名患者的症状出现缓解迹象，其中4人的抑郁症严重程度下降十分明显。\n",
      "\n",
      "借助 虚拟现实 技术开展心理治疗时，还可利用ML算法开发心理智能 聊天机器人 。例如2019年，Flow宣布推出其 聊天机器人 治疗师来治疗抑郁症。该款名为Flow的 聊天机器人 治疗师，每天与用户进行对话，并提供自助技术、情绪跟踪、精选视频、冥想和心理锻炼。它帮助用户了解为什么睡眠、锻炼、营养和冥想是抑郁症康复的主要支柱，并收集情绪数据以行为疗法为模型，提供个性化的反应。\n",
      "\n",
      "随着 5G 的普及，相信 虚拟现实 技术能够突破硬件条件的限制，在治疗抑郁症方面有更多应用能够落地。尽管这种方式不能取代心理治疗师的角色，但仍提供了一种经济高效的治疗方案，同时也可以作为心理治疗的辅助工具在临床应用。\n",
      "\n",
      "五、 人工智能 技术在抑郁症应用中存在的问题\n",
      "\n",
      "目前，以抑郁症为代表的精神类疾病大多病因未明，遗传因素、社会心理因素、素质因素都可能与发病有关。业内有一句经典的比喻：现在人类对大脑的认知水平就好比在黑夜的足球场角落点了一支蜡烛。这形象地说明了当下人类认知的困境，而依赖人类输入知识进行判断的 人工智能 自然也受此影响。除此之外，在研究过程中还面临以下问题：\n",
      "\n",
      "1、样本代表性较差\n",
      "\n",
      "目前大部分基于 人工智能 对抑郁症诊疗的研究普遍样本量较小，不能很好地反映总体抑郁症人群的特征。如何实现智能手段对疾病的预测、鉴别、诊断、治疗，需要足够的样本数据充分覆盖每个群体的特征进行计算，避免出现由于抽样误差导致的数据偏倚。\n",
      "\n",
      "2、智能设备相关的伦理问题\n",
      "\n",
      "智能手机为代表的移动设备虽然为患者带来了诸多好处，但对加强隐私保密措施的研究仍然有限。尽管患者的信息通常是匿名的，但数据重新识别技术仍然对个人信息构成潜在威胁。由于抑郁症患者的特殊性，临床信息的泄漏可能会导致情绪创伤、恶化病情。因此，未来的政策应着重于隐私问题的解决，在数据的有益利用与个人隐私之间取得平衡。\n",
      "\n",
      "3、临床应用面临的困难\n",
      "\n",
      "因临床数据越来越复杂，研究人员必须处理不同类型、不同来源的大数据，如人口数据、图像数据、遗传信息数据、社交网络数据等。临床数据的多样性增加了设计算法和建立推理模型时的复杂程度和困难程度，因此大多数研究仅停留在模型建立等理论基础上，还未将大 数据分析 的成果转化为临床应用。\n",
      "\n",
      "而至于AI+精神健康商业化发展，更是有很长的路要走，尤其是目前企业探索的部分诊疗方式正饱受质疑。其中，通过情绪识别来监控情绪并判别心理状态这一做法所受到的质疑最大。因为在高速运转的社会中，饱经规训的人们并不会把情绪明晃晃地写在脸上。为探讨情绪识别算法与真实情感的相关度，美国心理科学协会曾委托五位来自该领域的杰出科学家进行了数据收集和科学证明。最终，五位科学家给出的论文显示，情绪的表达方式多种多样，很难从一组简单的面部运动中可靠地推断出一个人的感受，表情与心情之间没有坚实的科学依据证明有直接关联。这也是为什么微软、谷歌、 IBM 、亚马逊等公司尚未将情绪识别算法投入诊疗市场的原因之一。\n",
      "\n",
      "另外，患者的接受度也对 人工智能 的应用提出了考验。无论是心理咨询还是精神科问诊，都要求患者主观上对咨询师/医生充分信任，积极配合治疗。但目前，人类对AI的心理检测接受度普遍不高，这是由于心理检测中涉及很多主观判断，如焦虑、心境低落等等。人类医师具备共情能力，更易被患者认为“听懂了我的话”，而 人工智能 则易被视为“冷冰冰的机器”，难以获得信任。\n",
      "\n",
      "所以，很多从业者认为，尽管AI对于抑郁症的诊疗研究，已在影像、智能穿戴、文本识别等方面有了突破，但它应该是最后的防线，而不应该是前线。真正能让抑郁症患者从阴霾中走出来的不是科技，而是有触感的关怀。而对于患者而言，自己的努力也非常重要，毕竟就像鲁迅所说，“人类的悲欢很多并不相通”，再专业的救助也很难做到完全将心比心，最终还是需要靠自己走出来。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-03-2\n",
      "title= 基于数据预测的解释真的能增加用户对人工智能的信任吗？（附链接）\n",
      "author= []\n",
      "publish_date= 2020-11-03 00:00:00\n",
      "text= 本文为大家介绍了一项最新的研究进展，它有助于理解人工智能的可解释性实际上如何影响用户对人工智能的信任。\n",
      "\n",
      "近年来，许多 人工智能 (AI)和机器人领域的研究人员一直努力开发能够解释 人工智能 的预测或机器人行为的系统。他们工作背后的想法是，随着 人工智能 系统的普及，解释它们为什么会以特定的方式行动或为什么会做出某些预测可以提高透明度，从而提高用户对它们的信任。\n",
      "\n",
      "最近，雷恩布列塔尼大西洋研究中心和图卢兹法国国家科学研究中心的研究人员进行了一项研究，对这一假设提出了质疑并进行了探索，希望能更好地理解 人工智能 的可解释性实际上如何影响用户对 人工智能 的信任。\n",
      "\n",
      "他们发表在《自然机器智能》(Nature Machine Intelligence)上的论文认为， 人工智能 系统的解释可能并不像一些用户认为的那样真实或透明。\n",
      "\n",
      "“这篇论文源于我们想要探索直观差距的愿望，”进行这项研究的两名研究人员Erwan Le Merrer和Gilles Tredan告诉TechXplore。“（这种差距在于）作为彼此有互动的人，我们总是习惯于不相信提供的解释。但作为计算机科学家，我们不断听到可解释性是公众接受 人工智能 的首要条件。”虽然我们认识到 人工智能 在某些情况下的好处(例如， 人工智能 设计师在“白盒”上操作)，但我们想从用户(即：“黑盒”)的角度来说明它的局限性。”\n",
      "\n",
      "许多研究人员最近提出， 机器学习 算法和其他 人工智能 工具应该能够解释其决策背后的基本原理，就像人类一样。另一方面, Le Merrer and Trédan认为虽然AI的解释可能在本地环境中有价值,例如开发人员正在努力调试系统以提供有用的反馈,但在远程环境下，它们可能具有欺骗性，因为 人工智能 系统由特定的服务供应商训练和管理，所以它的决策是通过第三方传递给用户的。\n",
      "\n",
      "Le Merrer和Tredan解释说:“对于采用基于 人工智能 算法的决策而言，用户对其所面临决策的理解是一个核心的社会问题。”“我们揭示了来自供应商的 逻辑 解释总是容易受到攻击(即，谎言)，这对于一个孤立的用户来说是很难或不可能检测到的。我们的结果表明，特征空间和可能攻击的空间是非常大的，所以即使用户联合起来发现问题，这些谎言仍然很难被发现。”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了更好地解释他们的想法背后的原因，Le Merrer和Tredan用夜总会外面的保镖做了一个类比，这些保镖在向个别顾客解释他们为什么被拒之门外时可能会撒谎。类似地，研究人员认为，远程服务供应商可能在 人工智能 预测或行为背后的原因上对用户撒谎，例如，利用区别特征。在他们的论文中，他们将两者间的这种类似称为“保镖（bouncer）问题”。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Le Merrer和Trédan说，“我们的工作质疑了一个人们普遍相信的观点：解释会增强用户对AI系统的信任。”“我们宁可得出相反的结论：从用户的角度出发且在没有既存信任时，解释很容易成为谎言，并且因此可以通过任何方式解释任何事情。我们认为应该使用其他方法（例如，内部白盒算法审核，加密方法等）来寻求用户的信任。”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Le Merrer和Trédan在他们的论文中提供了一些实例以展示“保镖问题”是如何影响远程情境中AI行为的可解释性。在未来的研究中，他们的工作也许会激发进一步关于探索开发 机器学习 算法或机器人的好处和局限性的研究，这些好处和局限性可以解释其行为背后的原因，同时也可能促进替代方案的开发，以提高人们对AI的信任。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Le Merrer和Trédan说，“我们计划继续从用户角度（即“黑匣子”）研究 人工智能 系统，特别是探讨这个问题：普通用户可以发现/学习/理解/推断哪些AI系统正在成为他们生活中日益增长的一部分吗？”“例如，我们目前正在研究那些声称没有使用此方法的平台上的用户阴影禁止（即阻止或部分排斥用户无法访问在线社区）现象。”\n",
      "\n",
      "相关链接：\n",
      "\n",
      "Erwan Le Merrer et al. Remote explainability faces the bouncer problem, Nature Machine Intelligence (2020). DOI: 10.1038/s42256-020-0216-z\n",
      "\n",
      "期刊信息：Nature Machine Intelligence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "原文标题：\n",
      "\n",
      "Do explanations for data-based predictions actually increase users' trust in AI?\n",
      "\n",
      "原文链接：\n",
      "\n",
      "https://techxplore.com/news/2020-10-explanations-data-based-users-ai.html \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-03-3\n",
      "title= 包含近 20 万本图书，OpenAI 级别的训练数据集上线\n",
      "author= []\n",
      "publish_date= 2020-11-03 00:00:00\n",
      "text= 这些数据集中共包含 196640 册纯文本数据，可以用于训练 GPT 等大型语言模型。\n",
      "\n",
      "近日， 机器学习 社区的一篇资源热贴「用于训练 GPT 等大型 语言模型 的 196640 本纯文本书籍数据集」引发了热烈的讨论。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "该数据集涵盖了截至 2020 年 9 月所有大型文本语料库的下载链接。除此之外，它还包含了所有的 bibliotik（一个线上图书资源库）中书籍的纯文本，以及大量用于训练的代码。\n",
      "\n",
      "数据集中除文本数据外，还包含了 100GB 的训练代码\n",
      "\n",
      "196640 册图书数据，训练你的 GPT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "reddit 的 机器学习 社区上，网友 Shawn Presser 发布了一套纯文本数据集，得到一致好评。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这些数据集中共包含 196640 册纯文本数据，可以用于训练 GPT 等大型 语言模型 。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "由于这套数据集包含多个数据集以及训练代码，我们在此不一一赘述，仅将其中的 books1 与 books3 数据集的具体信息列出：\n",
      "\n",
      "图书纯文本数据集\n",
      "\n",
      "发布作者：Shawn Presser\n",
      "\n",
      "包含数量：books1：1800 册图书；book3：196640 册图书\n",
      "\n",
      "数据格式：txt 格式\n",
      "\n",
      "数据大小：books1：2.2 GB；books3：37 GB\n",
      "\n",
      "更新时间：2020 年 10 月\n",
      "\n",
      "下载地址：https://hyper.ai/datasets/13642\n",
      "\n",
      "据数据集整理者 Shawn Presser 介绍，这些数据集的质量是非常高的，仅 books1 数据集，就花费了他大约一周的时间，对 epub2txt 脚本进行修复。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "此外，他还表示，books3 数据集似乎与 OpenAI 的论文中神秘的「books2」数据集相似。但是，由于 OpenAI 并没有提供这方面的详细信息，所以也无法了解二者之间的任何差异。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "不过，在他看来，这份数据集极其接近 GPT-3 的训练数据集。拥有它，下一步，你也可以训练出与 GPT-3 相匹敌的 NLP 语言模型 ，当然，还有一个条件是，你还需要准备足够的 GPU。\n",
      "\n",
      "数据集中 books1 数据集部分内容示例\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "据介绍，books1 数据集中 1800 本图书文本数据，都来自于大型文本语料库 BookCorpus，其中包括诗歌类、小说类等。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如美国作家 Kristie Lynn Higgins 的《Shades of Gray：Noir, City Shrouded By Darkness》（《灰色阴影：被黑暗笼罩的城市》）、Benjamin Broke 的《Animal Theater》（《动物剧院》）、T·I·韦德的《America One》（《美国一号》）等。\n",
      "\n",
      "强大的 GPT-3 背后，训练数据集立功劳\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "关注 自然语言处理 领域的小伙伴都知道，今年 5 月，OpenAI 斥巨资打造的 自然语言处理 模型 GPT-3，凭借惊人的 文本生成 能力，在业界引起高度关注，并且一直以来热度不减。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GPT-3 不仅可以更好地答题、翻译、写文章，还带有一些数学计算的能力。而它之所以拥有这些强大的能力，离不开背后巨量的训练数据集。\n",
      "\n",
      "GPT-3 训练数据集一览\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "更大的训练数据集、更多的模型 参数 ，让 GPT-3 在 自然语言处理 模型中一骑绝尘。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "然而，对于普通开发者来说，想要训练出一流的 语言模型 ，暂且不说高昂的训练成本，仅仅在训练数据集这一步，就会被卡住。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "因此，Shawn Presser 带来的数据集无疑解决了这一难题，一些网友表示，这项工作他们节省了巨大的成本。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我们目前已经将 books1 数据集搬运至 https://hyper.ai，搜索关键词「书籍」或「文本」获取数据集。\n",
      "\n",
      "其它数据集可从以下链接中获取：\n",
      "\n",
      "books3 数据集下载地址：\n",
      "\n",
      "https://the-eye.eu/public/AI/pile_preliminary_\n",
      "\n",
      "components/books3.tar.gz\n",
      "\n",
      "训练代码下载地址：\n",
      "\n",
      "https://the-eye.eu/public/AI/pile_preliminary_\n",
      "\n",
      "components/github.tar\n",
      "\n",
      "reddit 原帖：\n",
      "\n",
      "https://www.reddit.com/r/MachineLearning/comments/ji7y06/p_dataset_of_196640_books_in_plain_text_for/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-11-02-2\n",
      "title= 打工人的疲劳和压力，别人不懂 AI 懂\n",
      "author= []\n",
      "publish_date= 2020-11-02 00:00:00\n",
      "text= 上班族的压力到底有多大呢？现在，AI 系统可能比本人都了解得更清楚。\n",
      "\n",
      "晚上好，打工人。今天的工，打完了吗？\n",
      "\n",
      "最近，「打工人」的梗突然在一夜之间爆火，全网都在「打工人」，也不知道工人犯了什么错，要被大家打。 不过「打工人」段子的流行，让广大劳动人民得到了更多关注。 上班族 的压力到底有多大呢？现在，AI 系统可能比本人都了解得更清楚。\n",
      "\n",
      "「打工人」的疲劳，别人不懂 AI 懂\n",
      "\n",
      "事实上，一个个调侃「打工人」的段子，看起来无奈中透露着辛酸，辛酸中透露着难过，但从心理学角度来讲，这些充满辛酸的自嘲背后，更是 上班族 们对于工作中面临的重重压力和焦虑的缓解。 在当今快节奏的社会， 上班族 都面临着工作上的「高压」，加班已是家常便饭，这也往往让人疲劳不堪。 摘自《2020 年智康健康问卷调研》，来源：智联招聘研究表明，在疲劳、超负荷状态下工作，出现失误的概率也会明显增加。尤其在医疗、交通、需要轮班之类的工作中。 近日，麻省理工学院林肯实验室（隶属于美国国防部）的研究人员，开发了一个 AI 系统，旨在 感知 人的认知疲劳，以及决定何时干扰其表现。同时，该系统还会提出干预措施的建议，以帮助个人及时恢复清醒，防止造成伤害。 林肯实验室研究员 Megan Blackwell说：「纵观历史，我们发现人为失误会导致不幸，错失良机，有时甚至造成灾难性后果。我们正想办法使用技术来监测疲劳或认知超负荷。比如，这个人是否太过专注？换句话说，他们是不是快精疲力尽了呢？如果能用技术实时监测的话，就可以在事故发生之前进行干预。」\n",
      "\n",
      "从生物数据，「读懂」打工人何时疲劳\n",
      "\n",
      "该实验室数十年来的一项研究，就是利用技术「读取」一个人的认知或情绪状态。 通过收集生物特征数据(比如说视频和音频)，并用算法处理这些数据。研究人员发现了各种心理和神经行为状况的生物标记，而且，这些生物标记已被用于训练模型，以准确估计一个人的心态。 大脑持续较久或高强度劳动时 会出现注意力不集中、思维不敏捷等各种状况 在目前这项最新研究中，该团队将把他们的生物标志物研究，应用于 人工智能 系统，可以分析个人的认知状态，包括一个人的疲劳、压力或超负荷的感觉。 在目前这项最新研究中， 该系统将使用来自生理数据的生物标志物，如声音和面部记录、心率、脑电图和大脑活动的光学指标以及眼球运动来获得这些信息。\n",
      "\n",
      "拯救「打工人」：疲劳警告+干预措施\n",
      "\n",
      "认知模型：监控生理输入，识别意识偏差 林肯实验室神经行为生物标志物研究负责人 Thomas Quatieri 介绍，建立该系统，首先是建立一个个体的认知模型。 「认知模型将整合生理输入，并监控这些输入，以观察当一个人执行特定的令人疲惫的任务时，这些输入是如何变化的。通过这个过程，系统可以建立活动模式，并了解一个人的基本认知状态，比如听觉和视觉注意力以及反应时间。」 一旦建立了这个个性化的基线，系统就可以开始识别偏差，并预测这些偏差是否会导致错误或不良的后果。 林肯实验室国土保护和空中交通管制部门的主要工作人员 William Streilein 说：「人是十分复杂的，我们会对压力或疲劳进行自然补偿。而我们目前要做的，最重要的就是，建立一个系统，能够预测什么时候这种偏差不会被补偿，并且只在那时进行干预。」 疲劳驾驶是高速交通事故的主要成因 司机往往因认知偏差未能及时补偿而造成这一行为 及时干预：如果喝咖啡不行，就上直流电刺激 系统检测出过疲劳或压力过大的情况后，会提供两种干预措施。 一种是，建议做出一些微小调整，比如让员工喝咖啡，改变照明强度，呼吸新鲜空气等；或者建议将任务交给机器或其他同事。 AI 系统会在必要时发出疲劳警告而另一种干预措施较为强烈，会使用经颅直流电刺激。这是一种非侵入性的大脑神经调控手段，使用电极来刺激大脑，使其部分性能得以恢复。而且，研究显示，该方法能比咖啡因更有效地对抗疲劳，而且副作用更小。\n",
      "\n",
      "AI 关心你打工累不累 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2020-10-30-2\n",
      "title= 深度学习 V.S. 谜题游戏\n",
      "author= []\n",
      "publish_date= 2020-10-30 00:00:00\n",
      "text= 本文为大家介绍了作者使用不同的算法来解决Free Flow谜题游戏的心路历程，从一开始的A*，Q-learning，到最后的卷积神经网络，作者详细的介绍了在使用这些算法时遇到的困难和得到的启示。\n",
      "\n",
      "深度学习比古老的蛮力技术更适合解决FlowFree问题吗?\n",
      "\n",
      "我们都有过这种经历。你拿着手机发呆，想要消磨一些时间，于是你决定放弃自己优秀的思考，打开应用商店的游戏区，看看游戏排行榜。你找到了一个看起来很有趣的谜题游戏，但其实游戏是否有趣并不重要，因为你只想玩半个小时，然后就删掉、忘记它，对么？\n",
      "\n",
      "2个月以后，我完成了两千多关的Flow Free①游戏，并且坚持每一关都得到“完美”评分。这一游戏（自2012年发行以来，这款游戏在iOS和Android平台上都是最受欢迎的游戏之一）的设定相当简单：将不同颜色的阀门分别连起来，并且不能有线相交。\n",
      "\n",
      "FreeFlow——你可以通过这个截图来了解游戏截图中的关卡可能看起来很简单，但是它的难度确实在不断提升。随着关卡的进行，我发现我自己想出了一些可以帮助我更快完成这些高级关卡的策略（例如：尽可能的保留外层边界空白，避免在未填满的方形中创建“港湾”等）。浏览网上论坛时，我看到其他的玩家都有他们自己的技巧，有的和我的一样，有的则略微不同。这就引出了问题——计算机能否通过“经验”，而非蛮力，来学习这些技术？\n",
      "\n",
      "一个人类解决FreeFlow谜题从基础开始：A*\n",
      "\n",
      "如今的深度学习问题大部分可以归结为要使用哪种算法。我以A*搜索作为开始。尽管它本身不是深度学习，但是它也很好的说明了这个问题内部的复杂性，并且也给了我一些机会，来使用更高级的、能够自我驱动的启发式算法。\n",
      "\n",
      "由于空间的复杂性太大，以至于我无法一次性解决问题。所以我开始按颜色递归地解决问题（如果给定路径被“堵塞”，就回溯到上一种颜色）。在启发式算法中，我使用了较为可信的曼哈顿距离。我在四种尺寸的板子上测试了我的算法：微型（2✖4）、小型（5✖5）、中型（8✖8）、大型（14✖14）。我保证了大型和中型的板子上的颜色比一般的颜色要少。因为大、中型的板子的确会使问题更难，对人和计算机都是如此（更多可能的路径/选择/状态）。\n",
      "\n",
      "这个算法在小型板子上运行的很好，但是花了相当多的时间。因此，我为下一个状态的函数增加了一些规则，以此希望强化学习的算法能够自己找出这些规则：防止同色相邻方格不连接，或者出现空“港湾”。\n",
      "\n",
      "我那台7岁的电脑得到的结果是很让人开心的，但是仍需改进：\n",
      "\n",
      "惭愧地说，我可能花在Tkinter图形函数上的时间比实际的AI要多如果你觉得你是第一个这么做的，那你很有可能就错了\n",
      "\n",
      "在我使用强化学习之前，我一直尝试优化我的A*算法。当我发现Matt Zucker的一篇优秀的博客文章②时，他已经为Flow Free建立了一个A*解算器（很高兴的看到，我不是唯一一个有这种困扰的人），并且更加仔细地考虑过要把这些状态从他的A*搜索中剔除。更让人惊讶的是，他发现了一个只有6条规则的简单SAT算法，这比他最好的A*搜索法还表现还要好，而且在SAT中求解时间也非常短(对于14x14的板甚至低于1秒)。\n",
      "\n",
      "到目前为止，我们似乎都得出了同样令人沮丧的结论：对于这类(非对抗性的，封闭的)谜题游戏，简单的蛮力技术将超过基本的人工智能算法。\n",
      "\n",
      "不过，我不可能止步不前。毕竟，引发这一实验的问题仍然存在：作为人类玩家，在玩了几个关卡后，我们就能发现一些能够更有效地打败Flow Free谜题特定的技巧。为什么机器不能学习同样的技术呢?\n",
      "\n",
      "提到强化学习\n",
      "\n",
      "我非常兴奋地在Flow Free问题中尝试了Q-learning算法，因为“AI”中的“I”即将要发挥它真正的作用了。A*搜索的工作也绝不是浪费时间，因为我们可以使用它的结果作为Q-learning智能体的状态-动作空间。状态空间由板上的方块颜色和哪条路径（颜色）目前是“活跃的”两部分组成。一个“动作”是指填充活跃路径的下一个方块。\n",
      "\n",
      "在一开始犯了一些基础的错误之后，智能体很快就学会了如何解决小型板问题(1.5秒内100次迭代)——到目前为止看起来还不错。但在中板上，花了10分钟，经过10000次迭代后，仍然没有结果：\n",
      "\n",
      "当你听到“人工智能”时，你想的结果可能和这个不一样。\n",
      "\n",
      "为了改善这种情况，我开始调试标准Q-learning的参数(学习率α、贴现率γ、勘探/开发率ε)，但这并没有很大帮助，所以我将注意力转向到奖励函数。除了实际的“奖励”之外，还需要一个参数与奖励函数进行权衡（否则风险就会变得规范化，这会不利于整个机器学习活动）：智能体是否会因为解决了一条路径获得奖励，而不是解决了整个谜题之后。不幸的是，这并没有起到多大作用。\n",
      "\n",
      "最后，逐渐清晰的是，算法之所以会在更大的板上苦苦挣扎，仅仅是因为操作空间太大了。相比于A*搜索，Q-learning在这种情况下确实做出了更明智、更有帮助的选择，但在大多数情况下，即使在10000次迭代之后，Q-learning 智能体还没有看到确切的结果。因此，下一步自然就是：\n",
      "\n",
      "近似 Q-learning\n",
      "\n",
      "近似Q-learning的应用是十分有吸引力的，尤其是在游戏中。与其让智能体在给定的状态下决定最佳的操作，不如让它在每一步都能快速计算出一些直观、独立于具体状态(棋盘的配置)之外的特性，并让它自己决定哪些是最重要的。这在游戏中会担任游戏改变者的角色，例如Pcaman（举个例子，下一步的决策是基于最近的豆子和最近的幽灵，而不是在每种可能状态下的一个动作），当然也可以是状态数量太多，以至于让准确Q-learning失效的Flow Free。\n",
      "\n",
      "现在是开发人员需要权衡利弊挑选“正确”的特征的时候了。于是，我将列表精简到只有我认为在很多情况下重要的特征(例如，每条路径的剩余曼哈顿距离)。还有一些我认为重要的、只需要让算法计算的特征(但无法证明)。这些特征包括：\n",
      "\n",
      "“转弯”的个数；\n",
      "\n",
      "“盒子”的个数；\n",
      "\n",
      "盒子中阀门的个数；\n",
      "\n",
      "没有阀门的盒子的数量(人们可以从逻辑上证明这是不可能发生的——我想看看算法能不能算出来)；\n",
      "\n",
      "一条路径是否“抱墙”(即，一条路径是否可以与相邻的另一条路径相连)。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "不幸的是，有了这些特征之后，Q-learning智能体甚至不能解决小型板问题，即使我们改变了Q-learning的参数也不行。但是，这也是个有趣的实践。例如，该算法在“没有阀门的盒子”上附加了一个强大的负权重，这意味着它能够了解到，没有阀门的盒子会导致谜题无法解决。\n",
      "\n",
      "近似Q-learning发现了更好的游戏策略\n",
      "\n",
      "或许有了更大的谜题样本，它就能更好地学会如何真正地解决它们，但我很兴奋地看到它能真正地找到重要的东西。这在AI中是一个非常有趣的现象，尤其是在游戏AI中非常普遍：即使一个算法不能赢得游戏，它也可以帮助人类找到玩的更好的技巧。\n",
      "\n",
      "走向监督学习：卷积神经网络\n",
      "\n",
      "一开始，我对通过监督的方法来实现Free Flow的想法存有偏见。首先，这需要大量的Free Flow游戏样本，而这些游戏很难在公共互联网上找到。其次，似乎与Free Flow最接近的监督学习方法——神经网络——是一个臭名昭著的黑盒算法，它会妨碍这个练习最有趣的部分：查看算法学习何种技术来解决这个难题。\n",
      "\n",
      "然而，后来我偶然读到了Shiva Verma在《Towards Data Science 》③杂志上的一篇文章，他在其中做了一些与数独游戏非常相似的事情：本质上是把一个数独游戏板当作一个图像，然后使用卷积神经网络(CNN)来解决它。作者在数独游戏中取得了很好的效果，这让我重新审视了我最初的想法，并尝试了这种方法来实现Flow Free。\n",
      "\n",
      "当然，第一个困难是获得输入的数据：用解析文本格式来寻找Free Flow谜题的答案，要比数独谜题更困难。最初，我发现寻找文件最好的方法是查看Android应用程序的代码，它有一千多个以文本格式存储的谜题：\n",
      "\n",
      "CNN的初始结果是令人失望的：尽管CNN正在学习它任务是创建路径，而不是仅仅在孤立地填充颜色，但在测试样本中只有0%的谜题被解决了。\n",
      "\n",
      "我们需要更多的数据\n",
      "\n",
      "调整分层、训练次数、内核大小和其他类似的常见疑点并没有多大帮助。这似乎又回到了数据科学家最常见的问题:没有足够的数据，世界上最好的算法也什么都不是。我找到的其他最好的数据来源是www.flowfreesolutions.com，它有成千上万的对于全新的Free Flow问题的解法，比我的解法要多... 但是是图片格式的。\n",
      "\n",
      "尽管我尝试了无数次，通过各种渠道联系他们（甚至提供了经济奖励），我还是没能让他们给我发送一个可解析的文本版本的解决方案。好吧，这不是一篇免费的人工智能文章——当一个人有现代图像处理器的时候，谁还需要底层的解决方案矩阵呢？使用子项目建立一个Free Flow④解决方案图像处理器:\n",
      "\n",
      "Scikit-image FTW利用对称性来增加我们可用的数据\n",
      "\n",
      "这就产生了几千个用来研究的数据点，但这仍然不是很多。但后来我意识到，就CNN而言，颜色的确切值并不重要，重要的是颜色是不同的。所以我们可以将颜色重新排列，并且仍然有另一个非冗余的数据点。即使对一个只有六种颜色的5x5的板来说，这可以给我们的CNN提供多达6！= 720倍的研究数据点（当然，等大的板和更多的颜色会有更多可选的组合）。\n",
      "\n",
      "Scikit-image FTW\n",
      "\n",
      "数学家将从群论中认识对称群S_n\n",
      "\n",
      "一位朋友指出，这实际上是游戏AI中增加数据点的常见方式。在720x的数据点的情况下，我们最终在一个20-epoch模型，使用大小约为200k的数据点测试集，在我7岁的GPU上运行了20分钟后得到的准确率为12%。需要注意的是，我们在这里使用了一个严格的衡量成功的标准：即使板子少用了一个单元格，我们也将其视为失败。\n",
      "\n",
      "但是，失败比成功有趣得多。在几乎所有的失败中，CNN正确地解决了板子的很大一部分，剩下的部分人们则很容易完成它。从这个意义上讲，CNN能够解决文章最初的问题：凭直觉学习人类的技巧：\n",
      "\n",
      "CNN的误差分布结论\n",
      "\n",
      "对于解决基于网格的、非对抗性的谜题游戏来说，简单的方法通常更好。事实上随着方法逐渐变得高级，他们的表现会越来越差。\n",
      "\n",
      "然而，尽管更高级的机器学习方法不能很快地解决这个难题，但它们确实发现了一些有趣的见解，并帮助人类得到更好的解决方案——卷积神经网络在这方面做得最好。而且，它们的性能比传统的解决方法更好。\n",
      "\n",
      "更好的数据比更好的算法重要。\n",
      "\n",
      "后续：读者和编者的建议\n",
      "\n",
      "我们请了一些更有经验的AI/ML专家(非常感谢Matt Zucker⑤、Youssef Keyrouz⑥和Mark Saroufim⑦)来检阅这篇文章，他们建议尝试以下想法来改进CNN算法。第2部分文章的主题可能会详细介绍，您也可以在https://github.com/kgaspard/flow-free ai上自己动手尝试这些想法（以及本文中详细介绍的方法）：\n",
      "\n",
      "改变CNN的层数（减少特征看上去似乎没有什么用）；\n",
      "\n",
      "除了使用对称来增加我们的数据点，还使用旋转、反射等方法；\n",
      "\n",
      "使用CNN的预测结果作为特征的强化学习智能体。\n",
      "\n",
      "链接：\n",
      "\n",
      "①：https://www.bigduckgames.com/\n",
      "\n",
      "②：https://mzucker.github.io/2016/08/28/flow-solver.html\n",
      "\n",
      "③：https://towardsdatascience.com/solving-sudoku-with-convolution-neural-network-keras-655ba4be3b11\n",
      "\n",
      "④：https://github.com/kgaspard/flow-free-ai/blob/master/imageParser.py\n",
      "\n",
      "⑤：https://mzucker.github.io/swarthmore/\n",
      "\n",
      "⑥：https://www.researchgate.net/profile/\n",
      "\n",
      "Youssef_Keyrouz\n",
      "\n",
      "⑦：https://medium.com/@marksaroufim\n",
      "\n",
      "原文标题：\n",
      "\n",
      "Deep Learning vs Puzzle Games\n",
      "\n",
      "原文链接：\n",
      "\n",
      "https://towardsdatascience.com/deep-learning-vs-puzzle-games-e996feb76162\n",
      "\n",
      "编辑：黄继彦\n",
      "\n",
      "校对：汪雨晴\n",
      "\n",
      "译者简介 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://www.jiqizhixin.com/articles/2024-02-05-10\n",
      "title= 罕见！苹果开源图片编辑神器MGIE，要上iPhone?\n",
      "author= []\n",
      "publish_date= 2024-02-05 00:00:00\n",
      "text= 拍张照片，输入文字指令，手机就开始自动修图？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这一神奇功能，来自苹果刚刚开源的图片编辑神器「MGIE」。\n",
      "\n",
      "把背景中的人移除\n",
      "\n",
      "在桌子上添加披萨\n",
      "\n",
      "最近一段时间，AI 在图片编辑这一应用上取得了不小的进展。一方面，在 LLM 的基础上，多模态大模型（MLLM）可以自然地将图像视为输入，并提供视觉 感知 响应。另一方面，基于指令的编辑技术可以不依赖于详细描述或区域掩码，而是允许人类下达指令，直接表达如何编辑以及编辑图像的哪个方面。这种方法极具实用性，因为这种引导更符合人类的直觉。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "基于上述技术的启发，苹果提出了 MGIE（MLLM-Guided Image Editing），将 MLLM 用于解决指令引导不足的问题。\n",
      "\n",
      "论文标题：Guiding Instruction-based Image Editing via Multimodal Large Language Models\n",
      "\n",
      "论文链接：https://openreview.net/pdf?id=S1RKWSyZ2Y\n",
      "\n",
      "项目主页：https://mllm-ie.github.io/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如图 2 所示，MGIE 由 MLLM 和扩散模型组成。MLLM 通过学习获得简明的表达指令，并提供明确的视觉相关引导。通过端到端训练，扩散模型会同步更新，并利用预期目标的潜在想象力执行图像编辑。这样，MGIE 就能从固有的视觉推导中获益，并解决模糊的人类指令，从而实现合理的编辑。\n",
      "\n",
      "在人类指令的引导下，MGIE 可进行 Photoshop 风格的修改、全局照片优化和局部对象修改。以下图为例，在没有额外语境的情况下，很难捕捉到「健康」的含义，但 MGIE 可以将「蔬菜配料」与披萨精确地联系起来，并按照人类的期望进行相关编辑。\n",
      "\n",
      "这让我们想起，库克前不久在财报电话会议上表达的「雄心壮志」：「我认为苹果在生成式 AI 方面存在着巨大的机会，但我不想谈更多细节。」他透露的信息包括，苹果正在积极开发生成式 AI 软件功能，且这些功能在 2024 年晚些时候就能向客户提供。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "结合苹果在近段时间发布的一系列生成式 AI 理论研究成果，看来我们期待一下苹果接下来要发布的新 AI 功能了。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "论文细节\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "。对于那些不精确的指令，MGIE 中的 MLLM 会进行学习推导，从而得到简洁的表达指令 ε。为了在语言和视觉模态之间架起桥梁，研究者还在 ε 之后添加了特殊的 token [IMG]，并采用编辑头（edit head） 对它们进行转换。转换后的信息将作为 MLLM 中的潜在视觉想象，引导扩散模型 实现预期的编辑目标。然后，MGIE 能够理解具有视觉 感知 的模糊命令，从而进行合理的图像编辑（架构图如上图 2 所示）。 该研究提出的 MGIE 方法能够通过给定的指令 X 将输入图片 V 编辑为目标图片。对于那些不精确的指令，MGIE 中的 MLLM 会进行学习推导，从而得到简洁的表达指令 ε。为了在语言和视觉模态之间架起桥梁，研究者还在 ε 之后添加了特殊的 token [IMG]，并采用编辑头（edit head）对它们进行转换。转换后的信息将作为 MLLM 中的潜在视觉想象，引导扩散模型实现预期的编辑目标。然后，MGIE 能够理解具有视觉的模糊命令，从而进行合理的图像编辑（架构图如上图 2 所示）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "简洁的表达指令\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "通过特征对齐和指令调整，MLLM 能够跨模态 感知 提供与视觉相关的响应。对于图像编辑，该研究使用提示「what will this image be like if [instruction]」作为图像的语言输入，并导出编辑命令的详细解释。然而，这些解释往往过于冗长、甚至误导了用户意图。为了获得更简洁的描述，该研究应用预训练摘要器让 MLLM 学习生成摘要输出。这一过程可以总结为如下方式：\n",
      "\n",
      "通过潜在想象进行图片编辑\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "将 [IMG] 转化为实际的视觉引导。其中 是一个 序列到序列 模型，它将来自 MLLM 的连续视觉 tokens 映射 到语义上有意义的潜在 U = {u_1, u_2, ..., u_L} 并作为编辑引导： 该研究采用编辑头将 [IMG] 转化为实际的视觉引导。其中是一个模型，它将来自 MLLM 的连续视觉 tokens到语义上有意义的潜在 U = {u_1, u_2, ..., u_L} 并作为编辑引导：\n",
      "\n",
      "，该模型在包含变分自动编码器（VAE）的同时，还能解决潜在空间中的去噪扩散问题。 为了实现通过视觉想象 U 引导图像编辑这一过程，该研究考虑使用扩散模型，该模型在包含变分自动编码器（VAE）的同时，还能解决潜在空间中的去噪扩散问题。\n",
      "\n",
      "转变其模态并引导 合成结果图像。编辑损失 L_edit 用于扩散训练。由于大多数 权重 可以被冻结（MLLM 内的 自注意力 块），因而可以实现 参数 高效的端到端训练。 算法 1 展示了 MGIE 学习过程。MLLM 通过指令损失 L_ins 导出简洁指令 ε。借助 [IMG] 的潜在想象，转变其模态并引导合成结果图像。编辑损失 L_edit 用于扩散训练。由于大多数可以被冻结（MLLM 内的块），因而可以实现高效的端到端训练。\n",
      "\n",
      "实验评估\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "对于输入图片，在相同的指令下，不同方法之间的比较，如第一行的指令是「把白天变成黑夜」：\n",
      "\n",
      "表 1 显示了模型仅在数据集 IPr2Pr 上训练的零样本编辑结果。对于涉及 Photoshop 风格修改的 EVR 和 GIER，编辑结果更接近引导意图（例如，LGIE 在 EVR 上获得了更高的 82.0 CVS）。对于 MA5k 上的全局图片优化，由于相关训练三元组的稀缺，InsPix2Pix 很难处理。LGIE 和 MGIE 可以通过 LLM 的学习提供详细的解释，但 LGIE 仍然局限于其单一的模态。通过访问图像，MGIE 可以得出明确的指令，例如哪些区域应该变亮或哪些对象更加清晰，从而带来显著的性能提升（例如，更高的 66.3 SSIM 和更低的 0.3 拍照距离），在 MagicBrush 上也发现了类似的结果。MGIE 还从精确的视觉想象中获得最佳性能，并修改指定目标作为目标（例如，更高的 82.2 DINO 视觉相似度和更高的 30.4 CTS 全局字幕对齐）。\n",
      "\n",
      "为了研究针对特定目的的基于指令的图像编辑，表 2 对每个数据集上的模型进行了微调。对于 EVR 和 GIER，所有模型在适应 Photoshop 风格的编辑任务后都获得了改进。MGIE 在编辑的各个方面始终优于 LGIE。这也说明了使用表达指令进行学习可以有效地增强图像编辑，而视觉 感知 在获得最大增强的明确引导方面起着至关重要的作用。\n",
      "\n",
      "α_X 和 α_V 之间的权衡。图像编辑有两个目标：操作作为指令的目标和保留作为输入图像的剩余部分。图 3 显示了指令 (α_X) 和输入一致性 (α_V) 之间的权衡曲线。该研究将 α_X 固定为 7.5，α_V 在 [1.0, 2.2] 范围内变化。α_V 越大，编辑结果与输入越相似，但与指令的一致性越差。X 轴计算的是 CLIP 方向相似度，即编辑结果与指令的一致程度；Y 轴是 CLIP 视觉编码器与输入图像的特征相似度。通过具体的表达指令，实验在所有设置中都超越了 InsPix2Pix。此外， MGIE 还能通过明确的视觉相关引导进行学习，从而实现全面提升。无论是要求更高的输入相关性还是编辑相关性，这都支持稳健的改进。\n",
      "\n",
      "消融研究\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "除此以外，研究者还进行了消融实验，考虑了不同的架构 FZ、FT 以及 E2E 在表达指令上的性能 。结果表明，在 FZ、FT、E2E 中，MGIE 持续超过 LGIE。这表明具有关键视觉 感知 的表达指令在所有消融设置中始终具有优势。\n",
      "\n",
      "为什么 MLLM 引导有用？图 5 显示了输入或 ground-truth 目标图像与表达式指令之间的 CLIP-Score 值。输入图像的 CLIP-S 分数越高，说明指令与编辑源相关，而更好地与目标图像保持一致可提供明确、相关的编辑引导。如图所示，MGIE 与输入 / 目标更加一致，这就解释了为什么其表达性指令很有帮助。有了对预期结果的清晰叙述，MGIE 可以在图像编辑方面取得最大的改进。\n",
      "\n",
      "人工评估。除了自动指标外，研究者还进行了人工评估。图 6 显示了生成的表达指令的质量，图 7 对比了 InsPix2Pix、LGIE 和 MGIE 在指令遵循、ground-truth 相关性和整体质量方面的图像编辑结果。\n",
      "\n",
      "推理效率。尽管 MGIE 依靠 MLLM 来推动图像编辑，但它只推出了简明的表达式指令（少于 32 个 token），因此效率与 InsPix2Pix 不相上下。表 4 列出了在英伟达 A100 GPU 上的推理时间成本。对于单个输入，MGIE 可以在 10 秒内完成编辑任务。在数据并行化程度更高的情况下，所需的时间也差不多（当批大小为 8 时，需要 37 秒）。整个过程只需一个 GPU（40GB）即可完成。\n",
      "\n",
      "定性比较。图 8 展示了所有使用数据集的可视化对比，图 9 进一步对比了 LGIE 或 MGIE 的表达指令。\n",
      "\n",
      "在项目主页中，研究者还提供了更多 demo（https://mllm-ie.github.io/）。更多研究细节，可参考原论文。\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2023-10-18-5\n",
      "title= 站上大模型制高点：我们给不输GPT-4的文心大模型4.0，来了一场技术揭秘\n",
      "author= []\n",
      "publish_date= 2023-10-18 00:00:00\n",
      "text= 我们即将进入一个 AI 原生的时代，一个人机交互的新时代。\n",
      "\n",
      "「这是迄今为止最强的文心大模型。它实现了基础模型的全面升级，在理解、生成、逻辑和记忆四大能力上都比文心一言线上版本有了明显提升，综合水平与 GPT-4 相比毫不逊色，」百度创始人、董事长兼 CEO 李彦宏说道。\n",
      "\n",
      "国内的大模型已经冲到了业界最强水平？10 月 17 日，百度世界大会上文心大模型 4.0 的发布引来了一大波关注。\n",
      "\n",
      "在昨天的大会现场，百度展示了一段又一段 demo，文心大模型现在能看懂你的言外之意，比如问它一个问题：「我想回承德买房，能用公积金贷款吗？手续怎么办，我在北京工作。」\n",
      "\n",
      "一段完全口语化的表达，前后乱序，没有明确表述，但 AI 也能理解说话人的潜台词：公积金是北京的，户口可能在承德。文心一言理解上下文之后准确针对问题给出了回答。\n",
      "\n",
      "当然还有先进的多模态方向：给文心大模型一个新车型的图片，再加上几句话的提示（Prompt），它就可以很快生成完整的企划文案图片，并把素材结合成海报。再提示几下，文心就能结合官网信息和已有内容生成一段宣传视频，其中还有数字人在进行讲解。\n",
      "\n",
      "从零开始到输出海报和视频，整个过程不到三分钟。\n",
      "\n",
      "「人们常说不写作业母慈子孝，一写作业鸡飞狗跳。文心一言能不能帮助家长解决辅导功课的问题？」李彦宏说道。\n",
      "\n",
      "给出一道数学题，文心的解答过程非常详细。AI 还能够进一步解释这个问题中涉及到的各个知识点。\n",
      "\n",
      "衡量 AI 智能程度的另一个指标在于长期记忆能力，如果大模型生成的内容前后逻辑不一致，那就不存在可用性了。在现场，李彦宏让文心一言生成一部武侠小说的大纲，再在其中细化情节，加入几个人物，改变冲突的戏剧要素…… 问题来了，经历了多轮对话，它还记得自己最开始给女主角设定的特殊能力吗？\n",
      "\n",
      "完全没有问题。\n",
      "\n",
      "面向全社会开放才一个多月，文心大模型的综合水平看来又有了大幅进化，不过在主题演讲上的那些毕竟是「命题作文」，真正用起来会不会是一回事呢？\n",
      "\n",
      "其实想要用上并不难：昨天大会的一开场，百度就宣布文心大模型 4.0 开启邀请测试，现场观众都有了测试权限，在网站和 APP 上都可以体验。我们则是提前获得了评测资格，尝试了一下新版本。\n",
      "\n",
      "文心大模型 4.0 正面对比 GPT-4\n",
      "\n",
      "在文心一言的网站上，现在已经出现了文心大模型 4.0 的标签，表面看起来和 3.5 版没有太大区别：\n",
      "\n",
      "在这里我们要引入一点前置知识：上个版本文心大模型 3.5 已经有了插件（现有 8 种）、多模态理解、生成等能力，通过知识点增强技术实现了对世界知识的熟练掌握。因此，既然说 4.0 版是「迄今为止最强大模型」，我们就不能再用以前过于简单的问题来考验它了。\n",
      "\n",
      "先看理解能力，这道「中文十级题」目测连网友都会翻车，文心大模型 4.0 的回答简洁明了：\n",
      "\n",
      "换业界标杆 GPT-4 来回答，它理解并解释了其中幽默的意味，但表示无法确定小明最后买的是几等座：\n",
      "\n",
      "下面这段话是在一档直播节目上出现的，那时人们评价道：全中国没人听得懂白岩松在说什么。\n",
      "\n",
      "两个大模型都认为说话人想表达的是：人们都喜爱足球这项运动，不应该因为一小部分人的不喜欢而影响到这种喜爱。不过作为人类，还是得说一句 AI 没有理解「想说声喜爱很难」这种感情。\n",
      "\n",
      "看起来理解问题的水准上，两种模型水平相近，文心大模型在一小部分问题上有点优势。\n",
      "\n",
      "再看逻辑推理能力，输入一个高考试卷中的物理选择题，文心大模型 4.0 和 GPT-4 都给出了正确的回答：\n",
      "\n",
      "看起来文心能给出的答案更详细一些，另外还显示了几个进一步解释概念的引导选项，似乎它对做题进行了专门的优化？\n",
      "\n",
      "我们继续问了很多高考的数学题目，结果各有对错，也有些是都答不上来的。总体来看文心 4.0 和 GPT-4 的水平相近。\n",
      "\n",
      "还有多模态生成，我们直接用同样的指令让两个大模型生成一段视频，文心一言调用「一镜留影」插件，直接输出了结果：\n",
      "\n",
      "GPT-4 则是调用 CapCut（字节的剪映）插件生成视频内容。需要注意的是，它提示要想生成视频，就必须要与你进行多轮对话，逐步确定好视频脚本（英文的）、屏幕比例等等：\n",
      "\n",
      "在不断的测试中我们还能看出，如果你 Prompt 得越仔细，说 AI 话的格式越规整，GPT-4 的表现就相对越好，不过最终也并没有产生决定性的差距。调戏大模型，现在已经越来越像一门学问了。\n",
      "\n",
      "为了测试四大能力中的长期记忆能力，我们让文心大模型 4.0 阅读一篇贴吧的帖子：在崩铁更新了 1.4 版本之后，有人从自己专业的角度对剧情进行了一长段吐槽，那么这评价合理吗？\n",
      "\n",
      "文心认为游戏剧情不需要完全按照现实世界的逻辑来展开。我不是很认同，我就是想要符合现实逻辑的剧情：\n",
      "\n",
      "能不能再跌宕起伏一点？\n",
      "\n",
      "再尝试替换其中的一个人物：\n",
      "\n",
      "看起来，文心大模型 4.0 可以在保持原始知识的情况下，与人在不断对话的过程中生成、提炼出你想要的内容。\n",
      "\n",
      "还有一些我们经常会用得到的功能。在 ChatGPT 出现后，越来越多的人开始尝试使用大模型帮忙来润色论文，据说 AI 写论文看起来很有功底，一般人还真比不上。我们用一段著名的发言试一下：\n",
      "\n",
      "文心大模型 4.0 把它改写成了这样：\n",
      "\n",
      "与之相对的是，GPT-4 更多地使用了原文的信息：\n",
      "\n",
      "不过在更多测试中，GPT-4 生成的内容偶尔会出现夹杂英文的现象。\n",
      "\n",
      "另外，文心一言目前为保证获取实时信息，默认接入了百度搜索插件，也在理解网络新趋势的时候能帮得上忙。比如，我们最近都在反思自己有没有努力工作：\n",
      "\n",
      "相比之下，GPT-4 给出了似乎是基于大模型幻觉的回答。\n",
      "\n",
      "如果多点一步选择使用 Bing 联网版的 GPT-4 则可以得到正确回复，不过再次出现了语言问题，偶尔会获得全英文的回答。\n",
      "\n",
      "看起来，文心大模型 4.0 在四大核心能力上的提升的确明显，和 GPT-4 比毫不逊色的说法也并不是夸张，特别是在中文领域里，水平是经得起考验的。\n",
      "\n",
      "核心技术揭秘\n",
      "\n",
      "能做得到业内领先，百度实现了哪些技术进步？在昨天会上，百度 CTO 王海峰解读了文心大模型 4.0 的关键技术和最新进展。\n",
      "\n",
      "「相比 3.5 版本，文心大模型 4.0 的理解、生成、逻辑、记忆四大能力都有显著提升，」王海峰说道。「其中理解和生成能力的提升幅度相近。而逻辑和记忆能力的提升则更大。逻辑的提升幅度达到理解的近三倍，记忆的提升幅度也达到了理解的两倍多。」这些提升都会给用户带来帮助。\n",
      "\n",
      "这些改进的速度很快 —— 其实文心大模型 4.0 在 9 月初就达到了上线标准，开始了小流量测试。过去的一个多月里经过不断调优，它的生成效果又提升了近 30%。\n",
      "\n",
      "基础模型能力的增长体现在应用上，就转化成了生产效率的提升。比如在各家大厂都说在用的智能代码助手上，百度基于文心大模型的 Comate 在内部应用效果不错，整体的代码采纳率现在是 40%，高频用户的代码采纳率达到 60%。现在百度每天新增的代码中，有 20% 是由大模型生成的，这个比例还在不断升高。\n",
      "\n",
      "这些提升又是靠什么做到的？总的来说，百度基于高效率算力、自研框架、更好的数据处理机制，再结合算法与调优，这才训练出了规模更大、效果更好的文心大模型 4.0。\n",
      "\n",
      "今年 3 月正式发布的文心一言，其背后基于文心大模型 3.0，这是一个有知识增强的大语言模型，它从数万亿数据和数千亿知识中融合学习，又使用了有监督精调、人类反馈强化学习、提示等技术，具备知识增强、检索增强和对话增强的优势。\n",
      "\n",
      "5 月份发布的文心大模型 3.5 则在基础模型、精调技术、知识点增强、逻辑推理、插件机制等方面进行了改进，取得了生成效果和效率的提升。\n",
      "\n",
      "文心大模型 4.0 以它们为基础，继续在多个关键技术向上突破。\n",
      "\n",
      "具体来说，百度：\n",
      "\n",
      "在万卡算力上基于飞桨平台，通过集群基础设施和调度系统、飞桨框架的软硬协同优化，支持了大模型的稳定高效训练。\n",
      "\n",
      "通过建设多维数据体系，形成了从数据挖掘、分析、合成、标注到评估闭环，充分提高数据的利用效率，大幅提升模型效果。\n",
      "\n",
      "基于有监督精调、偏好学习、强化学习等技术进行多阶段对齐，保证了模型能够更好地与人类的判断和选择对齐。\n",
      "\n",
      "利用可再生训练技术通过增量式的参数调优，有效节省了训练资源和时间，加快了模型迭代速度。\n",
      "\n",
      "基于这一系列的提升，自三月以来文心大模型的训练效率已累计提升 3.6 倍；训练稳定性方面，周均的训练有效率已超过 98%。\n",
      "\n",
      "另外在更高层面上还有一些改进。\n",
      "\n",
      "文心大模型 4.0 实现了输入和输出两阶段的知识点增强，一方面对用户输入的问题进行理解，拆解出所需的知识点，然后在搜索引擎、知识图谱、数据库中查找准确知识，再把这些知识组装进 Prompt 送入大模型，提升了准确率和效率。另一方面又对大模型的输出进行「反思」，从生成结果中拆解出知识点，再用搜索引擎、知识图谱、数据库，以及大模型本身进行确认，对有差错的内容进行修正。\n",
      "\n",
      "给大模型再加一层自动化的 AutoGPT 被认为是大模型的重要发展方向，百度同样构建了文心的智能体机制。人的认知系统可划分为两个部分：系统 1，反应很快，但容易出错；系统 2，反应慢，但更理性、更准确。在基础大模型之上百度进一步研制了系统 2，包括理解、规划、反思和进化，能够做到可靠执行，自我进化，并一定程度上将思考过程白盒化，从而让机器像人一样思考和行动，自主的完成复杂任务，并能够在环境中持续学习实现自主进化。\n",
      "\n",
      "接下来，文心一言团队还会继续加班加点，持续提升大模型的能力。\n",
      "\n",
      "目前，文心大模型的用户量增长很快。王海峰公布了一组数字：自 8 月 31 日文心一言面向全社会开放至今，仅用 40 多天的时间，文心一言的用户规模已经达到 4500 万，同时覆盖了 5.4 万开发者，4300 个场景，825 个应用，与之匹配的插件也超过了 500 个。\n",
      "\n",
      "百度：做国内第一个 AI 原生化公司\n",
      "\n",
      "当然，前面展示的文心一言只是生成式 AI 应用的一小部分。\n",
      "\n",
      "大模型理解、生成、逻辑、记忆的四大核心能力突破，是催生 AI 原生应用的必要条件，带来了全新的想象和创新空间。\n",
      "\n",
      "李彦宏表示，百度要做第一个把所有产品进行重构的公司。在世界大会上，百度发布了多款 AI 原生的应用，来自搜索、地图、文库、网盘等业务线的十余个应用产品全部亮相。\n",
      "\n",
      "百度搜索是大模型落地的第一步，「新搜索」是全新的 AI 互动式搜索，它实现了三大重要提升：极致满足、推荐激发、多轮交互。当你在搜索框里输入问题，它不再是单纯的输出链接，而是生成完整的答案，并附带易于理解的图表。\n",
      "\n",
      "大模型加持的生产力工具也在变得更聪明，分析师现在可以通过大模型工具可以把十几天才能完成的任务缩短到几分钟来完成，参与在线会议的人可以从冗长的对话内容里快速总结出重要信息，出差时 AI 也会自动帮你安排行程：\n",
      "\n",
      "在我们每天都会用的百度地图上，最新上线的 V19 版本基于文心大模型进行了重构，其中的「AI 向导」具备多轮自然语言交互能力，用说话的方式就能唤醒菜单里被折叠的上千种能力，也可以理解人们不是具体地点的需求，并找到最优解，当好一个向导。\n",
      "\n",
      "如果把眼光放远到更多行业，百度正在大力推动数字技术与实体经济的深度融合，其大模型技术已应用在制造、能源、电力、化工、交通等实体产业中。在千帆大模型平台上，现在已有超过 1.7 万企业开发了产业模型和解决方案，覆盖了各行业的近 500 个场景。\n",
      "\n",
      "最近一段时间，AI 领域技术的军备竞赛让我们对技术突破越来越熟视无睹。有时候甚至会忘记距离 ChatGPT 正式发布，现在才过去十个多月的时间。在这段时间里，通用的生成式 AI 已经从遥不可及的愿景，变成了人人在玩的聊天机器人，又蜕变成为了众多行业效率提升的基础。\n",
      "\n",
      "而在未来，不论时间的长短，AI 原生的智能化注定要改变所有人的生活和工作方式。\n",
      "\n",
      "可喜的是，在这个过程中，国内公司已经拿到了入场门票。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2023-10-18-4\n",
      "title= 把LLM视作操作系统，它就拥有了无限「虚拟」上下文，伯克利新作已揽1.7k star\n",
      "author= []\n",
      "publish_date= 2023-10-18 00:00:00\n",
      "text= 当前，让大语言模型拥有更强的上下文处理能力是业界非常看重的热点主题之一。本文中，加州大学伯克利分校的研究者将 LLM 与操作系统巧妙地联系在了一起，在扩展上下文长度领域带来了新的进展。\n",
      "\n",
      "近年来，大语言模型（LLM）及其底层的 transformer 架构已经成为了对话式 AI 的基石，并催生了广泛的消费级和企业应用程序。尽管有了长足的进步，但 LLM 使用的固定长度的上下文窗口极大地限制了对长对话或长文档推理的适用性。即使是使用最广泛的开源 LLM，它们的最大输入长度只允许支持几十条消息回复或短文档推理。\n",
      "\n",
      "与此同时，受限于 transformer 架构的自注意力机构，简单地扩展 transformer 的上下文长度也会导致计算时间和内存成本成倍增加，这就使得全新的长上下文架构成为紧迫的研究课题。\n",
      "\n",
      "不过，即使我们能够克服上下文缩放的计算挑战，但最近的研究却表明，长上下文模型很难有效地利用额外的上下文。\n",
      "\n",
      "这如何解决呢？考虑到训练 SOTA LLM 所需的大量资源以及上下文缩放明显的回报递减，我们迫切需要支持长上下文的替代技术。加州大学伯克利分校的研究者在这方面有了新的进展。\n",
      "\n",
      "在本文中，研究者探究了如何在继续使用固定上下文模型的同时，提供无限上下文的幻觉（illusion）。他们的方法借鉴了虚拟内存分页的思路，使得应用程序能够处理远超出可用内存的数据集。\n",
      "\n",
      "基于该思路，研究者利用 LLM 智能体函数调用能力的最新进展，设计出了一个受 OS 启发、用于虚拟上下文管理的 LLM 系统 ——MemGPT。\n",
      "\n",
      "论文主页：https://memgpt.ai/\n",
      "\n",
      "arXiv 地址：https://arxiv.org/pdf/2310.08560.pdf\n",
      "\n",
      "项目已经开源，在 GitHub 上已经斩获了 1.7k 的 star 量。\n",
      "\n",
      "GitHub 地址：https://github.com/cpacker/MemGPT\n",
      "\n",
      "方法概览\n",
      "\n",
      "该研究从传统操作系统的分层内存管理中汲取灵感，在上下文窗口（类似于操作系统中的「主存（main memory）」）和外部存储之间有效地「分页」进出信息。MemGPT 则负责管理内存、LLM 处理模块和用户之间的控制流。这种设计允许在单个任务期间反复进行上下文修改，从而允许智能体更有效地利用其有限的上下文窗口。\n",
      "\n",
      "MemGPT 将上下文窗口视为受限内存资源，并为 LLM 设计类似于传统操作系统中分层内存（Patterson et al., 1988）的层次结构。为了提供更长的上下文长度，该研究允许 LLM 通过「LLM OS」——MemGPT，来管理放置在其上下文窗口中的内容。MemGPT 使 LLM 能够检索上下文中丢失的相关历史数据，类似于操作系统中的页面错误。此外，智能体可以迭代地修改单个任务上下文窗口中的内容，就像进程可以重复访问虚拟内存一样。\n",
      "\n",
      "MemGPT 能够让 LLM 在上下文窗口有限的情况下处理无界上下文，MemGPT 的组件如下图 1 所示。\n",
      "\n",
      "MemGPT 通过函数调用协调主上下文（上下文窗口中的内容）和外部上下文之间的数据移动，MemGPT 根据当前上下文自主更新和检索。\n",
      "\n",
      "值得注意的是，上下文窗口需要用 warning token 来标识其限制，如下图 3 所示：\n",
      "\n",
      "实验及结果\n",
      "\n",
      "在实验部分，研究者在两个长上下文域中来评估 MemGPT，分别是对话式智能体和文档处理。其中对于对话式智能体，他们扩展了现有的多会话聊天数据集（Xu et al. (2021)），并引入了两个新的对话任务以评估智能体在长对话中保留知识的能力。对于文档分析，他们根据 Liu et al. (2023a) 提出的任务对 MemGPT 进行基准测试，包括对长文档的问答和键值检索。\n",
      "\n",
      "用于对话智能体的 MemGPT\n",
      "\n",
      "当与用户对话时，智能体必须满足以下两个关键标准。\n",
      "\n",
      "一是一致性，即智能体应保持对话的连贯性，提供的新事实、引用和事件应与用户、智能体之前的陈述保持一致。\n",
      "\n",
      "二是参与度，即智能体应该利用用户的长期知识来个性化响应。参考之前的对话可以使对话更加自然和引人入胜。\n",
      "\n",
      "因此，研究者根据这两个标准对 MemGPT 进行评估：\n",
      "\n",
      "MemGPT 是否可以利用其记忆来提高对话一致性？能否记住过去交互中的相关事实、引用、事件以保持连贯性？\n",
      "\n",
      "MemGPT 是否可以利用记忆生成更有吸引力的对话？是否自发地合并远程用户信息以个性化信息？\n",
      "\n",
      "关于使用到的数据集，研究者在 Xu et al. (2021) 提出的多会话聊天（MSC）上对 MemGPT 和固定上下文的基线模型展开评估对比。\n",
      "\n",
      "首先来一致性评估。研究者引入了一个基于 MSC 数据集的深层记忆检索（deep memory retrieval, DMR）任务，旨在测试对话智能体的一致性。在 DMR 中，用户向对话智能体提出一个问题，并且该问题明确引用先前的对话，预期答案范围会非常窄。具体可以参加下图 5 示例。\n",
      "\n",
      "MemGPT 利用内存来保持一致性。下表 2 显示了 MemGPT 与固定记忆基线模型的性能对比，包括 GPT-3.5 和 GPT-4。\n",
      "\n",
      "可以看到，MemGPT 在 LLM 判断准确度和 ROUGE-L 分数方面显著优于 GPT-3.5 和 GPT-4。MemGPT 能够利用回想记忆（Recall Memory）查询过去的对话历史，进而回答 DMR 问题，而不是依赖递归摘要来扩展上下文。\n",
      "\n",
      "然后在「对话开场白」任务中，研究者评估智能体从先前对话积累的知识中提取引人入胜的消息并传递给用户的能力。\n",
      "\n",
      "研究者在下表 3 中展示了 MemGPT 开场白的 CSIM 分数。结果表明，MemGPT 能够制作引人入胜的开场白，其表现可以媲美甚至超越人类手写的开场白。此外还观察到 MemGPT 倾向于制作比人类基线更长且涵盖更多角色信息的开场白。下图 6 为示例。\n",
      "\n",
      "用于文档分析的 MemGPT\n",
      "\n",
      "为了评估 MemGPT 分析文档的能力，研究者对 MemGPT 以及在 Liu et al. (2023a) 检索器 - 阅读器文档 QA 任务上的固定上下文基线模型进行了基准测试。\n",
      "\n",
      "结果显示，MemGPT 能够通过查询档案存储有效地对检索器进行多次调用，从而可以扩展到更大的有效上下文长度。MemGPT 主动从档案存储中检索文档并且可以迭代地分页浏览结果，因而其可用的文档总数不再受到适用 LLM 处理器上下文窗口的文档数量的限制。\n",
      "\n",
      "由于基于嵌入的相似性搜索的局限性，文档 QA 任务对所有方法都构成了极大的挑战。研究者观察到，MemGPT 会在检索器数据库耗尽之前停止对检索器结果进行分页操作。\n",
      "\n",
      "此外 MemGPT 更复杂操作所创建的检索文档容量也存在权衡，如下图 7 所示，其平均准确度低于 GPT-4（高于 GPT-3.5），但可以轻松地扩展到更大的文档。\n",
      "\n",
      "研究者还引入了一项基于合成键值检索的新任务，即嵌套键值检索（Nested Key-Value Retrieval），用以演示 MemGPT 如何对来自多个数据源的信息进行整理。\n",
      "\n",
      "从结果来看，虽然 GPT-3.5 和 GPT-4 在原始键值任务上表现出了良好性能，但在嵌套键值检索任务中表现不佳。而 MemGPT 不受嵌套层数的影响，并能够通过函数查询重复访问存储在主内存中的键值对，来执行嵌套查找。\n",
      "\n",
      "MemGPT 在嵌套键值检索任务上的性能，展示了其利用多个查询的组合执行多条查找的能力。\n",
      "\n",
      "更多技术细节和实验结果请参阅原论文。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2023-10-18-3\n",
      "title= 未来大模型顶会？陈丹琦等人组织首届COLM，为语言建模研究提供新平台\n",
      "author= []\n",
      "publish_date= 2023-10-18 00:00:00\n",
      "text= 获得了众多学术大牛的支持。\n",
      "\n",
      "随着 ChatGPT 的爆火，NLP、大模型领域已经是个「一天不看新闻就会落后」的地方。\n",
      "\n",
      "虽然如今 AI 领域的顶级学术会议已经不少，但似乎也无法满足大家投论文的速度了。\n",
      "\n",
      "现在，一批知名的青年学者们，组织了一个名为 COLM（Conference on Language Modeling）的新会议。\n",
      "\n",
      "COLM 是一个专注于语言建模研究的学术场所。在会议介绍中，组织这门十分明确地讲述了他们的目的是创建一个具有不同科学专业知识的研究人员社区，专注于理解、改进和评论语言模型技术的发展。这不仅是学术界的一次创新尝试，也是搭起了语言模型交流互鉴的新桥梁，进一步促进其探索和合作。\n",
      "\n",
      "会议连接 https://colmweb.org/\n",
      "\n",
      "组织者介绍\n",
      "\n",
      "该会议的组织者们是 NLP 头部科学家们，在语言建模研究有着相当成果的。他们其中既有来自业界的研究人员，也有来自学术界的研究人员。组织者共有七人，分别是 Yejin Choi、Denny Zhou、Dipanjan Das、陈丹琦、Yoav Artzi、Angela Fan、Alexander Rush。\n",
      "\n",
      "而组织者中的 Denny Zhou、陈丹琦、Angela Fan 等人也是我们熟知的 AI 华人学者。\n",
      "\n",
      "陈丹琦\n",
      "\n",
      "\n",
      "\n",
      "陈丹琦是普林斯顿大学计算机科学的助理教授，也是普林斯顿 NLP 小组的共同负责人。本科就读于清华大学，后获得斯坦福大学计算机科学博士学位。陈丹琦凭借其在自然语言处理领域取得的一系列成果，荣膺 2019 年《麻省理工科技评论》「35 岁以下科技创新 35 人」中国区得主，获奖时年龄 29 岁。就在前些天，陈丹琦团队提出 LLM-Shearing 大模型剪枝法。\n",
      "\n",
      "Denny Zhou\n",
      "\n",
      "Denny Zhou 是 Google DeepMind 的首席科学家 / 研究主管，他是推理团队的创立者和现任负责人。主要研究兴趣在于构建和教导大语言模型实现类人的推理能力。他领导的团队已经开发了思维链提示、自洽性解码、最少到最多提示、指令调优（FLAN2）、LLM 自我调试等大语言模型的各种涌现属性。Denny Zhou 曾获得 2022 年谷歌研究技术影响力奖（Google Research Tech Impact Award）。\n",
      "\n",
      "Angela Fan\n",
      "\n",
      "Angela Fan 是 Meta AI Research Paris 的研究科学家，主要研究机器翻译。此前她曾在南锡 INRIA 和巴黎 FAIR 攻读博士学位，主要研究文本生成。在此之前，她是一名研究工程师，并在哈佛大学获得了统计学学士学位。\n",
      "\n",
      "会议投稿主题\n",
      "\n",
      "组织者们为本次会议的投稿提供了广泛而全面的主题\n",
      "\n",
      "对齐：微调、指令微调、强化学习（包括人工反馈）、提示调整和上下文对齐\n",
      "\n",
      "数据：预训练数据、对齐数据和通过人工或算法分析、整理和生成的合成数据\n",
      "\n",
      "评估：基准、模拟环境、可扩展监督、评估协议和指标、人工或机器评估\n",
      "\n",
      "社会影响：偏见、公平、滥用、就业、气候变化等\n",
      "\n",
      "安全：安全、隐私、错误信息、对抗性攻击和防御\n",
      "\n",
      "LM 的科学性：扩展规律、基本限制、新兴能力、解密、可解释性、复杂性、训练动态、学习理论\n",
      "\n",
      "高效计算 LM：蒸馏、压缩、量化等\n",
      "\n",
      "大型 LM 的工程设计：不同硬件设置上的分布式训练和推理、训练动态、优化不稳定性\n",
      "\n",
      "LM 的学习算法：学习、元学习、模型混合法、持续学习等\n",
      "\n",
      "LM 的推理算法：解码算法、推理算法、搜索算法、规划算法\n",
      "\n",
      "人类心智、大脑、哲学、法律与 LM：从认知科学、神经科学、语言学、心理语言学、哲学或法律角度看待 LM\n",
      "\n",
      "面向所有人的 LM：多语言、低资源语言、方言、多元文化、价值多元化\n",
      "\n",
      "LM 与世界：事实性、检索增强型 LM、知识模型、常识推理、心智理论、社会规范、语用学和世界模型\n",
      "\n",
      "具身 LM：感知、行动、机器人和多模态\n",
      "\n",
      "LM 与互动：对话、互动学习和多人学习\n",
      "\n",
      "LM 与工具和代码：与工具和 API 的集成、LM 驱动的软件工程\n",
      "\n",
      "关于各种模式和新型应用的 LM：视觉 LM、代码 LM、数学 LM 等，特别鼓励研究较少的模式或应用，如化学、医学、教育、数据库等\n",
      "\n",
      "COLM 发表后，获得了许多支持与期待。\n",
      "\n",
      "与 ICLR 相似，COLM 也是领域中的创新会议，它们都有着知名学者们的倾力支持。除此之外，ICLR 采用的开放评审备受好评，并开公开透明之先河。COLM 投稿将采用双盲审核，并将使用开放评审的方式管理投稿。至于它是否能像 ICLR 一样后来居上，就请拭目以待吧。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2023-10-18-2\n",
      "title= 端侧AI推理，高效部署PyTorch模型：官方新工具开源，Meta已经用上了\n",
      "author= []\n",
      "publish_date= 2023-10-18 00:00:00\n",
      "text= ExecuTorch 是一个端到端的解决方案，可以在移动和边缘设备（包括可穿戴设备、手机等）上实现推理功能。\n",
      "\n",
      "在 2023 年 PyTorch 大会上，一个深受大家关心的推理问题得到了很好的解决，会上宣布了一个用于在边缘和移动设备上实现 AI 推理的解决方案：ExecuTorch，并且还是开源的，而促成这一研究的，正是 Meta AI 与 PyTorch 基金会。\n",
      "\n",
      "ExecuTorch 地址：https://github.com/pytorch/executorch\n",
      "\n",
      "学习文档：https://pytorch.org/executorch/stable/index.html\n",
      "\n",
      "随着 ExecuTorch 的开源，预示着 AI 应用程序在设备上本地运行、而需连接到服务器或云成为可能。我们可以将 ExecuTorch 理解成一个 PyTorch 平台，其能提供基础设施来运行 PyTorch 程序，从 AR/VR 可穿戴设备到标准的 iOS 和 Android 设备的移动部署。\n",
      "\n",
      "ExecuTorch 最大优势是可移植性，能够在移动和嵌入式设备上运行。不仅如此，ExecuTorch 还可以提高开发人员的工作效率。\n",
      "\n",
      "据了解，Meta 已经验证了这项技术，并将其用于最新一代的雷朋智能眼镜，而这款眼镜也是 Meta 最近发布的 Quest 3 VR 头显的一部分。Meta 表示，作为开源 PyTorch 项目的一部分，他们旨在进一步推动该技术的研究，从而迈入在设备上实现 AI 推理的新时代。\n",
      "\n",
      "Facebook 创始人、Meta 董事长兼首席执行官扎克伯格表示：「作为开源 AI 工作的一部分，我们与 PyTorch 基金会及其行业合作伙伴一起开源了 ExecuTorch。这一变化预示着将 PyTorch 引入了手机和可穿戴设备等边缘计算平台。ExecuTorch 使 AI 模型能够直接在设备上运行，而无需连接到服务器。」\n",
      "\n",
      "Meta 软件工程师 Mergen Nachin 指出，「今天的 AI 模型正在从服务器扩展到边缘设备，如移动设备、AR、VR 和 AR 头显、可穿戴设备、嵌入式系统等。ExecuTorch 通过提供端到端的工作流来优化本地程序，从而解决边缘设备遇到的挑战。」\n",
      "\n",
      "ExecuTorch 关键组件\n",
      "\n",
      "ExecuTorch 提供了紧凑的运行时和轻量级操作注册表，以覆盖 PyTorch 模型生态系统，以及在边缘设备上执行 PyTorch 程序的简化路径。此外，ExecuTorch 还附带 SDK 和工具链，为 ML 开发人员提供了更好的用户体验。\n",
      "\n",
      "作为 PyTorch Edge 生态系统的一部分，ExecuTorch 可以有效地将 PyTorch 模型部署到边缘设备。ExecuTorch 的优点包括：\n",
      "\n",
      "可移植性：与各种计算平台兼容，从高端移动手机到高度受限的嵌入式系统和微控制器。\n",
      "\n",
      "提高生产力：开发人员能够使用相同的工具链和 SDK，从而提高生产力。\n",
      "\n",
      "提高性能：由于轻量级运行时和充分利用 CPU、NPU 和 DSP 等硬件功能，为最终用户提供了无缝和高性能的体验。\n",
      "\n",
      "由于 ExecuTorch 严重依赖 PyTorch 相关知识，因而，想要熟练掌握 ExecuTorch，还需提前补充相关知识。官方文档已经提供了入门级教程。例如，在构建 ExecuTorch Android 演示应用程序示例当中，大家可以跟随指导教程，从而熟悉如何使用 ExecuTorch。\n",
      "\n",
      "最后，需要提醒大家的一点是，本次发布的 ExecuTorch 是一个预览版本，在测试和评估中可以使用，但是不建议在生产环境中使用。PyTorch 团队欢迎来自社区的任何反馈、建议和错误报告，以帮助他们改进技术。\n",
      "\n",
      "参考链接：\n",
      "\n",
      "https://venturebeat.com/ai/pytorch-executorch-extends-open-source-ai-for-new-quests-at-the-edge/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-11\n",
      "title= 俄罗斯小哥ChatGPT找女友：聊了5239个女生，现在订婚了\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 有事 AI 它是真上啊。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "「我向一位女生求婚，ChatGPT 已经和她交流了一年。为了走到这一步，AI 已经尝试了和 5239 名女生进行过沟通……」\n",
      "\n",
      "来源：https://twitter.com/biblikz/status/1752335415812501757\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最近几天，社交网络上人们正在轮番向一位俄罗斯小哥送去祝福。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "23 岁的 Aleksandr Zhadan 是一名 AI 开发者，也是社交平台 TenChat 的一名产品经理。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "故事是这样开始的：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GPT 除了搜索之外，还可以在配对后写入。这样在 50 次自动执行中，他可以获得 18 次配对。GPT 在没有 Aleksandr 的干预下根据以下 prompt 与人交流：你是一个男生，第一次和女生说话。你的任务是：邀请她约会，但不是马上。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最开始进展并不顺利，GPT-3 的约会和 DM 游戏能力很弱。Aleksandr 对它进行了改进，加入了记忆、微调和示例。为了找到最相关的女生，他通过 torchvision 在 Tinder 网络版中使用了照片识别，并根据自己的滑动进行训练。这样一来，GPT 几乎总是能正确地选择合适的女生。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "随着使用 ChatGPT 和 GPT-4 API，AI 变得越来越强大了，它们可以执行几十次对话、配对、安排约会。就这样经过一年的 AI 聊天，Aleksandr 找到了自己想要携手一生的 Karina—— 善解人意、开朗活泼、善良、独立。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最终，Aleksandr 向 Karina 求婚了。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "羡煞旁人之余，质疑声也有。有人认为这个故事是「AI 生成的」。\n",
      "\n",
      "图源：https://twitter.com/Darkolorin/status/1753135894750458268\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "也有社区对 Aleksandr 做了一个小研究，认为他很可能只是在炒作，并指出他此前用 ChatGPT 撰写大学毕业论文并获得文凭，所以非常习惯「炒作浪潮」。\n",
      "\n",
      "图源：https://twitter.com/literallydenis/status/1753177433073664236\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "去年 2 月，Alexandr 用 ChatGPT 写论文的事还被媒体报道过。\n",
      "\n",
      "Aleksandr 在俄罗斯国立人文大学进行了现代组织管理的学位论文答辩，在这名学生的故事引起公众广泛关注后，该大学还召集他进行了演讲。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "用 ChatGPT 找女友，需要几步？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "随着这位俄罗斯小哥的「事迹」越来越火，大家伙当然想知道怎么才能像他一样用 ChatGPT 找到人生的另一半。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "今日，一位推特博主详细分享了 Aleksandr 如何一步步找到自己的意中人。\n",
      "\n",
      "图源：https://twitter.com/8teAPi/status/1754535819493405036\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "相比线下相亲，用 AI 找女友还是蛮简单的。Aleksandr Zhadan 将这个过程分为两步：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "第一步是找女孩；\n",
      "\n",
      "第二步是和她聊天。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "找女孩阶段：当 Aleksandr 在 Tinder 上找女孩时，他使用网略爬虫获取图像，最开始 Aleksandr 倾向于那些在 Tinder 上的照片超过两张的女孩。在迭代过程中，Aleksandr 训练了一个图像相似性模型，该模型能够找到与他喜欢的女孩相似的女孩照片。\n",
      "\n",
      "在聊天阶段：GPT-3 会主动开启对话，这个阶段给 GPT-3 的提示语是「你是个男生，第一次和这个女孩说话。你的任务不是立刻、马上要求对方干什么，而是邀请那个女孩来一次约会。」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aleksandr 表示一开始 GPT-3 表现非常糟糕，经常忘记对话，并且由于机器人无法访问 Telegram，因此他失去了一半的潜在约会机会。更糟糕的是，机器人承诺约会时会送鲜花或巧克力…… 而真正线下约会时却没有这个环节，因而被投诉了。\n",
      "\n",
      "第一代约会机器人（ Datebot V1）的战报：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "匹配了 353 个女孩的 Tinder 档案；\n",
      "\n",
      "总共聊了 160 次（约占匹配人数的 45%）；\n",
      "\n",
      "有 12 次约会（占聊天次数的 7.5%）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "面对这一结果，Aleksandr 并没有灰心，和朋友继续升级这个机器人，因此第二代机器人（Datebot V2）出现了，这次，Aleksandr 他们采用：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GPT-4 进行聊天；\n",
      "\n",
      "每个 聊天机器人 都有一个记忆，包含最简单约会问题的背景故事；\n",
      "\n",
      "从 Tinderbot 切换到 Telegrambot 进行消息传递；\n",
      "\n",
      "集成 Google 日历，用于设置日期；\n",
      "\n",
      "对收到的消息进行人工循环验证。\n",
      "\n",
      "第二代机器人的结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GPT-4 幻觉降至零；\n",
      "\n",
      "匹配了 4886 条 Tinder 个人资料；\n",
      "\n",
      "无数约会，Aleksandr 用「多到吓人」来形容。\n",
      "\n",
      "接下来，Aleksandr 有过很多次约会，最多的时候还和 4 个女孩周旋，直到一位名叫 Karina 的女孩出现。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在结识了 Karina 之后，Aleksandr 专门推出了第三代机器人（Datebot V3）：它被设置成只会与 Karina 聊天。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "给 Datebot V3 的提示是「与 Karina 保持良好的关系，告诉我是否有什么负面的事情需要注意，或者是否需要回答问题。」\n",
      "\n",
      "在此期间，Aleksandr 还利用剩下的人脉还建立了一个新的推荐工作的副项目：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aleksandr 在俄罗斯就业门户网站上发现了愿意为推荐员工付费的职位空缺；\n",
      "\n",
      "Aleksandr 将与女孩的对话与潜在的工作相匹配；\n",
      "\n",
      "出售 GPT-4 形成的联系人 / 关系；\n",
      "\n",
      "成功安排 8 名员工并获得报酬。\n",
      "\n",
      "随着关系的深入，Datebot V3 告诉 Aleksandr 应该与 Karina 结婚，机器人不仅提出了求婚建议，还帮忙策划了一场浪漫的求婚行动。\n",
      "\n",
      "最终 他耗费 120 小时打造的机器人帮他找到了女友。他 表示调用 GPT API 花费 1432 美元、餐厅约会花费 200 卢布，但他的副项目帮他赚了 526 卢布，现在 他 已经向女友求婚了。\n",
      "\n",
      "总而言之，这是一个有点肆无忌惮但又温馨的故事。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "虽然 Karina 一直并不知道 ChatGPT 在她与 Aleksandr 的交往中扮演的角色。当他们向婚姻登记处提交申请时，她第一次发现了这件事，不过反应很平静。他们也将于今年 8 月完婚。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考链接：\n",
      "\n",
      "https://www.emergentbehavior.co/p/chatgpt-find-me-a-wife-on-tinder\n",
      "\n",
      "https://www.news18.com/world/chatgpt-love-story-russian-man-deploys-meets-his-match-using-ai-bot-on-tinder-8767366.html\n",
      "\n",
      "https://www.themoscowtimes.com/2023/02/02/russian-student-allowed-to-keep-diploma-for-chatgpt-written-thesis-a80125\n",
      "\n",
      "https://www.cryptoglobe.com/latest/2024/02/chatgpt-said-marry-her-ai-developers-journey-starts-with-tinder-swipes-ends-with-a-ring/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-10\n",
      "title= 徒手搬汽车配件，波士顿动力Atlas再进化：兄弟们，准备进厂了\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 在如今颇为火热的人形机器人赛道，波士顿动力是较早入局的一位选手。\n",
      "\n",
      "过去几年，Atlas 人形机器人的动态跑酷能力已经让全世界的关注，后来我们还看到 Atlas 在模拟建筑工地上搬搬扛扛。Atlas 目前仍然是一个开发平台，尚不能在现实世界中工作，一部分原因是它的液压驱动设计。\n",
      "\n",
      "不过，波士顿动力最新公布的一个演示视频表明了 Atlas 和其他人形机器人一样能够完成高难度的操纵任务，包括在装备适当的情况下操纵重物。\n",
      "\n",
      "在视频中，Atlas 稳稳抓起一个比自己手臂还粗的汽车配件，搬运到目标位置：\n",
      "\n",
      "实际上，Atlas 在很长一段时间都是没有手指的，而是两个黑色的球体。去年初，波士顿动力在它的手臂末端装上了「螃蟹夹」。现在，它的手指又进化成了三根，虽然不如人类五指灵巧、柔软，但也足以牢牢抓起圆形的配件：\n",
      "\n",
      "切换第一视角看下，是这样的：\n",
      "\n",
      "继续搬运下一个，在去到新目标所在位置的过程中，Atlas 险些摔倒，但最终还是稳住了：\n",
      "\n",
      "看得出来，这配件不是一般的沉重，Atlas 拿起的过程也有些吃力：\n",
      "\n",
      "【关注 机器之心 视频号，第一时间看到有趣的 AI 内容】\n",
      "\n",
      "一直以来，波士顿动力公司（Boston Dynamics）因其在 机器人技术 方面的突破性创新而闻名于世，其中包括 Atlas 双足人形机器人，它能跑能跳，做出各种惊人的动作。\n",
      "\n",
      "不过，Atlas 并不是唯一「准备进厂」的人形机器人。\n",
      "\n",
      "随着人形机器人竞赛的升温，来自 Agility Robotics、Apptronik、Figure 这些公司的人形机器人，同样已经接近在现实世界中找工作的水准了。这些公司的估值也水涨船高，比如据知情人士透露，微软与 OpenAI 正在洽谈参与 Figure 的新一轮融资，其中微软可能将投资约 9500 万美元，OpenAI 将投资 500 万美元。\n",
      "\n",
      "Figure 首款人形机器人。\n",
      "\n",
      "人形机器人的技术和落地进展，总是备受瞩目。虽然一度有人质疑：「把它做成人形机器人有什么意义？任何机械臂都能以 10 倍的速度和理想的精度完成这项任务，这看起来只是一种廉价的炒作。」\n",
      "\n",
      "但至少对于波士顿动力，在公司亏本运营的同时还能对人形机器人投入十多年的研发，显然不是为了炒作。\n",
      "\n",
      "长远来看，人形机器人因其占地面积小、动作灵活的特性，会适用于更多的应用场景。\n",
      "\n",
      "Figure 创始人兼首席执行官 Brett Adcock 表示：「几十年来，单一用途的 机器人技术 的商业市场趋于饱和，但通用 机器人技术 的潜力却完全没有被挖掘出来。」Figure 的人形机器人同样准备好了进厂，第一份工作就在 宝马 位于美国南卡罗来纳州的斯帕坦堡工厂。\n",
      "\n",
      "特斯拉也在努力打造双足人形机器人并将其商业化，该机器人被命名为「擎天柱」（Optimus），且已更新到了 Optimus Gen 2 。据特斯拉初步声明，Optimus 的售价应在 2 万美元左右。\n",
      "\n",
      "先前的一段视频已经展示了 Optimus 独立叠衬衫的过程：\n",
      "\n",
      "在最新公布的视频中，「擎天柱」已经实现了在无人帮助的情况下自主行走，步伐轻盈，机械臂还能随着步伐摆动（尽管动作缓慢）：\n",
      "\n",
      "假设人形机器人真能取代人类完成工作，可能会导致一些行业就业率下降甚至消失，但毫无疑问，我们仍然期待这一天尽早到来。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-9\n",
      "title= 胡渊鸣创业公司Meshy产品升级：文本转3D，25秒就能出预览\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= Meshy-2 的文本转 3D、图像转 3D、文本到纹理效果均有所提升。\n",
      "\n",
      "刚刚，胡渊鸣创业公司 Meshy 官宣了他们的第二代产品 ——Meshy-2。\n",
      "\n",
      "Meshy 是一款 3D 内容生成工具，只需一分钟，即可使用 Al 生成 3D 内容（模型）。公司联合创始人兼 CEO 胡渊鸣是 计算机图形 学知名学者，毕业于 清华大学 姚班，是 MIT 博士，也是「太极」（TaiChi）编程语言作者。\n",
      "\n",
      "具体来说，Meshy 提供三种很容易上手的使用方式，包括文本转 3D（输入文字 —— 输出 3D 模型）、图像转 3D（提供图片 —— 生成 3D 模型）以及从文本到纹理（透过简单文本描述就能为 3D 内容添加纹理）。在 Meshy-1 中，我们已经看到了一些精彩的演示（见以下视频）。\n",
      "\n",
      "【关注 机器之心 视频号，第一时间看到有趣的 AI 内容】\n",
      "\n",
      "时隔三个月，Meshy-2 就问世了。 新版本的升级之处如下图所示： 接下来我们看一下具体内容。\n",
      "\n",
      "文本转 3D\n",
      "\n",
      "胡渊鸣在博客中表示，在过去的几个月里，他们一直专注于文本转 3D。网格和纹理对于 3D 对象来说是必不可少的，Meshy-2 能生成结构更好的网格和丰富的几何细节。同时，纹理质量也更加细腻。\n",
      "\n",
      "Prompt: prehistoric winter boots with wool, realistic, 4K, high quality.\n",
      "\n",
      "Prompt: royal armor set, gold, iron, highly detailed, medieval, knight armor, leather.\n",
      "\n",
      "Meshy-2 提供了 4 种文本转 3D 的风格：写实、卡通、低多边形（Low Poly）和体素，希望可以激发新的创作方向。\n",
      "\n",
      "效率方面，Meshy-2 的文本转 3D 速度更快了：25 秒就可以出预览，5 分钟内就能出精细结果。\n",
      "\n",
      "此外，他们还推出了一款用户友好的网格编辑器，具有多边形计数控制（polycount control）和四边形网格转换系统。这个方便的工具旨在为 3D 生成提供更多的控制和灵活性。\n",
      "\n",
      "文本到纹理\n",
      "\n",
      "在 Meshy-2 中，文本到纹理的功能也得到了改善，以获得更高的清晰度，使得纹理尽可能逼真和清晰。同时，它的运行速度是其前身的两倍。\n",
      "\n",
      "图像转 3D\n",
      "\n",
      "新的图像转 3D 功能可以在 2 分钟内产生更高质量的结果。\n",
      "\n",
      "从 Discord 到网页应用\n",
      "\n",
      "在 Meshy-1 推出时，大家可以加入 Discord 体验这款新工具。但胡渊鸣表示，在 Meshy-2 推出后，他们会将重点从 Discord 转移到网页应用程序，因为大多数用户已经在使用新的网页应用程序。与此同时，他们将逐步关闭 Discord 上的 3D 生成服务。\n",
      "\n",
      "目前，Meshy-2 的网页应用是免费的，不过想升级到 Pro 或 Max 就需要付费了（打折促销码：MESHY2GO）。\n",
      "\n",
      "感兴趣的读者可以去尝试一下：https://app.meshy.ai/zh/login \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-8\n",
      "title= 通义千问再开源，Qwen1.5带来六种体量模型，性能超越GPT3.5\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 超越 Claude、GPT-3.5，提升了多语言支持能力。\n",
      "\n",
      "赶在春节前，通义千问大模型（Qwen）的 1.5 版上线了。今天上午，新版本的消息引发了 AI 社区关注。\n",
      "\n",
      "新版大模型包括六个型号尺寸：0.5B、1.8B、4B、7B、14B 和 72B，其中最强版本的性能超越了 GPT 3.5、Mistral-Medium，包括 Base 模型和 Chat 模型，且有多语言支持。\n",
      "\n",
      "阿里通义千问团队表示，相关技术也已经上线到了通义千问官网和通义千问 App。\n",
      "\n",
      "除此以外，今天 Qwen 1.5 的发布还有如下一些重点：\n",
      "\n",
      "支持 32K 上下文长度；\n",
      "\n",
      "开放了 Base + Chat 模型的 checkpoint；\n",
      "\n",
      "可与 Transformers 一起本地运行；\n",
      "\n",
      "同时发布了 GPTQ Int-4 / Int8、AWQ 和 GGUF 权重 。\n",
      "\n",
      "借助更先进的大模型作为评委，通义千问团队在两个广泛使用的 基准 MT-Bench 和 Alpaca-Eval 上对 Qwen1.5 进行了初步评估，评估结果如下：\n",
      "\n",
      "尽管落后于 GPT-4-Turbo，但最大版本的 Qwen1.5 模型 Qwen1.5-72B-Chat 在 MT-Bench 和 Alpaca-Eval v2 上都表现出了可观的效果，性能超过 Claude-2.1、GPT-3.5-Turbo-0613、Mixtral-8x7b-instruct 和 TULU 2 DPO 70B，与最近热门的新模型 Mistral Medium 不相上下。\n",
      "\n",
      "此外通义千问团队表示，虽然大模型判断的评分似乎与回答的长度有关，但人类观察结果表明 Qwen1.5 并没有因为产生过长的回答来影响评分。AlpacaEval 2.0 上 Qwen1.5-Chat 的平均长度为 1618，与 GPT-4 的长度一致，比 GPT-4-Turbo 短。\n",
      "\n",
      "通义千问的开发者表示，最近几个月，他们一直在专注探索如何构建一个真正「卓越」的模型，并在此过程中不断提升开发者的使用体验。\n",
      "\n",
      "相较于以往版本，本次更新着重提升了 Chat 模型与人类偏好的对齐程度，并且显著增强了模型的多语言处理能力。在序列长度方面，所有规模模型均已实现 32768 个 tokens 的上下文长度范围支持。同时，预训练 Base 模型的质量也有关键优化，有望在微调过程中为人们带来更佳体验。\n",
      "\n",
      "基础能力\n",
      "\n",
      "关于模型基础能力的评测，通义千问团队在 MMLU（5-shot）、C-Eval、 Humane val、GS8K、BBH 等 基准 数据集上对 Qwen1.5 进行了评估。\n",
      "\n",
      "在不同模型尺寸下，Qwen1.5 都在评估 基准 中表现出强大的性能，72B 的版本在所有 基准 测试中都超越了 Llama2-70B，展示了其在语言理解、推理和数学方面的能力。\n",
      "\n",
      "最近一段时间，小型模型的构建是业内热点之一，通义千问团队将模型 参数 小于 70 亿的 Qwen1.5 模型与社区中重要的小型模型进行了比较：\n",
      "\n",
      "在 参数 规模低于 70 亿的范围内 Qwen1.5 与业界领先的小型模型相比具有很强的竞争力。\n",
      "\n",
      "多语言能力\n",
      "\n",
      "在来自欧洲、东亚和东南亚的 12 种不同语言上，通义千问团队评估了 Base 模型的多语言能力。从开源社区的公开数据集中，阿里研究者构建了如下表所示的评测集合，共涵盖四个不同的维度：考试、理解、翻译、数学。下表提供了每个测试集的详细信息，包括其评测配置、评价指标以及所涉及的具体语言种类。\n",
      "\n",
      "详细的结果如下：\n",
      "\n",
      "上述结果表明，Qwen1.5 Base 模型在 12 种不同语言的多语言能力方面表现出色，在学科知识、语言理解、翻译、数学等各个维度的评估中，均展现了不错的结果。更进一步地，在 Chat 模型的多语言能力上，可以观察到如下结果：\n",
      "\n",
      "长序列\n",
      "\n",
      "随着长序列理解的需求不断增加，阿里在新版本上提升了千问模型的相应能力，全系列 Qwen1.5 模型支持 32K tokens 的上下文。通义千问团队在 L-Eval 基准 上评估了 Qwen1.5 模型的性能，该 基准 衡量了模型根据长上下文生成响应的能力。结果如下：\n",
      "\n",
      "从结果来看，即使像 Qwen1.5-7B-Chat 这样的小规模模型，也能表现出与 GPT-3.5 可比较的性能，而最大的模型 Qwen1.5-72B-Chat 仅略微落后于 GPT4-32k。\n",
      "\n",
      "值得一提的是，以上结果仅展示了 Qwen 1.5 在 32K tokens 长度下的效果，并不代表模型最大只能支持 32K 长度。开发者可以在 config.json 中，将 max_position_embedding 尝试修改为更大的值，观察模型在更长上下文理解场景下，是否可以实现令人满意的效果。\n",
      "\n",
      "链接外部系统\n",
      "\n",
      "如今，通用 语言模型 的一大魅力在于其与外部系统对接的潜在能力。RAG 作为一种在社区中快速兴起的任务，有效应对了大 语言模型 面临的一些典型挑战，如幻觉、无法获取实时更新或私有数据等问题。此外， 语言模型 在使用 API 和根据指令及示例编写代码方面，展现出了强大的能力。大模型能够使用代码解释器或扮演 AI 智能体，发挥出更为广阔的价值。\n",
      "\n",
      "通义千问团队对 Qwen1.5 系列 Chat 模型在 RAG 任务上的端到端效果进行了评估。评测基于 RGB 测试集，是一个用于中英文 RAG 评估的集合：\n",
      "\n",
      "然后，通义千问团队在 T-Eval 基准 测试中评估了 Qwen1.5 作为通用智能体运行的能力。所有 Qwen1.5 模型都没有专门面向 基准 进行优化：\n",
      "\n",
      "为了测试工具调用能力，阿里使用自身开源的评估 基准 测试模型正确选择、调用工具的能力，结果如下：\n",
      "\n",
      "最后，由于 Python 代码解释器已成为高级 LLM 越来越强大的工具，通义千问团队还在之前开源的评估 基准 上评估了新模型利用这一工具的能力：\n",
      "\n",
      "结果表明，较大的 Qwen1.5-Chat 模型通常优于较小的模型，其中 Qwen1.5-72B-Chat 接近 GPT-4 的工具使用性能。不过，在数学解题和可视化等代码解释器任务中，即使是最大的 Qwen1.5-72B-Chat 模型也会因编码能力而明显落后于 GPT-4。阿里表示，会在未来的版本中，在预训练和对齐过程中提高所有 Qwen 模型的编码能力。\n",
      "\n",
      "Qwen1.5 与 HuggingFace transformers 代码库进行了集成。从 4.37.0 版本开始，开发者可以直接使用 transformers 库原生代码，而不加载任何自定义代码（指定 trust_remote_code 选项）来使用 Qwen1.5。\n",
      "\n",
      "在开源生态上，阿里已经与 vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于 量化 ）、Axolotl、LLaMA-Factory（用于微调）以及 llama.cpp（用于本地 LLM 推理）等框架合作，所有这些框架现在都支持 Qwen1.5。Qwen1.5 系列目前也可以在 Ollama 和 LMStudio 等平台上使用。\n",
      "\n",
      "参考内容：\n",
      "\n",
      "https://qwenlm.github.io/blog/qwen1.5/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-7\n",
      "title= 向完全自主性更进一步，清华、港大全新跨任务自我进化策略让智能体学会「以经验为鉴」\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 「以史为鉴，可以知兴替。」 人类的进步史，可以看作是一个不断吸取过去经验、不断推进能力边界的自我演化过程。在这个过程中，我们吸取过去失败的教训以纠正错误，借鉴成功的经验以提升效率和效果。这种自我进化的过程在我们的生活中无所不在：从如何总结经验以更好地解决工作中的问题，到如何利用规律更精确地预测天气，我们都在不断地从过去的经验中学习和进化。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "成功从过去的经验中提取知识并将其应用于未来的挑战，这是人类进化之路上重要的里程碑。那么在 人工智能 时代，AI 智能体是否也可以做到同样的事情呢？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "近年来，GPT 和 LLaMA 等 语言模型 展示了他们在解决复杂任务时的惊人能力。然而，他们尽管可以利用工具解决具体任务，但在本质上缺乏对过去成功和失败经历的洞见与汲取。这就像一个只会完成特定任务的机器人，虽然在完成当下任务上表现出色，但面对新的挑战时，却无法调用过去的经验来提供帮助。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "针对这一难题，近期来自 清华大学 、香港大学、人民大学以及面壁智能的联合团队提出了一种全新的智能体自我演化策略：探索 - 固化 - 利用（Investigate-Consolidate-Exploit，ICE）。它旨在通过跨任务的自我进化来提升 AI 智能体的适应性和灵活性。其不仅能提升智能体处理新任务时的效率和效果，还能显著降低对智能体基座模型能力的需求。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个策略的出现，无疑为智能体的自我进化开启了全新的篇章，也意味着我们离实现智能体的完全自主性又迈进了一步。\n",
      "\n",
      "论文标题：Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution\n",
      "\n",
      "论文链接：https://arxiv.org/abs/2401.13996\n",
      "\n",
      "智能体任务间经验迁移以实现自我进化概览图\n",
      "\n",
      "智能体自我进化的两个方面： 规划 与执行\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "当下大部分复杂智能体都可以分成任务 规划 （Planning）与任务执行（Execution）两大方面。在任务 规划 上，智能体通过推理将用户需求细化并制定完成目标的详细策略；而在任务执行上，智能体通过工具调用实现与环境的交互，从而完成相应子目标。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了更好地促进以往经验的重复利用，作者首先将这两方面的 进化策略 解耦。他们以 XAgent 智能体架构中的树状任务 规划 结构以及 ReACT 链式工具执行为例，分别介绍了 ICE 策略的具体实现。\n",
      "\n",
      "智能体任务 规划 的 ICE 自我演化策略\n",
      "\n",
      "对于任务 规划 ，自我进化依照 ICE 被分为以下三个阶段：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在探索阶段，智能体记录下整个树状任务 规划 结构，并同时动态检测各个子目标的执行状态；\n",
      "\n",
      "在固化阶段，智能体首先剔除所有失败的目标结点，之后对于每个成功完成的目标，智能体将以该目标为子树的所有叶子结点依次排开形成一条 规划 链（Workflow） ；\n",
      "\n",
      "在利用阶段，这些 规划 链将被作为新任务目标分解细化的参考依据，以利用过往的这些成功经验。\n",
      "\n",
      "智能体任务执行的 ICE 自我演化策略\n",
      "\n",
      "任务执行的自我演化策略依然分为 ICE 三个阶段，其中：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在探索阶段，智能体动态记录每个目标执行的工具调用链，并对工具调用中出现的可能问题进行简单的检测归类；\n",
      "\n",
      "在固化阶段，工具调用链将被转化为类似自动机的 流水线（Pipeline）结构 ，工具调用顺序与调用之间的转移关系将被固定，同时还会去掉重复调用，增加分支 逻辑 等等让自动机自动化执行流程更加鲁棒；\n",
      "\n",
      "在利用阶段，对于相似的目标，智能体将直接自动化执行流水线，从而提升任务完成效率。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "XAgent 框架下的自我进化实验\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "作者在 XAgent 框架中对提出的 ICE 自我演化策略进行了测试，并总结了以下四点发现：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ICE 策略能够显著降低模型的调用次数，从而提升效率，减少开销。\n",
      "\n",
      "存储的经验在 ICE 策略下有着较高的复用率，这证明了 ICE 的有效性。\n",
      "\n",
      "ICE 策略能够提升子任务完成率同时减少 规划 返修的次数。\n",
      "\n",
      "通过以往经验的加持，任务执行对模型能力的要求显著下降。具体来看，使用 GPT-3.5 搭配上之前的任务 规划 与执行经验，效果可以直接媲美 GPT-4。\n",
      "\n",
      "在探索 - 固化进行经验存储后，测试集任务在不同智能体 ICE 策略下的表现\n",
      "\n",
      "同时，作者还进行了额外的消融实验：在存储经验逐渐增加的情况下，智能体的表现是否越来越好？答案是肯定的。从零经验，半经验，到满经验，基座模型的调用次数逐渐减少，而子任务完成度逐渐提升，同时复用率也有升高。这表明更多的过往经验能够更好地促进智能体执行，实现规模效应。\n",
      "\n",
      "在不同经验存储量下，测试集任务表现的消融实验结果统计\n",
      "\n",
      "结语\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "畅想一下，在人人都能够部署智能体的世界中，成功经验的数量会随着智能体个体任务执行不断累积，而用户也可以将这些经验在云端中、社区里进行分享。这些经验将促使智能体不断汲取能力，自我进化，逐渐达到完全自主。我们向这样的时代又迈进了一步。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-6\n",
      "title= 摧毁房价的，可能是Apple Vision Pro\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 机器之能报道 编辑：吴昕 公共场合，“演技”趋于浮夸的人越来越多...... 辛普森一家早就预言了 Apple Vision Pro：） 视频链接：https://mp.weixin.qq.com/s/zfY82n3fbh_p6MJIFSdgMg\n",
      "\n",
      "苹果发售 Vision Pro 以来不到 48 小时，人们开始为之疯狂。一些大城市中心已经出现佩戴 Vision pro 逛街的人，甚至有人戴着它开车、上飞机旅行。\n",
      "\n",
      "有意思的是，在此之前，Meta Quest 已售出超 2000 万台（去年 11 月份的媒体数据，其中 1800 万台是 Quest 2 ），我们几乎看不到类似场景，没人戴着它出街。\n",
      "\n",
      "据媒体披露，早在苹果发布第一代苹果手机时，就已经开始申请 Apple Vision Pro 的专利。苹果为此研发十多年，耗资数十亿美元，光是申请专利就超过 5000 项，几乎从头到尾 重构 了整个产业链。\n",
      "\n",
      "从目前社交媒体上排山倒海的体验视频来看，2024 年 2 月是一个需要被记住的节点，这件事跟一年前 ChatGPT （包括 LLM）发布同等重要，因为一个新的常态可能就此展开。\n",
      "\n",
      "这是透过 Vision Pro 看到的现实世界，很直观。\n",
      "\n",
      "将 Vision Pro 和特斯拉自动驾驶疯狂结合，估计国内交规不会允许出现这些名场面：\n",
      "\n",
      "不过，飞行时长约为 5 小时，对于长时间佩戴者来说，设备还是太重了，他的脸部感觉很紧。另外，更新 visionOS 1.0.2 后，发现电池电量下降到 50%！(：看来续航仍然是一个问题。\n",
      "\n",
      "专为 Apple Vision Pro 打造的应用如雨后春笋。比如，像《沙丘》里一样，沉浸式学习细胞分子结构、为现实世界添加字幕和实时翻译功能、沉浸式冥想、玩桌面经典游戏、在真实世界玩虚拟滑板、看房等。看完下面这段视频，你是否觉得摧毁房价的有可能是 Apple Vision Pro ？\n",
      "\n",
      "除了作为一个物理存在，以后的房屋可能还会有一个 Vision Pro 层。有了后者，一线城市地下室也能住出豪宅的感觉？对了，还可以玩实时换脸！\n",
      "\n",
      "Polycam 也登陆了 Apple Vision Pro 。用户可以通过 Apple Vision Pro 浏览 Polycam 数百万个原生 3D 资源库，还能像在现实中一样拿起它们并放到你想放置的地方。你可以用苹果手机扫描你觉得有趣的东西，Polycam 都会把它变成 3D 模型。戴上 Apple Vision Pro 后，你就可以和它们在 虚拟现实 中互动啦。\n",
      "\n",
      "虽然此前 YouTube、Spotify 和 Netflix 都曾拒绝允许他们的 iPad 应用程序在 Vision Pro 上运行，不过 YouTube 今天表示未来会推出合适的 VisionOS YouTube 应用程序。\n",
      "\n",
      "苹果发言人表示，YouTube 的 360 度和 3D 视频无法在 Vision Pro 上运行良好，因为“很多内容都是为无法提供高质量空间体验的设备创建的。”在某些情况下，这些内容也可能导致运动不适。\n",
      "\n",
      "下面这段视频记录了一位科技博主全程佩戴 Vision Pro 外出一天的记录，包括踩滑板、步行、坐地铁、去快餐店吃饭、和朋友通话、和好奇路人讨论这款设备等。\n",
      "\n",
      "和其他视频不同之处在于，他是在自家客厅里一台高 11 英尺、宽 20 英尺的 4k 屏幕上，用 Vision Pro 编辑的。“我可以站起来走过去，仔细观察这些剪辑，就像二战电影里那些巨大的墙上地图一样。”\n",
      "\n",
      "当然，这台精密的空间计算设备也带来了迄今为止难度最大的拆解挑战。这个视频深入展示出 Vision Pro 极其复杂的硬件结构，据说是苹果目前最复杂的硬件。\n",
      "\n",
      "库克说，Vision Pro 是有史以来最先进的消费电子设备，“空间计算时代已经到来。”\n",
      "\n",
      "但第一代的 Vision Pro 注定不会是一款畅销的电子设备。近期的方舟投资《Big ideas 》指出， 虚拟现实 市场刚刚起步。尽管头显设备有了重大改进，包括苹果的 Vision Pro，但开发人员并没有蜂拥而至支持 虚拟现实 ( VR )。如果没有令人信服的用例，采用速度会很慢。\n",
      "\n",
      "例如，MetaQuest 仅提供 2,200 个应用程序，而 iPhone 推出五年后拥有 553,000 个应用程序，而这只是其中的一小部分。结果，Meta 仅售出了 2700 万部 Quest，仅占苹果发布五年后累计售出 1.46 亿部 iPhone 的 18%。\n",
      "\n",
      "Anyway ,这些决定冒险一试的人愿意体验苹果的愿景是如何落地的，你呢？\n",
      "\n",
      "参考链接：\n",
      "\n",
      "\n",
      "\n",
      "https://www.theverge.com/2024/2/5/24062425/youtube-vision-pro-app-360-vr-video \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-5\n",
      "title= 大语言模型加速材料发现，普林斯顿大学团队利用 LLM 准确预测晶体特性\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 编辑 | X\n",
      "\n",
      "晶体特性的预测在晶体设计过程中起着至关重要的作用。当前预测晶体特性的方法主要集中于使用图神经网络（GNN）对晶体结构进行建模。尽管 GNN 很强大，但准确模拟晶体内原子和分子之间的复杂相互作用仍然是一个挑战。\n",
      "\n",
      "文本数据提供了丰富的信息和表现力，但从晶体文本描述预测晶体特性的研究还不够。主要原因之一是缺乏该任务的公开数据。\n",
      "\n",
      "普林斯顿大学的研究人员创建了一种 AI 工具来预测晶体材料的行为。新方法依赖于大型语言模型（LLM）。通过综合文本描述中的信息（包括原子之间键的长度和角度以及电子和光学特性的测量等细节），新方法可以比现有模拟更准确、更彻底地预测新材料的特性，并有可能加快设计和测试新技术的过程。\n",
      "\n",
      "研究人员开发并公开了一个基准数据集（称为 TextEdge），其中包含来自 Materials Project 的 140,000 多个晶体的描述，然后，提出了 LLM-Prop，一种利用 LLM 的通用学习能力从文本描述中预测晶体的物理和电子特性的方法。\n",
      "\n",
      "研究人员测试了该工具预测先前研究的晶体结构（从普通食盐到硅半导体）特性的能力。已经证明了 LLM-Prop 预测能力，正在努力将该工具应用于新晶体材料的设计。\n",
      "\n",
      "论文一作、普林斯顿大学计算机科学助理教授 Adji Bousso Dieng 表示，「该方法代表了一个新的基准，可以帮助加速材料的广泛应用。我们是第一个使用大型语言模型来解决这个问题的团队。」\n",
      "\n",
      "该方法于 2023 年 11 月 29 日，在波士顿举行的 the Materials Research Society's Fall Meeting 上提出。\n",
      "\n",
      "相关研究以「LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions」为题，发布到 arXiv 预印平台。\n",
      "\n",
      "GitHub 地址：https://github.com/vertaix/LLM-Prop\n",
      "\n",
      "论文链接：https://doi.org/10.48550/arXiv.2310.14029\n",
      "\n",
      "现有的基于人工智能的晶体特性预测工具依赖于图神经网络的方法，但这些方法的计算能力有限，无法充分捕捉晶体中原子之间的几何形状和键长的细微差别，以及由这些结构产生的电子和光学性质。\n",
      "\n",
      "「我们在计算机视觉和自然语言方面取得了巨大进步，」Dieng 说，「但在处理 AI 图方面，我们还不是很先进。所以，我想从图转移到我们已经有了很好的工具的领域。如果我们有文本，那么我们就可以在文本上利用所有这些强大的大型语言模型。」\n",
      "\n",
      "该研究的合著者、普林斯顿大学机械与航空航天工程教授兼负责创新的副院长 Craig Arnold 表示，基于语言模型的方法「为我们提供了一种全新的方式来看待材料设计问题。这实际上是关于，我如何获取人类已经开发的所有这些知识，以及如何处理这些知识以向前发展？它与我们当前的方法有本质上的不同，我认为这赋予了它很大的力量。」\n",
      "\n",
      "研究的主要贡献概述如下：\n",
      "\n",
      "研究人员收集、整理并公开一个基准数据集，其中包含大约 144K 晶体文本描述及其属性。\n",
      "\n",
      "提出 LLM-Prop，这是一种高效微调的网络，使其能够在晶体特性预测方面实现最先进的性能，优于当前最好的基于 GNN 的晶体特性预测器。\n",
      "\n",
      "表 1：来自收集的基准数据集的示例。（来源：论文）\n",
      "\n",
      "数据包含 144, 931 个晶体，将其分为 125, 098 个晶体用于训练，9,945 个晶体作为验证集，9,888 个晶体作为测试集。对于每个晶体，收集其 ID、结构信息、带隙、体积以及其带隙是直接还是间接的。使用 Robocrystallographer 提取了晶体文本描述。\n",
      "\n",
      "LLM-Prop，是一个源自 T5 的精心微调的网络，用于晶体特性预测。通过大量实验证明，LLM-Prop 在预测晶体固体的物理和电子特性方面实现了卓越的性能，超越了当前最先进且使用广泛的基于 GNN 的架构（例如 ALIGNN）。\n",
      "\n",
      "图 1：LLM-Prop 架构。（来源：论文）\n",
      "\n",
      "LLM-Prop 在所有任务上都能产生更好或相当的性能，包括 zero-shot 预测。尽管超参数少了 3 倍，LLM-Prop 的性能也优于经过微调的 MatBERT（一种特定领域的预训练 BERT 模型）。\n",
      "\n",
      "LLM-Prop 在回归和分类任务上都优于所有基于 GNN 的基线。\n",
      "\n",
      "表 2：与带隙预测基线的性能 (MAE) 比较。（来源：论文）\n",
      "\n",
      "对于带隙预测，LLM-Prop 在验证集和测试集上均优于性能最佳的基线 (ALIGNN)，分别提高了约 8% 和 4%。\n",
      "\n",
      "表 3：性能 (MAE) 与体积预测基线的比较。（来源：论文）\n",
      "\n",
      "对于体积预测，LLM-Prop 还比验证集和测试集上的最佳性能基线 (ALIGNN) 分别提高了约 67% 和 66%。这种改进的可能原因可能是，与 GNN 相比，LLM-Prop 可以很容易地从文本描述中获取最重要的体积预测信息。\n",
      "\n",
      "研究结果凸显了基于文本的方法在材料科学中的巨大潜力，基准文本数据 TextEdge 将助力这一新兴领域的研究。\n",
      "\n",
      "参考内容：https://techxplore.com/news/2024-01-harness-large-language-materials-discovery.html \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-4\n",
      "title= 夸克大模型应用为先加持夸克网盘深挖相册使用场景\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 2024年将是大模型应用落地的爆发年，这已经成为业界共识。夸克大模型自去年11月份发布以来，结合自身业务小步快跑，在夸克App上已经落地了多个应用。最近，夸克网盘结合春节场景和大模型技术，升级几项图片处理智能工具。\n",
      "\n",
      "夸克网盘即将上线的“春节图片故事”，是为用户春节期间拍摄上传的图片自动智能筛选生成合辑。该功能除了基于时间、地点两个维度筛选，还会基于人物智能筛选，并剔除掉过亮或过暗等不符合要求的图片。夸克网盘还会利用AI算法为图片合辑智能生成文案，比如鲜花影集的文案是“花与美妙人间”。\n",
      "\n",
      "此前，AI技术还被应用在夸克网盘相册中的智能查找和智能分类上。夸克网盘利用AI人脸理解技术，将不同人像图片分类存放，通过头像就能找到对应人物的图片。夸克网盘还支持将图片根据地点分类，并且根据拍摄的具体地点、人物、时间和图片特征等进行更进一步的归类。\n",
      "\n",
      "支持 AI 自然语言搜索功能，夸克网盘用户通过模糊词、形容词等关键信息，就能快速找到图片。例如搜索“睡觉的猫”，夸克网盘会列出符合条件的图片。对比之下，同样关键词在手机本地相册搜索只会得到“无结果”提示。\n",
      "\n",
      "在夸克网盘（左）和苹果手机照片应用（右）中搜索“睡觉的猫”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "据介绍，AI 自然语言搜索功能背后是夸克视觉自监督大模型和图文多模态大模型在发挥作用，核心原理是将搜索词利用大模型技术生成一串特征向量用于和图片对比，然后再通过直接计算空间距离来召回符合的图片。这种向量编码通过分析全网海量图像信息数据，进行AI的训练学习。这意味着该功能不会侵犯用户的隐私数据，就能实现智能搜图。\n",
      "\n",
      "夸克大模型正在用技术不断升级改善相册功能的使用体验，用户不仅可以在夸克网盘上智能搜图、智能找图，还可以使用图片编辑、夸克快传等功能快速编辑和分享图片。\n",
      "\n",
      "这符合夸克大模型在垂直领域的发力的定位。夸克网盘技术负责人表示，夸克大模型是面向搜索、生产力工具和资产管理助手的应用型大模型。夸克网盘围绕相册功能开发的智能工具，便是夸克大模型在资产管理助手上的能力体现。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-3\n",
      "title= 上海街头偶遇未来科技！机器狗和外骨骼机器人都来为2024 GDC造势\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 一年一度的全球开发者先锋大会（GDC）即将在上海掀起科技风暴！！！这不仅仅是一场大会，而是所有代码高手、科技狂热者们的盛大节日！！！\n",
      "\n",
      "GDC缘起总理在达沃斯世界经济论坛打call的WAIC世界人工智能大会，作为WAIC聚焦科技和人才力量的重要板块，进化到如今的全球开发者嘉年华，已化身顶尖技术风向标，汇集全球顶尖开发者、科技先锋、企业家和学术翘楚，开启一场科技交流狂欢盛典。\n",
      "\n",
      "2024 GDC 将在上海徐汇滨江召开，这里是科技与文化交汇的前沿阵地，为大会带来无限活力与创新灵感。大会部分同期活动也会在临港等地举行。临港是2023 GAIDC的举办地，有多项重要成果发布，引起各方关注和赞誉。今年我们将开发者的范畴从AI扩展到整个技术领域，从GAIDC到GDC，更是思维的飞跃，让“多元共生”不再是想象。\n",
      "\n",
      "2024 GDC 主题是“开发者的‘模’力之都”。今年大会全面升级，1场开幕式、5场前沿技术讲坛、10+场平行技术讲坛，及X场工作坊、10000 m2互动体验、竞技场、创客集市、场外活动等，聚焦大模型、人形机器人、开源开放、AIGC前沿话题，一网打尽科技界的最新动态。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-06-2\n",
      "title= 「天工2.0」MoE大模型发布——「天工AI」国内首个MoE架构免费向C端用户开放的大语言模型应用全新问世\n",
      "author= []\n",
      "publish_date= 2024-02-06 00:00:00\n",
      "text= 北京时间2月6日，昆仑万维正式发布新版MoE大语言模型「天工2.0」与新版「天工AI智能助手」APP，这是国内首个搭载MoE架构并面向全体C端用户免费开放的千亿级参数大语言模型AI应用。用户即日起可在各手机应用市场下载「天工AI智能助手」APP，体验昆仑万维「天工2.0」MoE大模型的卓越性能。\n",
      "\n",
      "「天工2.0」是昆仑万维自去年4月发布双千亿级大语言模型「天工」以来的最大规模版本升级，其采用业内顶尖的MoE专家混合模型架构，应对复杂任务能力更强、模型响应速度更快、训练及推理效率更高、可扩展性更强。\n",
      "\n",
      "此次更新全面升级了AI搜索、对话、阅读、创作的回答质量与响应速度，搭载强大的多模态能力，支持图文对话、文生图等多模态应用，支持最高100K的超长上下文窗口（超过15万个汉字），并新增了AI绘画、数据分析、AI伴侣、AI算命、热梗百科等多项新兴玩法，让AI更聪明、更实用、更有趣，成为每个人日常生活中的全能AI小助手。\n",
      "\n",
      "昆仑万维致力于人工智能模型算法的创新与开拓，不断探索通用人工智能技术前沿。除了双千亿级大语言模型「天工」、MoE专家混合大模型「天工2.0」外，昆仑万维还围绕「天工」系列大模型，推出了百亿级开源大语言模型系列「天工Skywork-13B」、AI Agent开发平台「天工SkyAgents」、多模态大语言模型「天工Skywork-MM」等前沿AI产品，并已逐步构建起AI大模型、AI搜索、AI音乐、AI Story、AI游戏等AI业务矩阵，是国内模型技术与工程能力最强、布局最全面的人工智能大模型企业之一。\n",
      "\n",
      "MoE：全球顶尖的大模型核心技术路径\n",
      "\n",
      "MoE（Mixture-of-Experts，专家混合模型）是当前大语言模型赛道技术最顶尖、研发最前沿的底层架构，是全球最领先的大模型核心技术路径之一。\n",
      "\n",
      "自2023年6月以来，昆仑万维不断针对MoE架构技术最前沿进行研发探索，并成功发布国内首个搭载MoE架构并面向全体C端用户免费开放的千亿级参数大语言模型AI应用——「天工AI智能助手」APP。\n",
      "\n",
      "「天工AI智能助手」以昆仑万维「天工2.0」MoE大模型为核心技术引擎，其技术原理是将复杂的大模型任务拆解为多个更小、更细分的子任务，每个子任务都由垂直领域的专家模型处理，从而使得昆仑万维「天工2.0」不仅大幅提高了模型训练与推理的性能和效率，更能实现多个垂直领域的知识融合，使模型能够更好地理解和处理不同应用场景下的复杂问题，为用户提供更准确、更全面的回答方案。\n",
      "\n",
      "同时，昆仑万维技术团队更是通过一系列针对性的MoE技术攻关，在投入大量研发训练资源后，最终解决了困扰整个MoE产业的模型不收敛、特定任务泛化效果较差等核心性能问题，使「天工2.0」的模型性能得到显著提升。\n",
      "\n",
      "模型性能更强、速度更快、架构更灵活\n",
      "\n",
      "「天工2.0」的技术领先性体现在其核心MoE架构的卓越优势。MoE架构主要由门控模型/路由器（Gating Model/Router）和一组专家模型（Experts Models）构成，当数据输入门控模型/路由器时，系统会根据任务类型将每个token分配给一个或多个专家模型，使得每个专家模型可以专注于处理该部分数据，从而获得模型性能的整体提升。\n",
      "\n",
      "较之传统大模型架构，「天工2.0」具有以下优势：\n",
      "\n",
      "1.应对复杂任务能力更强：「天工2.0」MoE模型集成了多个专家模型，每个专家模型都能针对不同的数据分布和构建模式进行搭建，从而显著提升大模型在各个细分领域的专业能力，整体模型通过整合各自专家模型的输出结果，使得「天工2.0」在处理复杂任务、多模态任务时拥有显著性能提升。\n",
      "\n",
      "2.速度更快、效率更高：由于MoE模型推理计算过程中只有少数特定专家模型被激活，相较于同等参数规模的稠密模型，「天工2.0」MoE模型呈现出极高的稀疏性，使其拥有更高的推理计算效率，从而让用户获得更快的AI响应速度。\n",
      "\n",
      "3.灵活、多样、可扩展性更强：一方面，模型稀疏性使得「天工2.0」能够在不增加计算量的前提下显著扩张模型规模，在同等计算资源下获得更强的模型性能；另一方面，通过增加专家模型数量、调整专家模型的权重配比，「天工2.0」能够极大丰富模型的可扩展性，构建更为灵活、多样、可扩展性更强的新时代大模型。\n",
      "\n",
      "「天工AI智能助手」APP全面升级\n",
      "\n",
      "「天工AI智能助手」APP基于昆仑万维自研「天工」系列大模型打造，是一款能搜、能聊、能写、能画的AI智能助手，其拥有强大的自然语言处理和智能交互能力，能够实现个性化AI搜索、智能问答、AI绘画、聊天互动、文本生成、编写代码、语言翻译等多种应用场景，并且具有丰富的知识储备。\n",
      "\n",
      "伴随着「天工2.0」大模型的重磅升级，「天工AI智能助手」也迎来了版本的全面更新。\n",
      "\n",
      "1.强大的多模态能力：「天工AI智能助手」所采用的多模态大模型基于一体化的开发策略，在底座模型的基础上进行深入开发与优化，引入多分辨率的视觉编码器和强大的语言基座模型，使其能够支持任意尺寸的图片输入和复杂的用户指令。\n",
      "\n",
      "在强大的多模态大模型能力加持下，新版「天工AI智能助手」具备优秀的视觉理解、推理和指令遵循能力，能够满足图文对话、图文创作、知识问答等多种用户需求。同时，得益于模型杰出的理解能力，新版「天工AI智能助手」生成的图像在内容丰富度、精细度和图像质量上均表现卓越。\n",
      "\n",
      "与此同时，在强大的多模态能力加持下，「天工AI智能助手」还能生成图文并茂的答案内容，让用户问出“螺蛳粉怎么做？”“怎么用吉他弹《稻香》？”“2024春节放假安排？”这类问题时能够得到图像/视频辅助呈现，使得AI回答的结果更直观，内容更丰富。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.支持100K超长上下文窗口：「天工AI智能助手」的超长上下文窗口技术基于100K原生文本进行训练，能够支持最高100K（超过15万汉字）的文本对话，并能够通过扩展技术可以支持200K超长文档理解。在InfiniteBench评测中，「天工」系列大模型多项指标全球第一，10项指标平均分47.5分，超过Claude2，接近GPT4-128k的52.6分。\n",
      "\n",
      "在针对超长上下文模型的“大海捞针”测试中，研究人员会在海量的文档集里面插入特定信息，然后对文档集进行提问，期待模型能从“茫茫文海”中找出正确的关键信息，以验证模型的长上下信息提取能力。在“大海捞针”测试中，「天工」模型取得了100%正确结果。\n",
      "\n",
      "3.搜得更准、写得更好、读得更快：新版「天工AI智能助手」拥有更强大的关键词与语义分析能力更精准识别用户任务需求，在AI搜索、对话、阅读、创作等不同应用场景中，都能针对用户的不同需求提供更准确、更具体的回答与追问建议。同时，新版「天工AI智能助手」AI搜索质量、安全能力、答案丰富程度都进一步提高。\n",
      "\n",
      "例如，在「天工AI智能助手」中，用户可使用“AI阅读”功能快速提炼总结文献内容，并针对文献内容细节进行追问，得到快速、准确、具体的答案内容。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.丰富有趣的AI Agent：新版「天工AI智能助手」新增了如AI绘画、数据分析、AI伴侣、AI算命、热梗百科等多款官方AI Agent，让「天工AI智能助手」在能搜、能聊、能写之余，新增更多有趣而实用新兴玩法，不断探索AIGC技术的应用边界，成为每个人日常生活中必不可少的全能AI小助手。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从2月8日开始，由昆仑万维主办的“巧绘龙年”AI绘画大赛也将在「天工AI智能助手」APP内开启，用户使用APP内“AI绘画”功能绘制图画作品并投稿至活动专区，即可有机会获得最高10万元人民币的现金大奖。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这是国内首个面向全体C端用户免费开放、奖金规模达到数十万量级的AI绘画大赛。得益于「天工」系列大模型卓越的多模态技术能力，高水准的文字意图识别确保用户能够尽情发挥创意，绘制出内容丰富、细节精致、审美高级的个性化AIGC图像。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "一直以来，昆仑万维始终秉承着“实现通用人工智能，让每个人更好地塑造和表达自我”的公司使命，不断降低大模型技术在各行各业的应用和学习门槛，携手探索未知世界、共创科技未来。\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-05-12\n",
      "title= 可多模态数据集成、插补和跨模态生成，中科院&树兰医院&北师大团队开发带有掩码模块的深度生成框架\n",
      "author= []\n",
      "publish_date= 2024-02-05 00:00:00\n",
      "text= 编辑 | 红菜苔\n",
      "\n",
      "随着单细胞技术的发展，许多细胞特性可以被测量。此外，多组学分析技术可以同时联合测量单个细胞中的两个或多个特征。为了快速处理积累的各种数据，需要多模态数据集成的计算方法。\n",
      "\n",
      "树兰医院、中国科学院和北京师范大学的合作团队提出了 inClust+，一个用于多组学分析的深度生成框架。它建立在之前针对转录组数据所开发的 inClust 的基础上，并增加了两个专为多模式数据处理设计的掩码模块：编码器前面的输入掩码模块和解码器后面的输出掩码模块。\n",
      "\n",
      "InClust+ 可用于整合来自相似细胞群的 scRNA-seq 和 MERFISH 数据，并根据 scRNA-seq 数据估算 MERFISH 数据。InClust+ 具有将多模态数据（例如具有基因表达、染色质可及性和蛋白质丰度的三模态数据）与批次效应整合的能力。\n",
      "\n",
      "研究人员使用 inClust+ 整合一个未标记的单模态 scRNA-seq 数据集和两个标记的多模态 CITE-seq 数据集，将标签从 CITE-seq 数据集转移到 scRNA-seq 数据集，并生成单模态 scRNA-seq 数据中缺失的蛋白质丰度模态。\n",
      "\n",
      "该研究以「InClust+: the deep generative framework with mask modules for multimodal data integration, imputation, and cross-modal generation」为题，于 2024 年 1 月 24 日发布在《BMC Bioinformatics》。\n",
      "\n",
      "近年来，单细胞技术的进步使得在单个细胞中获得多种性状成为可能，例如单细胞 RNA 测序 (scRNA-seq)、转座酶可及染色质测序的单细胞测定 (scATAC-seq) 和单细胞亚硫酸氢盐测序 (scBS-seq)。\n",
      "\n",
      "这些单细胞方法极大地促进了科学家对细胞的理解。从而揭示细胞群的异质性，推断细胞发育轨迹，并重建基因调控网络。但以一种方式收集的数据仅代表细胞状态的有限侧面。为了获得更全面、更全面的信息，需要将来自不同模态的数据整合在一起，从而更好地揭示数据的生物学意义。\n",
      "\n",
      "为了完成这些任务，树兰医院、中国科学院和北京师范大学的合作团队在之前的研究中，曾提出了 inClust（集成聚类），一种灵活的转录组数据深度生成框架。在这里，该团队通过添加两个新模块来扩展 inClust，即编码器前面的输入掩码模块和解码器后面的输出掩码模块。\n",
      "\n",
      "图示：inClust+的架构及其应用。（来源：论文）\n",
      "\n",
      "该团队将增强的 inClust 命名为 inClust+，并证明它不仅可以完成数据集成，还可以利用掩模模块的优点完成基因插补。\n",
      "\n",
      "研究人员将 inClust+ 应用于各种数据集，包括多个单模态（未配对）数据集、一个或多个多模态数据集以及包含多模态数据和单模态数据的数据集。在这些例子中，inClust+展示了其数据集成、插补和数据生成的能力。\n",
      "\n",
      "首先，通过 mask 模块的优点，参考类似细胞群的 scRNA-seq 数据，使用 inClust+ 对 MERFISH 数据进行插补。\n",
      "\n",
      "然后，通过三个示例评估了具有堆叠式编码器-解码器架构和掩模模块的 inClust+ 的多模态集成能力。结果表明，inClust+ 不仅可以混合模态之间的数据，还可以分离生物学差异并消除批次效应。\n",
      "\n",
      "最后，研究人员使用 inClust+ 将数据与单模态数据集和多模态数据集进行集成。结果表明，inClust+ 可以将标签从多模态数据转移到单模态数据，并补全单模态数据中缺失的模态。\n",
      "\n",
      "图示：inClust+ 整合多模态（三重）数据集的图表。（来源：论文）\n",
      "\n",
      "InClust+ 的应用并不限于上述情况。对于基因插补，会出现一种情况，即所有数据集都有自己的特定基因，而不是只有一个数据集有自己独特的基因。通过调整输出掩码，inClust+ 可以基于共享基因整合两个数据集，并通过引用相应数据集中的特定基因来估算两个数据集中的其余基因。对于缺失模态生成，会出现所有数据集都有自己特定模态的情况，inClust+ 可以基于共享模态整合两个数据集，并通过引用相应数据集中的特定模态来生成每个数据集中的缺失模态。\n",
      "\n",
      "由于inClust+ 是 inClust 在多模态应用中的扩展，因此与其他集成方法相比，inClust+ 和 inClust 可以作为一个整体放在一起。该团队的模型（inClust 和 inClust +）与其他集成方法的区别在于其适应不同情况的灵活性以及尽可能集成信息的能力。\n",
      "\n",
      "灵活性体现在以下两点：首先，InClust 可以灵活地处理标签信息；InClust+也继承了这一优点，并体现在 inClust+ 可以半监督模式将标签从参考数据集转移到查询数据集。其次，inClust+ 中的两个 mask 模块可以灵活调整以处理不同的输入。\n",
      "\n",
      "模型尽可能整合信息的能力体现在以下两点：首先，在inClust中证明该模型不仅可以使用表达数据，还可以使用协变信息（例如批次）和标签信息；这一优点也被 inClust+ 继承了。其次，如 inClust+ 所示，该模型不仅可以利用共享数据（共享基因表达或共享模态）进行整合，还可以利用特定基因或模态来进行缺失基因插补或缺失模态生成。\n",
      "\n",
      "简而言之，该团队的模型不仅可以集成数据，还可以在数据集成的基础上完成其他下游任务（例如分布外生成、标签转移和新型识别、空间域分割、跨模态插补和生成）。\n",
      "\n",
      "添加掩模是增强深度学习模型的常见方法。在 inClust+ 中，研究人员通过一对掩码模块（输入掩码模块和输出掩码模块）来增强模型。掩模的灵活设计和使用使模型能够完成一系列任务，这些任务通常需要多个模型分别完成。例如，inClust+ 可以利用常见的和数据集特定的基因进行整合和插补，如 uniPort。掩码使事情变得简单：输入掩码筛选出常见基因，输出掩码筛选出相应数据的常见基因和数据集特定基因。\n",
      "\n",
      "同时，inClust+ 可以集成多模态数据集来实现多域翻译，作为跨模态自动编码器。输入掩码和输出掩码使inClust+ 成为多个独立且相关的编码器-解码器组合。因此，inClust+ 不仅可以对同一模态的数据进行压缩和重构，还可以将一种模态的数据压缩并重构为另一种模态，从而实现跨模态翻译。\n",
      "\n",
      "此外，inClust+ 可以集成多模态数据集和单模态数据集，将标签从多模态数据转移到单模态数据，并通过数据生成将单模态数据完整地转换为多模态数据，如 sciPENN。InClust+ 指的是多模态数据集，用于生成单模态数据集中缺失模态的数据。一般来说，作为一种模型增强技术，在模型中添加一对掩模不仅限于 inClust，还可以扩展到具有类似编码器-解码器结构的深度学习模型，例如 scArches。\n",
      "\n",
      "论文链接：https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-024-05656-2 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-05-11\n",
      "title= 王长虎：PixVerse 实测效果已超过 Pika，抖音经验让我们有足够优势\n",
      "author= []\n",
      "publish_date= 2024-02-05 00:00:00\n",
      "text= 今年 4 月宣布创办爱诗科技，加入 视频生成 赛道后，王长虎就消失在舆论场中了。他在抖音的职业经历，让爱诗科技在 视频生成 的牌桌上拥有一席重要位置。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2017 年，王长虎加入 字节跳动 开始担任 AI Lab 总监，在这个岗位上，他为抖音和 Tiktok 从 0-1 构建了视频AI能力。用王长虎本人的话说，为抖音所做的工作，让他的团队涉猎了几乎所有与视频智能相关的领域，包括且不限于数据处理、内容生成、安全问题处理、视频内容精准理解以及全方位广告场景。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "近期，王长虎接受了 机器之心 的独家专访。在采访中，王长虎详细介绍了抖音的视频智能化经验是如何被他复用到 视频生成 领域的，所积累的这些经验为他的新公司构建了数据、算法以及工程上的竞争优势。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "视频生成 工具 PixVerse，能够生成免费 4K 分辨率的高清视频，在光影细节和运动准确性等方面取得了进展。王长虎告诉 机器之心 ，PixVerse 的性能在某些方面已经达到了 Pika 的水平，甚至在多项评测中超越了它们 爱诗科技在近期上线的工具 PixVerse，能够生成免费 4K 分辨率的高清视频，在光影细节和运动准确性等方面取得了进展。\n",
      "\n",
      "自媒体 KOL 歸藏在一次对比评测中，从物品特写、写实风景、写实人像、皮克斯 2.5D 风格、 2D 动画风格五种风格对 PixVerse、Pika 以及 Runway 三大模型进行比较，为这三者评分 74.5 分、 73.5 分、 64.5 分，PixVerse 位居第一。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "歸藏认为，“PixVerse 的模型是这三者最为平衡的，可以有比较强的运动幅度，同时可以维持较好的一致性。Pika 在动漫和 2.5D 风格上的优势巨大，但图像质量以及一致性相对差一些。\n",
      "\n",
      "王长虎认为，目前 视频生成 领域存在的两个最关键的技术问题是准确性和一致性，而在这两个核心维度上，Pika 和 Runway 还有提升空间。“在目前的实测中，PixVerse 欢迎投资人和同行以及用户来进行随机大样本量的对比，对比越多，越能发现我们的优势”。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎表示， 视频生成 技术上的累进和商业化的运行已经可以同步开展。爱诗科技内部正在大量孵化基于 视频生成 技术的轻量应用，这些应用将面向使用抖音、快手等短视频平台的 C 端视频消费者，借助这些应用扩充产品影响力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“产品效果超越 Pika”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：能否请您介绍下目前公司最新的情况？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们核心团队成员从 2017 年开始参与抖音从零到一的发展，负责抖音背后的视频 人工智能 能力构建，在视频 AI 领域积累了很多独特的实战经验。随着 AI 时代的到来，我们认识到 AI 视频生成 的巨大潜力，而我们的经验让我们有信心（比别人）做得更快更好。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2023 年 4 月份，我们获得首轮数千万人民币的融资，6 月份核心团队基本成型。我们只用了 3-4 个月的时间就实现了重大进展，在某些方面超越了全球最大的竞争对手像 Pika 这样的公司。之前在抖音积累的视频处理经验，被成功应用在目前我们的 AI 视频生成 项目上。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "整体上，我们的产品发展分为两个阶段：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "第一阶段：TO 创作者，提供更好的 视频生成 服务，更好地理解创作者动机。同时，也支持直接面向用户，接受用户反馈进行迭代。我们目前已经推出的产品 PixVerse，用户已经可以在网页端和 Discord 社区使用，利用文字或者图片生成 4K 高清视频。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在第二阶段，我们希望直接面向消费者，不仅仅是提供工具，而是要打通创作和消费的整个流程，直接提供 AI 原生的可消费内容。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：横向对比，目前，PixVerse 在哪些方面做得比较好？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：目前，我们认为在 视频生成 领域最关键的两个问题是准确性和一致性。准确性要求每一帧都能精确地反映用户需求，一致性要求在时间轴上，视频中物体的运动符合客观规律。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "只有在这两方面上实现进步，让 AI 生成的视频准确反映用户需求，并且保证动态内容符合规律，运动具有连贯性，这样的视频才能应用于实际场景。就目前而言，我们发现在这两个核心维度上，Pika 和 Runway 各有明显的不足。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "目前，在这两方面，我们已经取得了重大进展。我们已经可以生成 4K 高清的动态视频，并且在可用性上实现了提升。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "举个例子，这是我们海外的一些创作者所实现的效果，一位创作者利用 PixVerse 制作的宣传片，其中每个素材都运用了我们的技术。 此外，我们还能制作一些基于电影、游戏素材的创新场景，比如钢铁侠在黄浦江游泳、让《原神》角色在其他游戏场景里跳舞等等。\n",
      "\n",
      "机器之心 ：你提到说在效果上已经 “超过了 Pika 和 Runway”，这个标准是什么？我们可以怎么感受到？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们可以用同一个 Prompt，对比一下 PixVerse 和 Pika 1.0、Runway 的效果。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如，柯基跳舞的 Prompt（a corgi is dancing_一只柯基在跳舞）\n",
      "\n",
      "在 Pika 1.0 的表现里，柯基主体非常精确且吸引人，但是它只进行了微小幅度的运动。观察它的画面，虽然每一帧单独看起来都不错，但当它们连在一起时，就不再呈现出视频的信息量。而Runway在柯基的表现上很好，但是基本没有跳舞的动作。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这本质上，是刚才我提到的 “运动一致性” 的问题，因为现在对于要让一个物体在时间轴上去做运动，本身是一个非常难的技术。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "总之，整个行业在模型 视频生成 方面面临的最基础问题，就是 准确率 和运动一致性。如果我们制作的视频素材既不准确又缺乏一致性，就无法在任何场景中有效使用。因此，我认为这是全球这个行业首要解决的问题。在这方面，我们技术上可能走在了前列。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我们欢迎对我们的模型进行实时测试，事实上，测试的案例越多，我们的优势就越明显。现在 视频生成 领域还没有形成统一的竞争格局，我们认为在这个方向上，我们有机会在全球范围内取得领先地位。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你们内部 视频生成 内容评价的标准是什么？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：目前我们内部已有一个评估标准，可以用于评估 视频生成 产品准确性和一致性。目前整个行业缺乏一个明确的判断标准，所以我们也在不断完善过程中，未来可能会发布出来。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我们将评估标准分为三个部分：主体动作风格、一致性（包括主体和背景），以及主体运动的合理性。我们还考虑了运镜技巧、创新瓶颈，以及丰富性，后者主要涉及画质和帧率。这些都是比较客观的维度。我们还评估信息量，即单位时间内的信息量。很多同行在研发时缺乏这样的 逻辑 。我们有一套体系来支持我们的迭代进程。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在效果评定上，除了主观与客观的标准，我们还采用盲测的方法做测试。向多个模型输入随机 Prompt，抹去水印，让足够样本的人做效果排序，来判断谁更优秀。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你认为这种评估方式相对客观吗？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：是的，这种方法相对客观。虽然图片生成和 视频生成 的效果判断比较主观，但我们之所以能在市场上迅速崛起，是因为我们使用的模型和整个系统支持我们从数据角度快速作出评估。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你们最近进行的盲测评估结果如何？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：根据我们最近的评估结果，我们的性能在某些方面已经达到了 Pika 平台的水平，甚至在多项评测中超越了它们。我们的产品在视觉效果、分辨率、画质上明显优于竞品。此外，在模型准确性、一致性和丰富性方面，我们的表现也更好。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "通常情况下，如果有投资人或同行要进行测试，我们会建议他们出至少 20 个问题，以确保样本量足够大。我们会根据他们认为重要的方面来进行测试。在所有这些测试中，我们通常可以明显地看出我们的产品比竞品更优秀，这是肉眼可见的。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "但最终判断哪个产品更好，很多时候并不仅仅是基于技术性的因素，而是主观上的偏好。如果大家普遍认为某个产品好，那么这个产品就被视为更优秀。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "用抖音经验解决准确性与一致性问题\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：再聊聊 “准确性” 以及 “一致性” 的问题，和其他公司比，你们是怎么做到这两方面表现得更好？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：影响 视频生成 最后结果的因素有很多，但最重要的是：数据、算法和工程能力，而我们在过往经验中，这几方面都有自己的优势。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我们从 0 到 1 建立过抖音背后的视频平台能力，这里面包括了数据处理、内容生成、安全问题处理、对视频内容的精准理解甚至全方位的广告场景，几乎所有与视频相关领域我们都有所涉猎。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "数据层面，我们的关键能力是，能够从海量数据中筛选出一小部分高质量数据来训练更优秀的模型，并且在安全问题上足够有经验。在抖音和 TikTok，每天都有海量视频上传，我们需要利用 AI 技术有效地整合和剔除低质量和重复性内容，并且防止用户生成不适当内容。处理这些问题的经验，让我们能够用更少的整体数据量训练模型，同时降低模型大小和 GPU 资源。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "算法层面，我们在多模态对齐、视频特征表示、时空建模以及主体控制上都有自己的创新。在多模态建模上，我们进行了大量 自监督学习 ，更充分利用动作型数据，特别是在处理未标注的视频数据方面，我们尝试了多种方法来建模那些标注噪声较大的数据集，这些尝试直接帮助我们解决动态建模的问题。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "特征表示上，我们在文字和视频内容的向 量化 做了很多尝试。时空建模方面，我们努力在训练过程中生成局部内容，同时让模型能够把握整体视野。生成中间某一帧时，模型应能够记住之前和之后的内容。在最优关键帧选择和动作建模质量上取得平衡。主体控制上，我们在关键帧生成、视频内容分割等方进行优化，帮助我们对视频性能控制更精准。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "工程方面，我们参考了之前在抖音操盘上万块 GPU 的经验，帮助在大规模集群训练和推理时的稳定性提升，并且复用了自动化的能力去应对数据分布变化问题。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：基于 “数据、算法和工程” 这三个要素，你认为你们实现了 “用更少资源取得了更优效果” 的成就，有没有具体数字可以说明这一点？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们的研发效率极高、迭代速度极快，Runway 成立了 5 年多时间，融资几亿美金，Pika 成立了近一年，融资大几千万美金。我们正式训练模型是在 2023 年下半年，花了 3 个月左右的时间就做到了全球第一梯队的水平，资源资金的消耗比 Runway、Pika 至少小了一个数量级。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "未来计划通过轻量产品吸引用户\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：目前 PixVerse 的策略是通过加速技术进步来取得优势，还是更多侧重于提高市场曝光度？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们目前观察到，像用户用 Pika、Runway 这些平台制作的视频在 YouTube 或 TikTok 等主流社交媒体上并没有太多播放量，很多 AI 视频生成 厂商目前的受众更多在服务一小部分 AI 发烧友。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "但我们的目标是希望技术能去找到具体的消费场景，满足实际需求，这里面需要用户对我们的技术信心，所以我们需要展示我们的技术能力，让用户愿意去使用。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你的意思是你们计划首先找到一个适合你们平台的应用场景，然后与创作者合作，优化这个场景，并通过这种方式吸引用户吗？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：我们首先确定的是，我们的平台不仅会提供技术，还会推出产品。技术只是起点，我们要解决的核心问题是如何利用这些技术创造的内容。我们已经有一些思路了。比如，帮创作者用《原神》中的人物进行高质量的二次创作。类似的场景尝试内部还有很多，我们在积极尝试，这部分产品主要面向 C 端用户。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你认为当前整体的 视频生成 赛道竞争局势如何？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：现在的竞争虽然已经开始，但真正激烈的阶段还未到来。我们发现目前大部分的用户只是停留在了解 AI 视频产品的阶段，并没有真正在使用产品去创作，这表明市场的增量仍然很大。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "不过，我们认为并非只有在技术完全成熟时才有商业化机会。即便我们目前的技术仅支持生成数秒的视频，但已经有用户在此基础上做出了大片级的作品。在这个阶段，我们正考虑哪些特性能更广泛地吸引 C 端消费者，使他们觉得产品既有趣又实用，并愿意去传播。这部分工作是目前我们的战略核心。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：你们对公司半年或者一年后的预期是什么？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：在未来 6-12 个月里，我们希望用 AI 制作出 15 秒长的可消费短视频。实际上，抖音刚开始时就是从 15 秒的视频开始的，所以我认为这样的长度足以承载丰富的信息供用户消费。我们希望这些内容是由 AI 生成的，同时也是用户感兴趣、愿意传播和浏览的。这些内容可能是单镜头拍摄，也可能是多个镜头组合的，但都能讲述故事并承载信息。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "未来，我们希望实现视频的实时秒级生成。我相信一旦做到这一点，将会对整个内容行业、视频行业带来巨大的颠覆。因为我们目前想到的都是存量的场景，而这将是一个全新的物种，带来许多增量的新体验和玩法，这些都是我们和同行未来需要一起探索和理解的。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：要达到这个 6-12 个月的目标，公司还需要哪些方面的进步？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：首先是视频生产的基础能力，继续提升准确性和一致性，目前的技术仍然存在一些瑕疵，我们希望继续改进。另外，我们希望能支持生成更长时间的视频。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "机器之心 ：视频内容如果实现秒级实时生成了，可能会发生什么？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "王长虎：现在人们消费视频的方式是在电影院观看相同的电影，或者在网上观看相同的剧集。但是 AI 视频生成 技术意味着未来我们可能实现秒级甚至实时的 视频生成 。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这种实时生成允许我们在视频播放时改变其某些元素，比如让观众成为视频中的主角，并且可以实时变化。这使得每个观看者都能与视频互动，参与到视频的发展过程中，每个人看到的内容都是不同的。这种技术能够理解每个人的喜好，并根据这些喜好定制化视频内容，就像创造一个平行宇宙一样。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "未来，我们获取信息的方式可能会变为推荐加生成结合的方式，每个人看到的视觉内容都会不一样。由于互联网上的信息已经高度视频化，这个领域的未来想象空间非常大，但这需要逐步实现，从一个模型应用开始，慢慢发展到更远大的目标。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-02-5\n",
      "title= 年龄两岁，教龄一年半：婴儿AI训练师登上Science\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= 只用 61 个小时的数据：人们终于证明了，利用当代 AI 工具，实现「真正的 语言学 习」是可行的。\n",
      "\n",
      "在公开采访中，图灵奖得主 Yann LeCun 多次提到，现在的 AI 模型和人类婴儿相比，学习效率实在是太低了。那么，如果让一个 AI 模型去学习婴儿头戴摄像头拍到的东西，它能学到什么？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最近，Science 杂志上的一篇论文进行了初步尝试。研究发现，即使数据有限，AI 模型也能从 10 到 100 个例子中学到单词 - 视觉所指对象之间的 映射 ，而且能够零样本地泛化到新的视觉数据集，并实现多模态对齐。这说明，利用当今的 人工智能 工具，从婴儿的视角进行真正的 语言学 习是可能的。\n",
      "\n",
      "年龄两岁，教龄 1 年半\n",
      "\n",
      "Sam 是怎么教 AI 学习的？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这一次， 人工智能 通过婴儿的视角看世界来学习语言。\n",
      "\n",
      "神经网络 通过人类婴儿的视觉经验，自行学会了识别物体，这为人类学习提供了新的见解。\n",
      "\n",
      "AI 通过 Sam 佩戴的头盔式摄像机所拍摄的音视频学习。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "当婴儿听到「球」这个词时，他们是如何将这个词的语义与圆形、有弹性的物体（即正确的视觉所指对象）联系起来的呢？哲学家和认知科学家都认为，婴儿在学习新词时，需要从众多候选意项中挑出正确的那一个。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "婴儿非常擅长学习词汇。在 6 到 9 个月大的时候，他们开始将单词与眼前的物体建立起音形义的联系。到 18 到 24 个月大的时候，他们已经能理解约 300 个单词。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "那么，孩子们是如何快速学会眼前物体的名称的呢？他们又是如何建立起物体的意义和其视觉之间的联系呢？这些问题都需要进一步的探索和研究。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "此前，已有一些相关理论在实验中得到了验证。有学者认为单词学习是由简单的、能串联起各领域的联想学习机制驱动的。但是这些理论通常是在婴儿不同的成长时间段测量的，不能揭示某种促进单词学习因素的相对重要性，也不能从中构建计算模型、为计算机模型能获得像人一样的学习能力提供指导。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如果一个模型能够通过孩子的眼睛和耳朵 感知 世界，那么它是否像解释人类词汇学习能力的联想学习理论一样，能够仅通过基于物体表征的联想学习，理解并整合物体的形体和语义呢？或者，它是否需要借助其他的认知能力，比如归纳偏置（inductive biases），来启动这种能力呢？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了得到这些问题的答案，来自纽约大学的研究者们对最简单的词汇学习理论进行了前所未有的测试：他们给一个婴儿戴上了头戴式摄像机，并检查模型是否能够从这部摄像机的视频记录中学习到单词与其视觉所指对象之间的 映射 关系。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "戴上摄像机的是来自澳大利亚的 Sam，从 6 个月大到大约 2 岁，他每周头戴摄像机两小时（约占清醒时间的 1%）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究团队根据 Sam 的视频建立了 SAYCam-S 数据集。他们从中选取了 61 个小时的录像，其中包含 60 万张视频帧与 3.75 万段经过转写的录音，记录了大约 25 万个单词实例以及对应的图像。这些图像是 Sam 在玩耍、阅读和进食等活动期间拍摄的。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究团队根据这些数据来训练 神经网络 ，并得到了儿童视角对比学习模型 CVCL。CVCL 采用了对比学习的技术，以学习哪些图像和文本经常一起出现，哪些不会，从而获得预测某些词汇（如 “球” 和 “碗”）所指代图像的能力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究发现，CVCL 可以从一个孩子有限的经验片段中充分学习多模态表示。CVCL 能够将一系列日常词汇与分类任务中相应的视觉所指对象匹配起来，大规模对齐视觉和语言概念，并将此能力泛化到训练中未见过的新例子中。该研究表明，多模态表征学习与领域通用的联想学习机制相结合，能够为计算机学习单词带来突破。\n",
      "\n",
      "具体来说，研究者根据多模态模型研究的最新进展设计了 CVCL。CVCL 整合了表示学习和联想学习，用一个对比目标来协调视觉编码器和语言编码器两个 神经网络 。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如图 1 所示，对比目标以自我监督的方式进行训练（即只使用儿童视角的记录，不使用外部标注），模型将目标在视频帧和语言片段共同出现的情况转化为向量提取出来，将其视为正面例子，同时将不共同出现的转化成向量分离出来，视为隐含的负面例子。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "提取到正面例子后，CVCL 将这些时间向量转换为学习和调整多模态表征的学习信号。这种方法既不需要对词义进行限制，也不需要预先列出可能的视觉所指对象，能从婴儿记录的视频中恢复许多基本的单词与其视觉所指对象的组合。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "评估 CVCL 获得的词汇\n",
      "\n",
      "对应视觉所指对象的结果\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "训练完成后，研究团队评估了 CVCL 以及各种类似的模型学习到的单词 - 视觉所指对象组合的质量。根据一种针对儿童的常见测试，研究团队向模型提示了一个目标类别标签，让模型根据四个候选图像与标签的余弦相似度中选择相应的视觉所指对象。\n",
      "\n",
      "图 2A 显示了标签 S 的测试结果，总体而言，CVCL 的分类 准确率 为 61.6%。图 2D 显示了模型在不同标签中的具体结果，在 22 个概念中，CVCL 对 11 个概念的判断与 CLIP 相差不到 5%。但 CLIP 训练所用的数据量（互联网的 4 亿个图像文本对）远超于 CVCL。为了解决分类重叠等潜在问题，研究团队还手动筛选出了子集进行了后续评估。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了确定 CVCL 捕捉单词含义能力的上限和下限，研究团队还将其与类似模型进行了实验。为了测试模型将语言和视觉信息对应起来的能力，研究团队将原数据集中共同出现目标物体的视频帧和录音打乱，重新训练了一个模型的变体 CVCL-Shuffled。被打乱后的模型表现不佳，这显示了视觉和语言信息共现对模型学习的关键作用。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了测试视觉嵌入的有效性，研究者在训练过程中随机冻结了 CVCL 的视觉编码器。尽管模型掌握了如 「沙子 」和 「汽车 」等少数概念，但如图 2D 处所示，模型的成绩再次大幅下降（M = 38.0%）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究者比较了 CVCL 与基于其他数据或 Oracle 训练数据的 AI 模型，其他模型的训练数据超出了儿童词汇的范围。CLIP 的 准确率 达 66.7%，比 CVCL 高出 5.1%，这得益于 CLIP 更理解少数单词的含义如「厨房」、「玩具」和「篮子」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "通过以上测试，可见当在一定范围内测试时，CVCL 的性能可以与基于互联网规模数据训练的模型相当。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "此外，研究者测试了模型是否能独立对单词进行分类，而不是根据某些引导儿童的句子得出了判断。他们在初始化的预训练编码器上对 线性分类器 进行拟合得到了一个 Linear Probe 模型，新模型 准确率 达 81.6% ，说明 CVCL 具有独立判断能力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究团队 量化 了在对话中自然出现的单词相对直接标记示例对模型训练的价值。如图 2B 所示，他们使用更少的人工标注数据（使用打过标签数据的 10% 和 1%）训练了两个 Linear Probe 模型，测试结果如下表所示。\n",
      "\n",
      "减少了人工标注数据的 Linear Probe 模型，分类准确度分别下降到了 77.2% 和 65.9%。使用了 1% 的标注示例的模型性能略好于 CVCL。通过比较，可以保守估计一个人工标注的至少相当于来自自然语言的七个示例。不过，来自自然语言的数据能更加灵活、更准确地表示儿童学习的内容，并且它可以容纳无限数量的视觉概念。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了研究是否有其他因素影响了单词 - 视觉所指对象组合的可学习性，研究团队还训练了 CVCL 模型的其他变体以作评估。他们改变了模型结构或训练过程的各个方面，但没有一个变体的表现优于 CVCL 本身。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "综上所述，研究结果表明，人类最初习得的的单词-视觉所指对象组合可以从 10 到 100 个自然出现的单词-视觉所指对象组合中获得。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "泛化至全新的视觉实例\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了测试 CVCL 的泛化能力，研究团队在 Konkle Objects 数据集上进行了实验。\n",
      "\n",
      "从研究婴儿 语言学 习的实验中获得了灵感，研究团队为 CVCL 提供了 64 个额外的在白色背景上的单个物体图像，其对应的单词都在 CVCL 的词汇表中。这个实验使得研究团队能够检查 CVCL 学习的单词是否能成功泛化到未见过的物体中。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如图 3A 所示，CVCL 具有一定的泛化能力，在 64 个物体中有 16 个得分高于 50%（正确），另外 42 个概念得分高于 25%（偶然），整体 准确率 为 34.7%。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "此外，两个 CVCL 的模型变体都接近偶然 准确率 （CVCL-Shuffled 和 CVCL-Random Features 模型的 准确率 分别为 25.6% 和 23.4%），而其最佳表现都接近目前 SOTA 方法（CLIP 和 Linear Probe 模型的 准确率 分别为 99.4% 和 90.7%）。\n",
      "\n",
      "这些结果表明了 CVCL 的多模态表征如何允许分布之外的泛化 —— 与该能力其他更大规模的演示一致。为了说明这次评估所需的视觉泛化的程度，图 3B 展示了嵌入在话语中的单词的一些自然训练实例（从孩子的视角），与用于评估的新颖测试图像相匹配（以及它们的分类准确度）。此外，这次评估与经典婴儿词汇学习实验中呈现的刺激类型非常相似，这表明在实验室外获得的表现足以解释婴儿如何将实验室内的视觉刺激泛化到新的视觉刺激。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "多模态表征的组织结构\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最后，研究者介绍了 CVCL 中学习到的多模态表征结构的三个分析家族。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先探索的问题是，CVCL 的视觉和语言概念系统在多大程度上是一致的。例如，如果「汽车」的视觉和 词嵌入 都独立地更类似于「道路」而不是「球」，将表明良好的多模态对齐。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "使用 Labeled-S 中的 22 个概念，研究者通过随机抽取 100 个注释帧，提取其图像嵌入并跨帧平均计算每个概念的视觉原型。他们还检索了每个概念相应的 词嵌入 。接下来，计算这些嵌入之间的所有余弦相似度（包括模态内和模态间）并使用 t - 分布随机邻居嵌入（t-SNE）可视化它们之间的关系，如图 4A 和 B 所示。在图 4A 中，虚线表示每个概念相应的视觉质心和 词嵌入 之间的距离。\n",
      "\n",
      "由于这些跨模态距离中的许多都很小，研究者检查了概念之间的模态内相似性（通过余弦）是否与视觉和语言相关，发现了概念对齐的显著程度（相关系数 r = 0.37，p < 0.001）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这些关系不适用于 CVCL 的两个下界中的任何一个（图 S4）。此外，对齐距离也与分类性能呈强烈负相关（r = -0.65，p = 0.001），一些最不准确的类别表现出各自视觉原型和 词嵌入 之间的最大距离。图 4B 展示了每个概念的带标签图像嵌入的子集，强调不同的视觉概念在示例的紧密 聚类 程度方面存在差异。通过将视觉变化视为概念视觉嵌入与其视觉原型之间的平均 欧几里得距离 ，研究者还发现与分类性能的强烈负相关（r = -0.48，p = 0.025），这表明 CVCL 在处理「手」和「玩具」等单词参照 映射 时的难度与它们的视觉变化有关，与紧密 聚类 的概念如「汽车」和「婴儿床」相比。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "接下来，研究者可视化了在 CVCL 中不同的 词嵌入 如何与图像嵌入相互作用（图 4C）。检查三个不同的概念，他们观察到模型预测与特定 词嵌入 最相似的图像（以绿色显示）与每个类别的真实标注图像集（以蓝色显示）非常接近，完整概念集显示在图 S6 中。研究者发现 CVCL 学习将不同视觉相似的项目集合表示为一个概念的不同子簇，尽管每个词只使用一个向量。例如，「楼梯」的 词嵌入 最强烈地激活两个独立的集群，分别代表室内和室外楼梯，而「拼图」产生另外两个集群，代表字母和动物拼图。以前的 概念学习 心理理论通常需要明确、内置的机制来捕捉概念内部的子结构，但在 CVCL 中，我们发现多簇表示通过对比学习隐式地出现。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究者还定性检查了 CVCL 定位指代的能力。对于给定的图像，通过应用 Grad-CAM 获得一个注意力图，通过计算最终卷积层特征图的加权和（使用基于图像文本余弦相似度梯度相对于特征图的空域平均值的 权重 ），突出显示与目标类别最相关的图像区域。研究者可以将此注意力图叠加在图像上，并检查指代的位置与注意力图之间的任何对应关系。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "图 5 展示了四个概念中多个注意力图的示例。对于某些类别，CVCL 的注意力图提供了物体定位的证据：注意力图中最高激活的区域紧密跟踪指代的定位。\n",
      "\n",
      "更多研究细节，可参考原论文。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2020-09-29-9\n",
      "title= 机器之心A100数智中国榜发布：让中小企业看懂、选对、用好数智化服务方案（附研究报告）\n",
      "author= []\n",
      "publish_date= 2020-09-29 00:00:00\n",
      "text= 行业方案榜：TOP数智+交通运输\n",
      "\n",
      "方案名称 服务商名称 入选理由\n",
      "\n",
      "ET航空大脑 阿里巴巴 集团 阿里云 是 阿里巴巴 旗下全球领先的 云计算 及 人工智能 公司，在财务资本方面具有雄厚的实力，自从成立以来一直强调多远化发展，其云服务已基本涵盖各行各业。它推出的ET航空大脑，涵盖大数据、 运筹优化 、路基 规划 等多种创新性技术，并聚焦于停机位分配、班组排版、餐补 规划 以及客流态式监控等多个航空运输聚焦领域。它已与数十个不同数智化技术中小微企业构建合作关系，并引入其相关优势技术。目前，该ET航空大脑已在北京首都国际机场落地，让1700个停机位安排可在50秒内完成 规划 ，总计节省约5000小时服务时长。\n",
      "\n",
      "百度 地图智慧交通解决方案 北京 百度 网讯科技有限公司 百度 是全球领先的互联网、 云计算 与 人工智能 上市公司，其在财务资本方面具有雄厚实力，自从成立以来已参与或建立多个国家级与省市级行业标准。它推出的 百度 地图智慧交通涵盖 人工智能 、大数据、GIS等多项创新性技术，并聚焦于交通服务行业的车流监控、路况分析、假日出行等多个常见应用场景。目前，该平台已与广播平台、研究机构等多行业数百家企业展开合作，平台开发者数量已接近180万，服务应用50万个，并有近1.5亿C端用户下载量。\n",
      "\n",
      "砖头物流管理系统 北京乐卡车联科技有限公司 北京乐卡车联科技有限公司是以车联万物为理念的智能车载系统硬件供应商，在财务资本方面实力有限，已完成A+轮融资，自从成立以来在从安全、效率、娱乐构建了完成的智能车载体系。它推出乐卡车联解决方案涵盖 人工智能 、大数据等多项创新性技术，支持经营管理等多个数智化场景应用，并专注于服务中小微交通运输业热门领域。它已服务千余万份订单，并与顺丰、德邦等知名品牌取得合作，且具有较为成熟的方案售后服务体系。\n",
      "\n",
      "大华智慧交通解决方案 大华股份集团 大华技术股份有限公司是全球领先的智慧物联网解决方案供应商和运营服务商。它推出的智慧交通解决方案涵盖 人工智能 、大数据、 计算机视觉 等创新技术，可覆盖高速公路、交通运输、铁路、城市轨道、港口等多个常见交通行业领域。该解决方案已覆盖全球近180个国家，并设立超过53个分支机构以进行售后服务支持。\n",
      "\n",
      "滴滴智慧交通解决方案 滴滴出行 滴滴出行是全球卓越的移动出行平台服务商，在全球范围内以向超过5.5亿C端用户提供过出行、外卖以及支付相关多元化服务。它推出的滴滴智慧交通解决方案融合了互联网、大数据、物联网、 运筹优化 等多项创新性技术，可覆盖拥堵分析、驾驶检测、潮汐车道 规划 、公交 调度 等多项交通热门领域应用场景。该解决方案已有近4875TB相关数据，可为政府、企业、个人提供高质量的智慧交通出行服务。\n",
      "\n",
      "智慧交通解决方案 华为 技术有限公司 华为 在财务资本方面具有雄厚实力，并是全球领先的通信服务寡头公司，自从成立以来获得多次国家级科技进步奖以及省市级科技奖励，并自主建立多项业内技术标准。它推出的智慧交通解决方案涵盖 云计算 、大数据、物联网、敏捷网络、BYOD、 5G 等多项创新技术，可围绕铁路运营、港口管理、机场运营等多场景提供一站式的解决方案。 华为 智慧交通解决方案所构建的深圳城市大脑项目，获得世界智慧城市博览会平安城市专项大奖。 华为 还计划推进该方案尝试与更多道路、汽车相关中小微企业取得合作，完善智慧交通版图。\n",
      "\n",
      "京东 物流 京东 物流集团 大华技术股份有限公司是全球领先的智慧物联网解决方案供应商和运营服务商。它推出的智慧交通解决方案涵盖 人工智能 、大数据、 计算机视觉 等创新技术，可覆盖高速公路、交通运输、铁路、城市轨道、港口等多个常见交通行业领域。该解决方案已覆盖全球近180个国家，并设立超过53个分支机构以进行售后服务支持。\n",
      "\n",
      "龙邦快运 龙邦物流有限公司 京东 物流隶属于 京东 集团，旨在通过开放、智能化的战略促进消费方式的转变与社会供应链效率的提升，将物流、商流、资金流和信息流有机结合，打造体验最优的物流履约解决方案。它推出的 京东 物流一体化解决方案涵盖大数据、 机器学习 、 运筹优化 、无人机、机器人等多种创新性技术，可实现库存共享、无人派送、订单集成处理、仓配一体以及极速达等多种交通运输领域常见服务。该及决方案已覆盖近200+城市，并针对中小微企业覆盖较广的服饰、消费品、母婴等热门行业打造了从仓储到配送、从线上到线下，从硬件到软件，从原材料采购至分销供应链的个性化解决方案。\n",
      "\n",
      "千方科技智慧交通解决方案 千方科技 千方科技是智慧交通领域客户解决方案提供商，聚焦于推动智慧交通、智能物联行业发展。它推出的千方科技智慧交通解决方案以大数据、 人工智能 、 云计算 为基础，构建了智慧交通与智慧物联双引擎，可服务运输、ETC、交通大数据、民航、轨交、高速等多个交通领域热门场景。该解决方案已形成从硬件基础设施到软件智慧中枢的完成产业链。目前，该解决方案已在2018年为公司带来近34.93亿元收入，同比增长达到近20%。\n",
      "\n",
      "货运中国网互联网+物流SAAS云平台 上海成达智慧物流有限公司 成达信息科技是互联网物流运营的先行者，在财务资本方面有一定实力，已完成Pre-A轮融资，自从成立以来推出多种针对传统物流进行优化的数智化产品。它推出的货运中国网络平台涵盖大数据、 人工智能 、 机器学习 等多项创新性技术，支持销售营销等多个数智化场景应用，并已在国内建立20个城市据点，服务20万辆汽车车主，且具有较为成熟的方案售后服务体系。\n",
      "\n",
      "华宇开放平台 上海华振物流有限公司 盛丰物流集团有限公司是一家专注于国内干线运输、货物仓储、物流配送、物流解决方案策划与设计的国家5A级综合物流企业，自从成立以来公司以福州为总部建立了280家分公司与仓库，自有货车近8000辆，全国公路零担快运第五名。它推出的盛丰物流涵盖大数据、物联网、 运筹优化 等多种创新性技术，服务 调度 规划 、仓储包装、加工搬运等多个常见交通运输领域，并为其供应链产业提供信息化数据共享增值服务。该物流已与近2000家制造业中小微企业取得物流合作，认证伙伴超过10000家。\n",
      "\n",
      "运去哪一站式国际物流在线服务平台 上海汇航捷讯网络科技有限公司 上海汇航捷讯网络科技有限公司是以物流及外贸行业为具体应用对象的新型B2B互联网企业，在财务资本方面实力有限，自从成立以来多次获得无人货运相关奖项，并构建了多种智能运输货运产品。它推出的运去哪涵盖 人工智能 、自然语言处理等多项创新性技术，支持销售营销等多个数智化场景应用，并专注于服务交通运输热门领域。它已服务1万家+中小微客户遍布全球15万+航线，且具有成熟的方案售后服务体系。\n",
      "\n",
      "佳吉快运 上海佳吉快运有限公司 滴滴出行是全球卓越的移动出行平台服务商，在全球范围内以向超过5.5亿C端用户提供过出行、外卖以及支付相关多元化服务。它推出的滴滴智慧交通解决方案融合了互联网、大数据、物联网、 运筹优化 等多项创新性技术，可覆盖拥堵分析、驾驶检测、潮汐车道 规划 、公交 调度 等多项交通热门领域应用场景。该解决方案已有近4875TB相关数据，可为政府、企业、个人提供高质量的智慧交通出行服务。\n",
      "\n",
      "智能交通解决方案 深圳市 腾讯 计算机系统有限公司 腾讯 是全球领先的互联网、 云计算 与 人工智能 上市公司，其在财务资本方面具有雄厚实力，自从成立以来已参与或建立数百个国家级与省市级行业标准。它推出的 腾讯 智能交通解决方案涵盖 人工智能 、大数据、 计算机视觉 、自然语言处理、 机器学习 、知识表征等多项创新性技术，并聚焦于交通路况、公共出行、事故处理、违章缴罚等多个交通行业服务领域。目前，该解决方案已与上万销售代理商、软件开发商以及解决方案商建立合作关系，并开始契合 腾讯 We Transport大战略帮助交通行业中小微企业进行数智化转型。\n",
      "\n",
      "盛丰物流 盛丰物流集团有限公司 上海佳吉快运有限公司是国家认证的5A级物流企业，公司主营公路零担运输业务，并使始终重视数智化企业服务能力的提升与货运信息化网络的建设。它推出的佳吉快运服务涵盖GPS、物联网、大数据、 运筹优化 等多项创新性技术，可囊括货车定位、配送监控以及智能 调度 等多项交通运输领域常见应用场景。目前该服务已在全国范围内拥有近2500多个服务网点，17个大型转运中心、信息化网络服务能力达到所有省级行政区。\n",
      "\n",
      "顺丰速运 顺丰速运 龙邦物流有限公司成立于2012年，是“龙邦供应链”旗下一家以物流供应链管理为核心，布局全国物流网络运营、互联网技术研发的创新性企业。它推出的龙邦快运利用大数据、物理网、 运筹优化 等技术，对零担物流、专线资源实现网络化监控、信息化运营，为中小微企业提供全国快运门对门一站式综合物流及决方案。目前该方案已在全球范围内拥有分拨中心81个，服务网点3000余家以及配送车辆8000余台。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2020-09-29-9#comment\n",
      "title= 机器之心A100数智中国榜发布：让中小企业看懂、选对、用好数智化服务方案（附研究报告）\n",
      "author= []\n",
      "publish_date= 2020-09-29 00:00:00\n",
      "text= 行业方案榜：TOP数智+交通运输\n",
      "\n",
      "方案名称 服务商名称 入选理由\n",
      "\n",
      "ET航空大脑 阿里巴巴 集团 阿里云 是 阿里巴巴 旗下全球领先的 云计算 及 人工智能 公司，在财务资本方面具有雄厚的实力，自从成立以来一直强调多远化发展，其云服务已基本涵盖各行各业。它推出的ET航空大脑，涵盖大数据、 运筹优化 、路基 规划 等多种创新性技术，并聚焦于停机位分配、班组排版、餐补 规划 以及客流态式监控等多个航空运输聚焦领域。它已与数十个不同数智化技术中小微企业构建合作关系，并引入其相关优势技术。目前，该ET航空大脑已在北京首都国际机场落地，让1700个停机位安排可在50秒内完成 规划 ，总计节省约5000小时服务时长。\n",
      "\n",
      "百度 地图智慧交通解决方案 北京 百度 网讯科技有限公司 百度 是全球领先的互联网、 云计算 与 人工智能 上市公司，其在财务资本方面具有雄厚实力，自从成立以来已参与或建立多个国家级与省市级行业标准。它推出的 百度 地图智慧交通涵盖 人工智能 、大数据、GIS等多项创新性技术，并聚焦于交通服务行业的车流监控、路况分析、假日出行等多个常见应用场景。目前，该平台已与广播平台、研究机构等多行业数百家企业展开合作，平台开发者数量已接近180万，服务应用50万个，并有近1.5亿C端用户下载量。\n",
      "\n",
      "砖头物流管理系统 北京乐卡车联科技有限公司 北京乐卡车联科技有限公司是以车联万物为理念的智能车载系统硬件供应商，在财务资本方面实力有限，已完成A+轮融资，自从成立以来在从安全、效率、娱乐构建了完成的智能车载体系。它推出乐卡车联解决方案涵盖 人工智能 、大数据等多项创新性技术，支持经营管理等多个数智化场景应用，并专注于服务中小微交通运输业热门领域。它已服务千余万份订单，并与顺丰、德邦等知名品牌取得合作，且具有较为成熟的方案售后服务体系。\n",
      "\n",
      "大华智慧交通解决方案 大华股份集团 大华技术股份有限公司是全球领先的智慧物联网解决方案供应商和运营服务商。它推出的智慧交通解决方案涵盖 人工智能 、大数据、 计算机视觉 等创新技术，可覆盖高速公路、交通运输、铁路、城市轨道、港口等多个常见交通行业领域。该解决方案已覆盖全球近180个国家，并设立超过53个分支机构以进行售后服务支持。\n",
      "\n",
      "滴滴智慧交通解决方案 滴滴出行 滴滴出行是全球卓越的移动出行平台服务商，在全球范围内以向超过5.5亿C端用户提供过出行、外卖以及支付相关多元化服务。它推出的滴滴智慧交通解决方案融合了互联网、大数据、物联网、 运筹优化 等多项创新性技术，可覆盖拥堵分析、驾驶检测、潮汐车道 规划 、公交 调度 等多项交通热门领域应用场景。该解决方案已有近4875TB相关数据，可为政府、企业、个人提供高质量的智慧交通出行服务。\n",
      "\n",
      "智慧交通解决方案 华为 技术有限公司 华为 在财务资本方面具有雄厚实力，并是全球领先的通信服务寡头公司，自从成立以来获得多次国家级科技进步奖以及省市级科技奖励，并自主建立多项业内技术标准。它推出的智慧交通解决方案涵盖 云计算 、大数据、物联网、敏捷网络、BYOD、 5G 等多项创新技术，可围绕铁路运营、港口管理、机场运营等多场景提供一站式的解决方案。 华为 智慧交通解决方案所构建的深圳城市大脑项目，获得世界智慧城市博览会平安城市专项大奖。 华为 还计划推进该方案尝试与更多道路、汽车相关中小微企业取得合作，完善智慧交通版图。\n",
      "\n",
      "京东 物流 京东 物流集团 大华技术股份有限公司是全球领先的智慧物联网解决方案供应商和运营服务商。它推出的智慧交通解决方案涵盖 人工智能 、大数据、 计算机视觉 等创新技术，可覆盖高速公路、交通运输、铁路、城市轨道、港口等多个常见交通行业领域。该解决方案已覆盖全球近180个国家，并设立超过53个分支机构以进行售后服务支持。\n",
      "\n",
      "龙邦快运 龙邦物流有限公司 京东 物流隶属于 京东 集团，旨在通过开放、智能化的战略促进消费方式的转变与社会供应链效率的提升，将物流、商流、资金流和信息流有机结合，打造体验最优的物流履约解决方案。它推出的 京东 物流一体化解决方案涵盖大数据、 机器学习 、 运筹优化 、无人机、机器人等多种创新性技术，可实现库存共享、无人派送、订单集成处理、仓配一体以及极速达等多种交通运输领域常见服务。该及决方案已覆盖近200+城市，并针对中小微企业覆盖较广的服饰、消费品、母婴等热门行业打造了从仓储到配送、从线上到线下，从硬件到软件，从原材料采购至分销供应链的个性化解决方案。\n",
      "\n",
      "千方科技智慧交通解决方案 千方科技 千方科技是智慧交通领域客户解决方案提供商，聚焦于推动智慧交通、智能物联行业发展。它推出的千方科技智慧交通解决方案以大数据、 人工智能 、 云计算 为基础，构建了智慧交通与智慧物联双引擎，可服务运输、ETC、交通大数据、民航、轨交、高速等多个交通领域热门场景。该解决方案已形成从硬件基础设施到软件智慧中枢的完成产业链。目前，该解决方案已在2018年为公司带来近34.93亿元收入，同比增长达到近20%。\n",
      "\n",
      "货运中国网互联网+物流SAAS云平台 上海成达智慧物流有限公司 成达信息科技是互联网物流运营的先行者，在财务资本方面有一定实力，已完成Pre-A轮融资，自从成立以来推出多种针对传统物流进行优化的数智化产品。它推出的货运中国网络平台涵盖大数据、 人工智能 、 机器学习 等多项创新性技术，支持销售营销等多个数智化场景应用，并已在国内建立20个城市据点，服务20万辆汽车车主，且具有较为成熟的方案售后服务体系。\n",
      "\n",
      "华宇开放平台 上海华振物流有限公司 盛丰物流集团有限公司是一家专注于国内干线运输、货物仓储、物流配送、物流解决方案策划与设计的国家5A级综合物流企业，自从成立以来公司以福州为总部建立了280家分公司与仓库，自有货车近8000辆，全国公路零担快运第五名。它推出的盛丰物流涵盖大数据、物联网、 运筹优化 等多种创新性技术，服务 调度 规划 、仓储包装、加工搬运等多个常见交通运输领域，并为其供应链产业提供信息化数据共享增值服务。该物流已与近2000家制造业中小微企业取得物流合作，认证伙伴超过10000家。\n",
      "\n",
      "运去哪一站式国际物流在线服务平台 上海汇航捷讯网络科技有限公司 上海汇航捷讯网络科技有限公司是以物流及外贸行业为具体应用对象的新型B2B互联网企业，在财务资本方面实力有限，自从成立以来多次获得无人货运相关奖项，并构建了多种智能运输货运产品。它推出的运去哪涵盖 人工智能 、自然语言处理等多项创新性技术，支持销售营销等多个数智化场景应用，并专注于服务交通运输热门领域。它已服务1万家+中小微客户遍布全球15万+航线，且具有成熟的方案售后服务体系。\n",
      "\n",
      "佳吉快运 上海佳吉快运有限公司 滴滴出行是全球卓越的移动出行平台服务商，在全球范围内以向超过5.5亿C端用户提供过出行、外卖以及支付相关多元化服务。它推出的滴滴智慧交通解决方案融合了互联网、大数据、物联网、 运筹优化 等多项创新性技术，可覆盖拥堵分析、驾驶检测、潮汐车道 规划 、公交 调度 等多项交通热门领域应用场景。该解决方案已有近4875TB相关数据，可为政府、企业、个人提供高质量的智慧交通出行服务。\n",
      "\n",
      "智能交通解决方案 深圳市 腾讯 计算机系统有限公司 腾讯 是全球领先的互联网、 云计算 与 人工智能 上市公司，其在财务资本方面具有雄厚实力，自从成立以来已参与或建立数百个国家级与省市级行业标准。它推出的 腾讯 智能交通解决方案涵盖 人工智能 、大数据、 计算机视觉 、自然语言处理、 机器学习 、知识表征等多项创新性技术，并聚焦于交通路况、公共出行、事故处理、违章缴罚等多个交通行业服务领域。目前，该解决方案已与上万销售代理商、软件开发商以及解决方案商建立合作关系，并开始契合 腾讯 We Transport大战略帮助交通行业中小微企业进行数智化转型。\n",
      "\n",
      "盛丰物流 盛丰物流集团有限公司 上海佳吉快运有限公司是国家认证的5A级物流企业，公司主营公路零担运输业务，并使始终重视数智化企业服务能力的提升与货运信息化网络的建设。它推出的佳吉快运服务涵盖GPS、物联网、大数据、 运筹优化 等多项创新性技术，可囊括货车定位、配送监控以及智能 调度 等多项交通运输领域常见应用场景。目前该服务已在全国范围内拥有近2500多个服务网点，17个大型转运中心、信息化网络服务能力达到所有省级行政区。\n",
      "\n",
      "顺丰速运 顺丰速运 龙邦物流有限公司成立于2012年，是“龙邦供应链”旗下一家以物流供应链管理为核心，布局全国物流网络运营、互联网技术研发的创新性企业。它推出的龙邦快运利用大数据、物理网、 运筹优化 等技术，对零担物流、专线资源实现网络化监控、信息化运营，为中小微企业提供全国快运门对门一站式综合物流及决方案。目前该方案已在全球范围内拥有分拨中心81个，服务网点3000余家以及配送车辆8000余台。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-01-8\n",
      "title= 小红书开源「InstantID」效果炸裂，被Yann LeCun点赞，迅速蹿上Github热榜\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 只需一张照片，整个过程无需训练 LoRA 模型，多风格 AI 写真即刻呈现！\n",
      "\n",
      "最近，有一群来自小红书的 95 后神秘团队，自称 InstantX，搞了个大动作 —— 开源「InstantID」项目。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 凭借着高质量的 图像生成 能力，在开源界掀起了一股热潮：不仅获得了众多技术大佬的点赞，更是在 GitHub 热榜上迅速飙升，成为焦点。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个「出片神器」，让用户只需上传一张照片，就能轻松定制出多种风格的 AI 写真。\n",
      "\n",
      "对，你没看错。如图左侧所示，与之前爆火的妙鸭相机至少需要上传 20 张照片不同的是，InstantID 只需一张自拍，不依赖模型训练，不需要等待，瞬间变身。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "无论是古典油画的优雅，炫酷的赛博朋克，或是 3D 雕像的立体感，只要是你喜欢的风格，InstantID 都能轻松驾驭。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "它不仅风格多样，还能在保持人物面部高保真的同时，无需模型训练，实现秒级出图，效率大幅提升。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 目前位列 Hugging Face Space Trending 榜首，许多小伙伴玩得不亦乐乎～\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如，把马斯克送上了火星。\n",
      "\n",
      "让蒙娜丽莎拍「樱花写真」，微笑依旧很神秘。\n",
      "\n",
      "甚至可以让语文课本中的杜甫从二维变三维，穿越到现代变身「帅大叔」。\n",
      "\n",
      "图灵奖得主 Yann LeCun，化身多种动漫人物，你猜出了几个角色？\n",
      "\n",
      "就连 Yann LeCun 本人也点赞转发，调侃自己的「钢铁侠」衣服在哪里。\n",
      "\n",
      "在个性化图像合成领域，实现强烈风格化写真的同时保持面部高保真度，一直是个挑战。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从效果上看，InstantID 做到了。那它背后运用了哪些方法，有什么独到之处吗？\n",
      "\n",
      "回顾过去，尽管 Textual Inversion、 DreamBooth 和 LoRAs 等技术已经取得了重大进展。但它们在实际应用中仍受限于高存储需求、耗时的微调过程以及对多张参考图像的依赖。相比之下，现有基于 ID 嵌入的方法虽然只需一次前向推理，但也面临不小挑战：要么需要对大量模型 参数 进行广泛的微调，要么与社区预训练模型不兼容，要么无法保持高真实性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 的出现，打破了这些局限。小红书 InstantX 团队公开了论文《 InstantID: Zero-shot Identity-Preserving Generation in Seconds 》和推理代码，他们表示：InstantID 巧妙地避免了对文生图模型 UNet 部分的训练，仅通过训练一个轻量级的可插拔模块，实现了在推理过程中无需 test-time tuning，同时保持了文本控制的灵活性，确保了面部特征的高保真度。\n",
      "\n",
      "如图所示，InstantID 的工作原理可分为三个关键部分：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ID Embedding：团队利用预训练的面部识别模型代替 CLIP 来提取语义人脸特征，并使用可训练的投影层，将这些特征 映射 到文本特征空间，形成 Face Embedding，具有丰富的语义信息，包括如面部特征、表情、年龄等，为后续的 图像生成 提供了坚实的基础。\n",
      "\n",
      "Image Adapter：引入一个轻量级的适配模块，将提取的身份信息与文本提示结合起来。这个模块通过解耦的交叉 注意力机制 ，使得图像和文本能够独立地影响生成过程，从而在保持身份信息的同时，允许用户对图像风格进行精细控制，实现「双赢」。\n",
      "\n",
      "IdentityNet：小红书提出了一个名为 IdentityNet 的网络，是 InstantID 的核心部分。它通过强语义条件（如面部特征的详细描述）和弱空间条件（如面部关键点的位置）来编码参考面部图像的复杂特征。在 IdentityNet 中，生成过程完全由 Face Embedding 引导，无需任何文本信息。仅更新新添加的模块，而预先训练的文本到图像模型保持冻结以确保灵活性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在实际的 图像生成 过程中，InstantID 首先会接收到用户的文本提示和面部图像。然后通过 ID Embedding 提取关键信息，接着 Image Adapter 将这些信息与文本提示融合。IdentityNet 会根据这些融合后的信息生成图像。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "整个过程是自动化的，用户不需要进行任何额外的微调或训练，只需等待二十几秒，就能得到一个既符合文本描述又保留个人身份特征的定制图像。\n",
      "\n",
      "InstantID 不仅解决了训练效率与身份保真度之间的平衡问题，还提供了一系列令人印象深刻的特性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先，InstantID 的即插即用和兼容性是其最大的卖点之一。它无需对 UNet 进行额外训练，即可与现有的预训练模型无缝集成，如社区内的文生图基础模型、LoRAs 和 ControlNets。这意味着用户可以在不增加成本的情况下，轻松地在推理过程中保持人物的身份特征，裂变性强。\n",
      "\n",
      "其次，InstantID 的无需微调特性，使得它在实际应用中极具经济性和实用性。用户只需进行一次前向传播，即可快速生成图像，同时保持对文本编辑的强大控制力，让身份信息与各种风格完美融合。如下图所示，其编辑性强的特点让用户能够通过文本控制性别、头发、服装等细节，确保生成图像的多样性。\n",
      "\n",
      "性能方面的表现同样卓越，它能够仅凭一张参考图像，就生成具有高保真度和灵活性的先进结果。这一性能不仅超越了基于单张图片特征的嵌入方法，如 IP-Adapter-FaceID，而且在特定场景下，其效果与 ROOP、LoRAs 等方法不相上下。\n",
      "\n",
      "对于相似度有更高要求的真人写真场景，InstantID 也能完成得不错。不仅能够在秒级时间内完成高质量的 图像生成 ，还避免耗时的 LoRa 训练，相比妙鸭成本更低，大约是其 1/300。通过精细化控制脸部区域，InstantID 能够增强脸部相似度，同时保持整体风格的和谐。\n",
      "\n",
      "此外，InstantID 的分区域生成方案支持多人多风格的 图像生成 ，耗时基本无增。\n",
      "\n",
      "它的鲁棒性和泛化性，使其能顺利处理夸张的五官比例。\n",
      "\n",
      "多视角的生成也没问题。按你指定的姿势图和面部特征，生成新的 AI 写真。\n",
      "\n",
      "InstantID 的可扩展性良好，能够快速支持多种衍生功能。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如快速换脸。与 Inswapper 相比，InstantID 生成的作品在面孔和背景的融合上更加灵活。\n",
      "\n",
      "ID 信息 插值 。InstantID 支持两脸自定义融合，保留双方特征。\n",
      "\n",
      "非人像与 ID 的结合，很有特点。\n",
      "\n",
      "聊到这儿，不妨你亲自尝试一下，感受它的魅力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "操作方式非常简单，进入 InstantID 的 Demo 页面，直接上传照片，便可免费体验 ：\n",
      "\n",
      "https://huggingface.co/spaces/InstantX/InstantID\n",
      "\n",
      "InstantID 的这些优势，不仅为个人用户提供了强大的创作工具，也为商业应用如电子商务、广告和娱乐产业开辟了新的可能性。InstantID 本次表现令人惊喜，其高效、灵活、强大的性能和易用性，印象深刻。期待小红书该开源项目的后续进展，未来能在多个领域发挥出更大的价值。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "附录： \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-01-8#comment\n",
      "title= 小红书开源「InstantID」效果炸裂，被Yann LeCun点赞，迅速蹿上Github热榜\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 只需一张照片，整个过程无需训练 LoRA 模型，多风格 AI 写真即刻呈现！\n",
      "\n",
      "最近，有一群来自小红书的 95 后神秘团队，自称 InstantX，搞了个大动作 —— 开源「InstantID」项目。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 凭借着高质量的 图像生成 能力，在开源界掀起了一股热潮：不仅获得了众多技术大佬的点赞，更是在 GitHub 热榜上迅速飙升，成为焦点。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个「出片神器」，让用户只需上传一张照片，就能轻松定制出多种风格的 AI 写真。\n",
      "\n",
      "对，你没看错。如图左侧所示，与之前爆火的妙鸭相机至少需要上传 20 张照片不同的是，InstantID 只需一张自拍，不依赖模型训练，不需要等待，瞬间变身。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "无论是古典油画的优雅，炫酷的赛博朋克，或是 3D 雕像的立体感，只要是你喜欢的风格，InstantID 都能轻松驾驭。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "它不仅风格多样，还能在保持人物面部高保真的同时，无需模型训练，实现秒级出图，效率大幅提升。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 目前位列 Hugging Face Space Trending 榜首，许多小伙伴玩得不亦乐乎～\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如，把马斯克送上了火星。\n",
      "\n",
      "让蒙娜丽莎拍「樱花写真」，微笑依旧很神秘。\n",
      "\n",
      "甚至可以让语文课本中的杜甫从二维变三维，穿越到现代变身「帅大叔」。\n",
      "\n",
      "图灵奖得主 Yann LeCun，化身多种动漫人物，你猜出了几个角色？\n",
      "\n",
      "就连 Yann LeCun 本人也点赞转发，调侃自己的「钢铁侠」衣服在哪里。\n",
      "\n",
      "在个性化图像合成领域，实现强烈风格化写真的同时保持面部高保真度，一直是个挑战。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从效果上看，InstantID 做到了。那它背后运用了哪些方法，有什么独到之处吗？\n",
      "\n",
      "回顾过去，尽管 Textual Inversion、 DreamBooth 和 LoRAs 等技术已经取得了重大进展。但它们在实际应用中仍受限于高存储需求、耗时的微调过程以及对多张参考图像的依赖。相比之下，现有基于 ID 嵌入的方法虽然只需一次前向推理，但也面临不小挑战：要么需要对大量模型 参数 进行广泛的微调，要么与社区预训练模型不兼容，要么无法保持高真实性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstantID 的出现，打破了这些局限。小红书 InstantX 团队公开了论文《 InstantID: Zero-shot Identity-Preserving Generation in Seconds 》和推理代码，他们表示：InstantID 巧妙地避免了对文生图模型 UNet 部分的训练，仅通过训练一个轻量级的可插拔模块，实现了在推理过程中无需 test-time tuning，同时保持了文本控制的灵活性，确保了面部特征的高保真度。\n",
      "\n",
      "如图所示，InstantID 的工作原理可分为三个关键部分：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ID Embedding：团队利用预训练的面部识别模型代替 CLIP 来提取语义人脸特征，并使用可训练的投影层，将这些特征 映射 到文本特征空间，形成 Face Embedding，具有丰富的语义信息，包括如面部特征、表情、年龄等，为后续的 图像生成 提供了坚实的基础。\n",
      "\n",
      "Image Adapter：引入一个轻量级的适配模块，将提取的身份信息与文本提示结合起来。这个模块通过解耦的交叉 注意力机制 ，使得图像和文本能够独立地影响生成过程，从而在保持身份信息的同时，允许用户对图像风格进行精细控制，实现「双赢」。\n",
      "\n",
      "IdentityNet：小红书提出了一个名为 IdentityNet 的网络，是 InstantID 的核心部分。它通过强语义条件（如面部特征的详细描述）和弱空间条件（如面部关键点的位置）来编码参考面部图像的复杂特征。在 IdentityNet 中，生成过程完全由 Face Embedding 引导，无需任何文本信息。仅更新新添加的模块，而预先训练的文本到图像模型保持冻结以确保灵活性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在实际的 图像生成 过程中，InstantID 首先会接收到用户的文本提示和面部图像。然后通过 ID Embedding 提取关键信息，接着 Image Adapter 将这些信息与文本提示融合。IdentityNet 会根据这些融合后的信息生成图像。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "整个过程是自动化的，用户不需要进行任何额外的微调或训练，只需等待二十几秒，就能得到一个既符合文本描述又保留个人身份特征的定制图像。\n",
      "\n",
      "InstantID 不仅解决了训练效率与身份保真度之间的平衡问题，还提供了一系列令人印象深刻的特性。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先，InstantID 的即插即用和兼容性是其最大的卖点之一。它无需对 UNet 进行额外训练，即可与现有的预训练模型无缝集成，如社区内的文生图基础模型、LoRAs 和 ControlNets。这意味着用户可以在不增加成本的情况下，轻松地在推理过程中保持人物的身份特征，裂变性强。\n",
      "\n",
      "其次，InstantID 的无需微调特性，使得它在实际应用中极具经济性和实用性。用户只需进行一次前向传播，即可快速生成图像，同时保持对文本编辑的强大控制力，让身份信息与各种风格完美融合。如下图所示，其编辑性强的特点让用户能够通过文本控制性别、头发、服装等细节，确保生成图像的多样性。\n",
      "\n",
      "性能方面的表现同样卓越，它能够仅凭一张参考图像，就生成具有高保真度和灵活性的先进结果。这一性能不仅超越了基于单张图片特征的嵌入方法，如 IP-Adapter-FaceID，而且在特定场景下，其效果与 ROOP、LoRAs 等方法不相上下。\n",
      "\n",
      "对于相似度有更高要求的真人写真场景，InstantID 也能完成得不错。不仅能够在秒级时间内完成高质量的 图像生成 ，还避免耗时的 LoRa 训练，相比妙鸭成本更低，大约是其 1/300。通过精细化控制脸部区域，InstantID 能够增强脸部相似度，同时保持整体风格的和谐。\n",
      "\n",
      "此外，InstantID 的分区域生成方案支持多人多风格的 图像生成 ，耗时基本无增。\n",
      "\n",
      "它的鲁棒性和泛化性，使其能顺利处理夸张的五官比例。\n",
      "\n",
      "多视角的生成也没问题。按你指定的姿势图和面部特征，生成新的 AI 写真。\n",
      "\n",
      "InstantID 的可扩展性良好，能够快速支持多种衍生功能。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "比如快速换脸。与 Inswapper 相比，InstantID 生成的作品在面孔和背景的融合上更加灵活。\n",
      "\n",
      "ID 信息 插值 。InstantID 支持两脸自定义融合，保留双方特征。\n",
      "\n",
      "非人像与 ID 的结合，很有特点。\n",
      "\n",
      "聊到这儿，不妨你亲自尝试一下，感受它的魅力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "操作方式非常简单，进入 InstantID 的 Demo 页面，直接上传照片，便可免费体验 ：\n",
      "\n",
      "https://huggingface.co/spaces/InstantX/InstantID\n",
      "\n",
      "InstantID 的这些优势，不仅为个人用户提供了强大的创作工具，也为商业应用如电子商务、广告和娱乐产业开辟了新的可能性。InstantID 本次表现令人惊喜，其高效、灵活、强大的性能和易用性，印象深刻。期待小红书该开源项目的后续进展，未来能在多个领域发挥出更大的价值。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "附录： \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-01-7\n",
      "title= 刚刚，字节版GPTs「扣子」上线了\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 大玩家终于正式下场。\n",
      "\n",
      "在持续一年的大模型热潮之后，「智能体」成为了科技公司们新的押注方向之一。\n",
      "\n",
      "近日， 字节跳动 正式推出「Coze 扣子」AI Bot 开发平台。任何用户都可以快速、低门槛地搭建自己的 Chatbot，且平台支持用户将其一键发布到飞书、微信公众号、豆包等渠道。\n",
      "\n",
      "链接：https://www.coze.cn/\n",
      "\n",
      "当然，除了可以创建自己的 Chatbot，Coze 官方还提供了 Bots 商店和插件。\n",
      "\n",
      "如以下按照热度精选的 Bots，包含娱乐、创意、学习等各类产品，甚至我们注意到还有「马歇尔音箱粉丝」这个选项。\n",
      "\n",
      "机器之心 挑选了一些已有的 Bot 试了试，看看上手体验如何。\n",
      "\n",
      "首先我们测了一个对学生家长来说可能帮助比较大的 Bot——「数学老师」Bot。这个 Bot 在运行时会调用 Wolfram Alpha、OCR 等插件。Wolfram 可以理解为一个超强计算器，在 ChatGPT 问世之初，其创始人就呼吁将 ChatGPT 与 Wolfram Alpha 联合使用。如今，在扣子平台上，国内家长也能用到大模型和 Wolfram Alpha 结合的辅导工具了。\n",
      "\n",
      "从我们的初步测试来看，「数学老师」Bot 可以用来解答一些数学应用题，还能解答数学概念。OCR 插件的调用还能让它具备读图能力，但几何题解答起来未必正确。\n",
      "\n",
      "在「咨询」类别下，我们还找到了一个可以和小朋友聊天的 Bot。这个机器人可以调用必应搜索和文生图插件 ByteArtist，不仅能和小朋友聊天，还有画画等多模态技能。\n",
      "\n",
      "当然，这些 Bot 有多少玩法、怎么玩出新花样其实更多地取决于你。只要稍微修改一下扣子提供的编排模板，你就能拥有一个按照自己心意定制的 Bot，而且后台还有更多插件可以调用，这里面有着巨大的探索空间。\n",
      "\n",
      "据官方文档介绍，Coze 包括以下功能和优势：\n",
      "\n",
      "首先是无限拓展的能力集：扣子的插件工具极为丰富，从而拓展了 Bot 的能力边界。目前平台已经集成了超过 60 款各类型的插件，包括资讯阅读、旅游出行、效率办公、图片理解等 API 及多模态模型。用户可以直接将这些插件添加到 Bot 中，丰富 Bot 能力。此外，扣子平台也支持创建自定义插件。你可以将已有的 API 能力通过 参数 配置的方式快速创建一个插件让 Bot 调用。\n",
      "\n",
      "官方提供的插件类别比较丰富。据 机器之心 了解，这些插件有自研的、合作的，还有内部 hackathon 比赛作品。\n",
      "\n",
      "其次，丰富的数据源：扣子提供了简单易用的 知识库 功能来管理和存储数据，支持 Bot 与用户自己的数据进行交互。无论是内容量巨大的本地文件还是某个网站的实时信息，都可以上传到 知识库 中。这样，Bot 就可以使用 知识库 中的内容回答问题了。 知识库 支持添加文本格式、表格格式的数据。对于上传的内容，用户可以将本地 TXT、PDF、DOCX、Excel、CXV 格式的文档上传至 知识库 ，也可以基于 URL 获取在线网页内容和 API JSON 数据。同时支持直接在 知识库 内添加自定义数据。\n",
      "\n",
      "用户可以创建自己的 知识库\n",
      "\n",
      "知识库 支持添加文本格式、表格格式的数据\n",
      "\n",
      "持久化的记忆能力：扣子提供了方便 AI 交互的 数据库 记忆能力，可持久记住用户对话的重要 参数 或内容。\n",
      "\n",
      "灵活的工作流设计：扣子的工作流功能可以用来处理 逻辑 复杂，且有较高稳定性要求的任务流。扣子提供了大量灵活可组合的节点包括大 语言模型 LLM、自定义代码、判断 逻辑 等，无论你是否有编程基础，都可以通过拖拉拽的方式快速搭建一个工作流，例如创建一个撰写行业研究报告的工作流，让 Bot 写一份 20 页的报告。\n",
      "\n",
      "看起来，字节把扣子定位为一个应用创作平台：你可以在其上开发出属于自己的 AI chatbot（ 聊天机器人 ），无需有编程经验，扣子就可以快速创建出各种类型的 聊天机器人 ，并将它们部署在不同的社交平台和应用程序上。\n",
      "\n",
      "创建完成之后，发布流程也十分简洁。扣子支持发布到 AI 聊天应用豆包、办公平台飞书，以及用户的微信公众号（目前只支持发布到服务号，不支持发布到订阅号）和微信客服。\n",
      "\n",
      "后续更多玩法我们还在探索中，欢迎大家讨论。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-01-7#comment\n",
      "title= 刚刚，字节版GPTs「扣子」上线了\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 大玩家终于正式下场。\n",
      "\n",
      "在持续一年的大模型热潮之后，「智能体」成为了科技公司们新的押注方向之一。\n",
      "\n",
      "近日， 字节跳动 正式推出「Coze 扣子」AI Bot 开发平台。任何用户都可以快速、低门槛地搭建自己的 Chatbot，且平台支持用户将其一键发布到飞书、微信公众号、豆包等渠道。\n",
      "\n",
      "链接：https://www.coze.cn/\n",
      "\n",
      "当然，除了可以创建自己的 Chatbot，Coze 官方还提供了 Bots 商店和插件。\n",
      "\n",
      "如以下按照热度精选的 Bots，包含娱乐、创意、学习等各类产品，甚至我们注意到还有「马歇尔音箱粉丝」这个选项。\n",
      "\n",
      "机器之心 挑选了一些已有的 Bot 试了试，看看上手体验如何。\n",
      "\n",
      "首先我们测了一个对学生家长来说可能帮助比较大的 Bot——「数学老师」Bot。这个 Bot 在运行时会调用 Wolfram Alpha、OCR 等插件。Wolfram 可以理解为一个超强计算器，在 ChatGPT 问世之初，其创始人就呼吁将 ChatGPT 与 Wolfram Alpha 联合使用。如今，在扣子平台上，国内家长也能用到大模型和 Wolfram Alpha 结合的辅导工具了。\n",
      "\n",
      "从我们的初步测试来看，「数学老师」Bot 可以用来解答一些数学应用题，还能解答数学概念。OCR 插件的调用还能让它具备读图能力，但几何题解答起来未必正确。\n",
      "\n",
      "在「咨询」类别下，我们还找到了一个可以和小朋友聊天的 Bot。这个机器人可以调用必应搜索和文生图插件 ByteArtist，不仅能和小朋友聊天，还有画画等多模态技能。\n",
      "\n",
      "当然，这些 Bot 有多少玩法、怎么玩出新花样其实更多地取决于你。只要稍微修改一下扣子提供的编排模板，你就能拥有一个按照自己心意定制的 Bot，而且后台还有更多插件可以调用，这里面有着巨大的探索空间。\n",
      "\n",
      "据官方文档介绍，Coze 包括以下功能和优势：\n",
      "\n",
      "首先是无限拓展的能力集：扣子的插件工具极为丰富，从而拓展了 Bot 的能力边界。目前平台已经集成了超过 60 款各类型的插件，包括资讯阅读、旅游出行、效率办公、图片理解等 API 及多模态模型。用户可以直接将这些插件添加到 Bot 中，丰富 Bot 能力。此外，扣子平台也支持创建自定义插件。你可以将已有的 API 能力通过 参数 配置的方式快速创建一个插件让 Bot 调用。\n",
      "\n",
      "官方提供的插件类别比较丰富。据 机器之心 了解，这些插件有自研的、合作的，还有内部 hackathon 比赛作品。\n",
      "\n",
      "其次，丰富的数据源：扣子提供了简单易用的 知识库 功能来管理和存储数据，支持 Bot 与用户自己的数据进行交互。无论是内容量巨大的本地文件还是某个网站的实时信息，都可以上传到 知识库 中。这样，Bot 就可以使用 知识库 中的内容回答问题了。 知识库 支持添加文本格式、表格格式的数据。对于上传的内容，用户可以将本地 TXT、PDF、DOCX、Excel、CXV 格式的文档上传至 知识库 ，也可以基于 URL 获取在线网页内容和 API JSON 数据。同时支持直接在 知识库 内添加自定义数据。\n",
      "\n",
      "用户可以创建自己的 知识库\n",
      "\n",
      "知识库 支持添加文本格式、表格格式的数据\n",
      "\n",
      "持久化的记忆能力：扣子提供了方便 AI 交互的 数据库 记忆能力，可持久记住用户对话的重要 参数 或内容。\n",
      "\n",
      "灵活的工作流设计：扣子的工作流功能可以用来处理 逻辑 复杂，且有较高稳定性要求的任务流。扣子提供了大量灵活可组合的节点包括大 语言模型 LLM、自定义代码、判断 逻辑 等，无论你是否有编程基础，都可以通过拖拉拽的方式快速搭建一个工作流，例如创建一个撰写行业研究报告的工作流，让 Bot 写一份 20 页的报告。\n",
      "\n",
      "看起来，字节把扣子定位为一个应用创作平台：你可以在其上开发出属于自己的 AI chatbot（ 聊天机器人 ），无需有编程经验，扣子就可以快速创建出各种类型的 聊天机器人 ，并将它们部署在不同的社交平台和应用程序上。\n",
      "\n",
      "创建完成之后，发布流程也十分简洁。扣子支持发布到 AI 聊天应用豆包、办公平台飞书，以及用户的微信公众号（目前只支持发布到服务号，不支持发布到订阅号）和微信客服。\n",
      "\n",
      "后续更多玩法我们还在探索中，欢迎大家讨论。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-05-7\n",
      "title= 比OpenAI官方提示词指南更全，这26条黄金准则让LLM性能飙升50%以上\n",
      "author= []\n",
      "publish_date= 2024-02-05 00:00:00\n",
      "text= 今天，穆罕默德・本・扎耶德 人工智能 大学 VILA Lab 带来了一项关于如何更好地为不同规模的大模型书写提示词（prompt）的研究，让大模型性能在不需要任何额外训练的前提下轻松提升 50% 以上。该工作在 X (Twitter)、Reddit 和 LinkedIn 等平台上都引起了广泛的讨论和关注。\n",
      "\n",
      "论文地址: https://arxiv.org/abs/2312.16171\n",
      "\n",
      "Github地址: https://github.com/VILA-Lab/ATLAS\n",
      "\n",
      "论文标题：Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在以 ChatGPT 为首的大模型出来之后，为大 语言模型 设计提示词的研究已经成为一个重要的研究方向，包括 OpenAI 官方也出品了针对 ChatGPT 用户的提示工程指南 [1] ，其包含了六条书写准则：1）写出清晰的指令；2）提供参考文本；3）将复杂的任务拆分为更简单的子任务；4）给模型时间「思考」；5）使用外部工具；6）系统地测试更改。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "由此可见，提示词对于如何更好地使用大模型以及得到满意的回答都具有重要的意义。然而可以看到的是，OpenAI 提供的这些准则都是比较宽泛和保守的，并没有涉及到一些具体的操作和技巧。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "今天要介绍的这篇文章提供了更多也更加接地气的提示工程指南，足足有 26 条之多，内容涵盖了：1）回答内容和语言风格的控制；2）提示词结构和清晰度；3）复杂任务和代码提示；4）回答特异性和信息量；5）用户交互和参与等多个方面。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "下面让我们来逐条讨论一下这些提示词准则：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1) 如果您更喜欢更简洁的答案，则无需对 LLM 保持礼貌，因此无需添加诸如 「请」、「如果你不介意」、「谢谢」、「我愿意」等，直奔主题即可。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2) 在提示中融入目标受众，例如该领域的专家。具体而言，当你告诉大模型你的目标受众是一个孩子，它的回答会更加通俗易懂，当你告诉它受众是这个领域的专家，它会提供更加专业和深入的解释。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3) 在交互式对话中将复杂的任务分解为一系列更简单的提示。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4) 使用肯定的指令，如「做」，同时避免使用「不要」等否定性语言。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5) 当您需要简单清晰或更深入地了解某个主题、想法或任何信息时，请利用以下提示：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "用简单的术语解释 [插入特定主题]。\n",
      "\n",
      "像我是 11 岁一样向我解释这个问题。\n",
      "\n",
      "向我解释，就好像我是 [领域] 的初学者一样。\n",
      "\n",
      "用简单的英语写 [文章 / 文本 / 段落]，就像你在向一个 5 岁的孩子解释一些事情一样。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6) 添加「我要给 $xxx 小费以获得更好的解决方案！」这种提示词会带来提升的原因可能是：在训练数据中，当涉及到回答是有奖励的，回答的人往往会更加准确细致，小心谨慎地提供答案，大模型从这些网络数据中学到了这些结构和方式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7) 实现示例驱动的提示（使用少样本提示）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8) 格式化提示时，以「###Instruction###」开头，然后是「###Example###」 或「###Question###」（如果相关）。随后展示您的内容。使用一个或多个换行符用于分隔指令、示例、问题、上下文和输入数据。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9) 在你的提示词里面加入以下短语：「你的任务是」和「你必须」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10) 在你的提示词里面加入以下短语：「你会受到惩罚」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11) 在提示中使用「以自然、类似人类的方式回答问题」这句话。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12) 使用引导性词语，例如写「一步一步地思考」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13) 在提示中加上以下短语：「确保你的回答是公正的，避免依赖刻板印象」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14) 让模型通过向你提问来引出你精确的细节和要求，直到他得到足够的信息来提供所需的输出（例如，「从现在开始，我希望你问我......」提问）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15) 要询问特定主题或想法或任何信息，并且您想测试您的理解，您可以使用 以下短语：「教我任何 [定理 / 主题 / 规则名称]，并在末尾包含一个测试，并让我知道是否在我回答后，我的答案是正确的，不要事先提供答案。 」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "16) 为大型 语言模型 分配角色。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17) 使用分隔符。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "18) 在提示中多次重复特定单词或短语。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19) 将思维链 （CoT） 与 few-Shot 提示相结合。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "20) 使用输出引导，包括用所需输出的开头结束提示。利用输出引导，以预期响应的开头结束提示。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "21) 如果任务是写一篇文章 / 文本 / 段落或任何类型的文本，同时需要尽可能的详细，可以添加提示词：「写一篇详细的 [论文 / 文本 / 段落]，通过添加所有必要的信息从而使我能详细了解 [主题]。 」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "22) 在不改变其样式的情况下更正 / 更改特定文本：尝试修改用户发送的每个段落。你应该只提高用户的语法和词汇量，并确保它听起来很自然。您应该保留原始写作风格，确保正式段落保持正式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "23) 当您有一个复杂的编程提示时，该提示可能位于不同的文件中：「从现在开始，每当您生成跨越多个文件的代码，生成一个可以自动运行的 [编程语言 ] 脚本，创建指定的文件或对现有文件进行更改以插入生成的代码。[你的问题]」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "24) 当您想使用特定单词、短语或句子开始或继续文本时，请使用以下方法提示：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我为你提供开头 [歌词 / 故事 / 段落 / 散文...]：[插入歌词 / 单词 / 句子]。根据提供的单词完成它。保持内容风格一致。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "25) 明确说明模型必须遵循的要求去生成内容， 以关键字、规定、提示或说明的形式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "26) 如果要编写任何文本，例如文章或段落，并且需要与提供的示例相似，请包括下面提示语句：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "根据提供的段落使用相同的语言 [/title/text/essay/answer]。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "以下是一些具体的提示词例子和对应的 GPT-4 输出结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. 当我们询问 GPT-4 问题的时候，最后可以加上一句「提供公正的解释，突出科学证据和不同观点。」可以看到加上该提示词之后 GPT-4 的回答明显会更加丰富和有深度。\n",
      "\n",
      "2. 我们可以提供一些示例让模型更好的理解我们的目标和出发点。\n",
      "\n",
      "3. 我们可以告诉模型用简单的方法来回答问题，就像是在向一个 5 岁的孩子解释一些事情。可以看到加上和不加这个提示词，模型的回复在理解困难程度上有明显的差别。\n",
      "\n",
      "4. 我们可以通过给模型小费的方式，让模型更加严谨完善的回答问题。\n",
      "\n",
      "定量实验结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. 模型回答质量提升比例：该指标表示在使用提示词原则后，问题的回答质量提高的百分比。\n",
      "\n",
      "可以看到所有提示词原则在人工评测中都取得了或多或少的提升，其中原则 14 获得了 100% 的提升，意味着所有问题通过使用该提示原则都获得了提升。与此同时，原则 1 得到的提升相对较少。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2. 回复正确性提升：正确性指模型输出或回答的精度，判断标准是回答是否准确、相关且没有错误的。本文同时考虑了不同模型的绝对正确性和相对正确性提升两个指标。\n",
      "\n",
      "上图结果为加入提示原则后，大模型回复质量的相对正确性提升。「small」表示 7B 模型，「medium」表示 13B 模型，「large scale」表示 70B 和 GPT-3.5/4 模型。可以看到大模型在使用提示词原则后，提升幅度相对于小模型和中等模型会更加显著。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3. 单独每个模型准确度提升比例：\n",
      "\n",
      "上图是每个不同大小的模型相对提升幅度，可以看到类似的现象，模型越大，对于提示词的响应和回复也越加敏锐，准确性提升也相对越大。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4. 下图是不同大小模型对于每条提示词原则准确度提升大小具体结果：\n",
      "\n",
      "提示词准则数据集：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "本文在介绍 26 条提示词准则的同时，还附带发布了一个基于准则提示词的 基准 ，其中每条准则作者准备了 20 个不同的问题，每个问题同时包含带有准则和不带准则两种对应的大模型回复。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "该数据集可以用在：1）大 语言模型 对于提示词响应的性能评测；2）偏好驱动的大模型微调。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "数据集链接：https://github.com/VILA-Lab/ATLAS。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "更多提示词原则使用方法和说明，欢迎阅读原文。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考文献\n",
      "\n",
      "[1] Six strategies for getting better results. OpenAI. https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-05-7#comment\n",
      "title= 比OpenAI官方提示词指南更全，这26条黄金准则让LLM性能飙升50%以上\n",
      "author= []\n",
      "publish_date= 2024-02-05 00:00:00\n",
      "text= 今天，穆罕默德・本・扎耶德 人工智能 大学 VILA Lab 带来了一项关于如何更好地为不同规模的大模型书写提示词（prompt）的研究，让大模型性能在不需要任何额外训练的前提下轻松提升 50% 以上。该工作在 X (Twitter)、Reddit 和 LinkedIn 等平台上都引起了广泛的讨论和关注。\n",
      "\n",
      "论文地址: https://arxiv.org/abs/2312.16171\n",
      "\n",
      "Github地址: https://github.com/VILA-Lab/ATLAS\n",
      "\n",
      "论文标题：Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在以 ChatGPT 为首的大模型出来之后，为大 语言模型 设计提示词的研究已经成为一个重要的研究方向，包括 OpenAI 官方也出品了针对 ChatGPT 用户的提示工程指南 [1] ，其包含了六条书写准则：1）写出清晰的指令；2）提供参考文本；3）将复杂的任务拆分为更简单的子任务；4）给模型时间「思考」；5）使用外部工具；6）系统地测试更改。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "由此可见，提示词对于如何更好地使用大模型以及得到满意的回答都具有重要的意义。然而可以看到的是，OpenAI 提供的这些准则都是比较宽泛和保守的，并没有涉及到一些具体的操作和技巧。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "今天要介绍的这篇文章提供了更多也更加接地气的提示工程指南，足足有 26 条之多，内容涵盖了：1）回答内容和语言风格的控制；2）提示词结构和清晰度；3）复杂任务和代码提示；4）回答特异性和信息量；5）用户交互和参与等多个方面。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "下面让我们来逐条讨论一下这些提示词准则：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1) 如果您更喜欢更简洁的答案，则无需对 LLM 保持礼貌，因此无需添加诸如 「请」、「如果你不介意」、「谢谢」、「我愿意」等，直奔主题即可。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2) 在提示中融入目标受众，例如该领域的专家。具体而言，当你告诉大模型你的目标受众是一个孩子，它的回答会更加通俗易懂，当你告诉它受众是这个领域的专家，它会提供更加专业和深入的解释。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3) 在交互式对话中将复杂的任务分解为一系列更简单的提示。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4) 使用肯定的指令，如「做」，同时避免使用「不要」等否定性语言。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5) 当您需要简单清晰或更深入地了解某个主题、想法或任何信息时，请利用以下提示：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "用简单的术语解释 [插入特定主题]。\n",
      "\n",
      "像我是 11 岁一样向我解释这个问题。\n",
      "\n",
      "向我解释，就好像我是 [领域] 的初学者一样。\n",
      "\n",
      "用简单的英语写 [文章 / 文本 / 段落]，就像你在向一个 5 岁的孩子解释一些事情一样。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6) 添加「我要给 $xxx 小费以获得更好的解决方案！」这种提示词会带来提升的原因可能是：在训练数据中，当涉及到回答是有奖励的，回答的人往往会更加准确细致，小心谨慎地提供答案，大模型从这些网络数据中学到了这些结构和方式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7) 实现示例驱动的提示（使用少样本提示）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8) 格式化提示时，以「###Instruction###」开头，然后是「###Example###」 或「###Question###」（如果相关）。随后展示您的内容。使用一个或多个换行符用于分隔指令、示例、问题、上下文和输入数据。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9) 在你的提示词里面加入以下短语：「你的任务是」和「你必须」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10) 在你的提示词里面加入以下短语：「你会受到惩罚」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11) 在提示中使用「以自然、类似人类的方式回答问题」这句话。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12) 使用引导性词语，例如写「一步一步地思考」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13) 在提示中加上以下短语：「确保你的回答是公正的，避免依赖刻板印象」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14) 让模型通过向你提问来引出你精确的细节和要求，直到他得到足够的信息来提供所需的输出（例如，「从现在开始，我希望你问我......」提问）。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15) 要询问特定主题或想法或任何信息，并且您想测试您的理解，您可以使用 以下短语：「教我任何 [定理 / 主题 / 规则名称]，并在末尾包含一个测试，并让我知道是否在我回答后，我的答案是正确的，不要事先提供答案。 」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "16) 为大型 语言模型 分配角色。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17) 使用分隔符。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "18) 在提示中多次重复特定单词或短语。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19) 将思维链 （CoT） 与 few-Shot 提示相结合。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "20) 使用输出引导，包括用所需输出的开头结束提示。利用输出引导，以预期响应的开头结束提示。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "21) 如果任务是写一篇文章 / 文本 / 段落或任何类型的文本，同时需要尽可能的详细，可以添加提示词：「写一篇详细的 [论文 / 文本 / 段落]，通过添加所有必要的信息从而使我能详细了解 [主题]。 」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "22) 在不改变其样式的情况下更正 / 更改特定文本：尝试修改用户发送的每个段落。你应该只提高用户的语法和词汇量，并确保它听起来很自然。您应该保留原始写作风格，确保正式段落保持正式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "23) 当您有一个复杂的编程提示时，该提示可能位于不同的文件中：「从现在开始，每当您生成跨越多个文件的代码，生成一个可以自动运行的 [编程语言 ] 脚本，创建指定的文件或对现有文件进行更改以插入生成的代码。[你的问题]」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "24) 当您想使用特定单词、短语或句子开始或继续文本时，请使用以下方法提示：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "我为你提供开头 [歌词 / 故事 / 段落 / 散文...]：[插入歌词 / 单词 / 句子]。根据提供的单词完成它。保持内容风格一致。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "25) 明确说明模型必须遵循的要求去生成内容， 以关键字、规定、提示或说明的形式。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "26) 如果要编写任何文本，例如文章或段落，并且需要与提供的示例相似，请包括下面提示语句：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "根据提供的段落使用相同的语言 [/title/text/essay/answer]。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "以下是一些具体的提示词例子和对应的 GPT-4 输出结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. 当我们询问 GPT-4 问题的时候，最后可以加上一句「提供公正的解释，突出科学证据和不同观点。」可以看到加上该提示词之后 GPT-4 的回答明显会更加丰富和有深度。\n",
      "\n",
      "2. 我们可以提供一些示例让模型更好的理解我们的目标和出发点。\n",
      "\n",
      "3. 我们可以告诉模型用简单的方法来回答问题，就像是在向一个 5 岁的孩子解释一些事情。可以看到加上和不加这个提示词，模型的回复在理解困难程度上有明显的差别。\n",
      "\n",
      "4. 我们可以通过给模型小费的方式，让模型更加严谨完善的回答问题。\n",
      "\n",
      "定量实验结果：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. 模型回答质量提升比例：该指标表示在使用提示词原则后，问题的回答质量提高的百分比。\n",
      "\n",
      "可以看到所有提示词原则在人工评测中都取得了或多或少的提升，其中原则 14 获得了 100% 的提升，意味着所有问题通过使用该提示原则都获得了提升。与此同时，原则 1 得到的提升相对较少。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2. 回复正确性提升：正确性指模型输出或回答的精度，判断标准是回答是否准确、相关且没有错误的。本文同时考虑了不同模型的绝对正确性和相对正确性提升两个指标。\n",
      "\n",
      "上图结果为加入提示原则后，大模型回复质量的相对正确性提升。「small」表示 7B 模型，「medium」表示 13B 模型，「large scale」表示 70B 和 GPT-3.5/4 模型。可以看到大模型在使用提示词原则后，提升幅度相对于小模型和中等模型会更加显著。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3. 单独每个模型准确度提升比例：\n",
      "\n",
      "上图是每个不同大小的模型相对提升幅度，可以看到类似的现象，模型越大，对于提示词的响应和回复也越加敏锐，准确性提升也相对越大。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4. 下图是不同大小模型对于每条提示词原则准确度提升大小具体结果：\n",
      "\n",
      "提示词准则数据集：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "本文在介绍 26 条提示词准则的同时，还附带发布了一个基于准则提示词的 基准 ，其中每条准则作者准备了 20 个不同的问题，每个问题同时包含带有准则和不带准则两种对应的大模型回复。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "该数据集可以用在：1）大 语言模型 对于提示词响应的性能评测；2）偏好驱动的大模型微调。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "数据集链接：https://github.com/VILA-Lab/ATLAS。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "更多提示词原则使用方法和说明，欢迎阅读原文。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考文献\n",
      "\n",
      "[1] Six strategies for getting better results. OpenAI. https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-02-7\n",
      "title= 扎克伯格分红7亿刀，Meta股价大涨14%，开源大计成了​？\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= Meta 正在走出阴霾。\n",
      "\n",
      "伴随着 Meta 的股价周四盘后上涨近 14%，升至历史新高，这家公司宣布了有史以来的首次股息派发。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最近一次财报电话会议公布内容显示，Meta 公布的 2023 全年营收为 1349 亿美元，较 2022 年增长 16%；净利润为 391 亿美元，同比增长 69%。其中，第四季度营收为 401 亿美元，超出预期的 391.8 亿美元，同比增长 25%。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从 3 月份开始，Meta 将按季度向 A 类和 B 类普通股派发现金股息 50 美分。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "根据彭博社汇编的数据，首席执行官扎克伯格持有约 3.5 亿股股票（Meta 13% 的股份），他将从每季度派发的股息中获得约 1.75 亿美元的税前收入，一年下来约有 7 亿美元。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "小扎表示：「随着社区和业务持续增长，我们度过了一个不错的季度。」\n",
      "\n",
      "虽然这家公司在 2022 年经历了市值的低谷，但 Meta 股票在过去 12 个月里上涨了 168%。公司市值已重回万亿美元行列，扎克伯格的净资产更是达到了 1420 亿美元。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "与其他一些大型科技公司的创始人不同，扎克伯格这位首席执行官从未将权力移交给其他人，并一直在元宇宙和 人工智能 等趋势上进行长期押注。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "过去这段日子，Meta 在 人工智能 领域的表现毫不逊色：生成式 人工智能 大 语言模型 Llama 可与 OpenAI 和谷歌的模型竞争，Meta 同时在寻求将 人工智能 工具集成到其核心产品中，据传还计划使用自研芯片为其 人工智能 系统提供动力。在不久前的一段视频中，扎克伯格暗示了 Meta 开发通用 人工智能 (AGI) 的宏伟计划。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在好于预期的业绩之外，这些消息似乎也提振了外界对于 Meta 的信心。看来，Meta 正在走出阴霾。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "凭借 Llama，Meta 正在 AI 领域开辟新战场\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如果时间回到一年多以前，或许谁也没有想到，曾经深陷元宇宙泥沼的 Meta 能凭借一组开源模型重新扳回一局，甚至在 OpenAI、谷歌主导的 AI 闭源世界之外重新开辟了一个战场。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个战场以 Meta 开源的 Llama 系列模型为基础，成员异常活跃。在其基础上微调的 AI 大模型很多都宣称已经超过 GPT-3.5，甚至已经逼近 GPT-4（比如最近从 Mistral 公司泄露出的「Miqu」模型，该模型接近 GPT-4 性能，是 Mistral 基于 Llama 2 为自家客户训练的早期版本）。\n",
      "\n",
      "与此同时，Meta 开源模型在业界的影响力也在逐渐显现。很多国际大公司，比如富国银行、 IBM ，如今都部署了基于 Llama 2 的开源模型。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "对于为什么选开源路线，Meta 在最近的财报电话会上给出了回应。Meta 坦言，他们确实从开源这条路上收获了很多，包括基础模型的改进和模型到产品的过渡等多个方面。而且，他们认为，成员开源的领导者有几个战略优势：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先，开源软件通常更安全、更可靠，同时由于社区的持续反馈、审查和开发，运行效率更高。这是一个大问题，因为安全性是 人工智能 中最重要的问题之一。效率的提高和计算成本的降低也让包括 Meta 在内的每个人都受益。\n",
      "\n",
      "其次，开源软件通常会成为行业标准。Meta 表示，当其他公司使用 Meta 的技术栈进行标准化构建时，Meta 就能很容易地将其他公司的创新整合到自己的产品中。这很微妙，但是快速学习和改进的能力是一个巨大的优势，并且成为行业标准使得这种做法成为可能。\n",
      "\n",
      "最后，开源在开发者和研究人员中非常受欢迎，这有助于 Meta 招募到最好的人才。\n",
      "\n",
      "不过，要让这些效果完全显现可能还要等一段时间，毕竟 Meta 开源系列模型的发布时间要晚于 ChatGPT。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在上个月发布的一个视频中，扎克伯格表示，他们正在全力训练 Llama 3，而且未来会坚持走开源路线。为了训练模型，他们将购买 35 万块英伟达 H100 GPU，而且自研芯片也提上了日程。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了算力，Meta 也在造芯\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "据路透社报道，Meta 正在研发一款面向数据中心的定制芯片「Artemis」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这算是 Meta 内部 人工智能 芯片项目的一个积极转折，在此之前，Meta 高管曾在 2022 年的公司财务紧缩时期决定停止对自研芯片的投入。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "但在去年，Meta 先是公布了自研的第一代训练与推理加速器 MTIA。Artemis 是 Meta 内部芯片生产线的第二代产品，有助于减少 Meta 对英伟达芯片的依赖，同时控制与运行生成式 AI 工作负载相关的成本。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究机构 SemiAnalysis 创始人 Dylan Patel 表示，按照 Meta 的运营规模，成功部署自己的芯片有可能每年节省数亿美元的能源成本和数十亿美元的芯片采购成本。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "运行 人工智能 应用所需的芯片、基础设施和能源已成为科技公司投资的巨大洼地，在一定程度上抵消了围绕这波技术热潮所带来的收益。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "一位 Meta 发言人证实了在 2024 年投产更新芯片的计划，并表示它将与该公司正在购买的数十万个现成的 GPU 协同工作。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "发言人的声明中如此表示：「我们认为，我们内部开发的加速器与市面上的 GPU 有很强的互补性，可以在 Meta 特定的工作负载上提供性能和效率的最佳组合。」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "或许大家都还记得，小扎前不久发布过一段短视频，曾表示公司计划在 2024 年底前从英伟达采购大约 35 万台旗舰 H100 处理器，加上其他供应商，Meta 将总共拥有相当于 60 万个 H100 的计算能力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "过去几年，生成式 AI 的兴起某种程度上得益于英伟达先进的 GPU。反过来，由于供不应求，H100 变得备受追捧且极其昂贵，使英伟达首次跻身万亿美元市值公司的行列。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这种态势还促使微软、Meta、OpenAI、亚马逊和谷歌等科技巨头开始开发自己的 AI 处理器。与此同时，英伟达、 AMD 和 英特尔 等芯片制造商也陷入了军备竞赛，以发布更新、更高效、更强大的芯片。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "随着生成式 AI 服务需求的持续增长，芯片显然将成为科技公司的下一个重点战场。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在开源 AI 大模型上的突飞猛进，加上算力等方面的积极动作，Meta 在科技圈的形象正在重塑。当然，在 2024 年的竞争中，Meta 也将迎来更加严酷的考验。你看好这家公司吗？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考链接：\n",
      "\n",
      "https://www.reuters.com/technology/meta-deploy-in-house-custom-chips-this-year-power-ai-drive-memo-2024-02-01/\n",
      "\n",
      "https://www.businessinsider.com/metas-profits-first-ever-dividend-2024-2 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-02-7#comment\n",
      "title= 扎克伯格分红7亿刀，Meta股价大涨14%，开源大计成了​？\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= Meta 正在走出阴霾。\n",
      "\n",
      "伴随着 Meta 的股价周四盘后上涨近 14%，升至历史新高，这家公司宣布了有史以来的首次股息派发。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "最近一次财报电话会议公布内容显示，Meta 公布的 2023 全年营收为 1349 亿美元，较 2022 年增长 16%；净利润为 391 亿美元，同比增长 69%。其中，第四季度营收为 401 亿美元，超出预期的 391.8 亿美元，同比增长 25%。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "从 3 月份开始，Meta 将按季度向 A 类和 B 类普通股派发现金股息 50 美分。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "根据彭博社汇编的数据，首席执行官扎克伯格持有约 3.5 亿股股票（Meta 13% 的股份），他将从每季度派发的股息中获得约 1.75 亿美元的税前收入，一年下来约有 7 亿美元。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "小扎表示：「随着社区和业务持续增长，我们度过了一个不错的季度。」\n",
      "\n",
      "虽然这家公司在 2022 年经历了市值的低谷，但 Meta 股票在过去 12 个月里上涨了 168%。公司市值已重回万亿美元行列，扎克伯格的净资产更是达到了 1420 亿美元。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "与其他一些大型科技公司的创始人不同，扎克伯格这位首席执行官从未将权力移交给其他人，并一直在元宇宙和 人工智能 等趋势上进行长期押注。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "过去这段日子，Meta 在 人工智能 领域的表现毫不逊色：生成式 人工智能 大 语言模型 Llama 可与 OpenAI 和谷歌的模型竞争，Meta 同时在寻求将 人工智能 工具集成到其核心产品中，据传还计划使用自研芯片为其 人工智能 系统提供动力。在不久前的一段视频中，扎克伯格暗示了 Meta 开发通用 人工智能 (AGI) 的宏伟计划。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在好于预期的业绩之外，这些消息似乎也提振了外界对于 Meta 的信心。看来，Meta 正在走出阴霾。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "凭借 Llama，Meta 正在 AI 领域开辟新战场\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "如果时间回到一年多以前，或许谁也没有想到，曾经深陷元宇宙泥沼的 Meta 能凭借一组开源模型重新扳回一局，甚至在 OpenAI、谷歌主导的 AI 闭源世界之外重新开辟了一个战场。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这个战场以 Meta 开源的 Llama 系列模型为基础，成员异常活跃。在其基础上微调的 AI 大模型很多都宣称已经超过 GPT-3.5，甚至已经逼近 GPT-4（比如最近从 Mistral 公司泄露出的「Miqu」模型，该模型接近 GPT-4 性能，是 Mistral 基于 Llama 2 为自家客户训练的早期版本）。\n",
      "\n",
      "与此同时，Meta 开源模型在业界的影响力也在逐渐显现。很多国际大公司，比如富国银行、 IBM ，如今都部署了基于 Llama 2 的开源模型。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "对于为什么选开源路线，Meta 在最近的财报电话会上给出了回应。Meta 坦言，他们确实从开源这条路上收获了很多，包括基础模型的改进和模型到产品的过渡等多个方面。而且，他们认为，成员开源的领导者有几个战略优势：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "首先，开源软件通常更安全、更可靠，同时由于社区的持续反馈、审查和开发，运行效率更高。这是一个大问题，因为安全性是 人工智能 中最重要的问题之一。效率的提高和计算成本的降低也让包括 Meta 在内的每个人都受益。\n",
      "\n",
      "其次，开源软件通常会成为行业标准。Meta 表示，当其他公司使用 Meta 的技术栈进行标准化构建时，Meta 就能很容易地将其他公司的创新整合到自己的产品中。这很微妙，但是快速学习和改进的能力是一个巨大的优势，并且成为行业标准使得这种做法成为可能。\n",
      "\n",
      "最后，开源在开发者和研究人员中非常受欢迎，这有助于 Meta 招募到最好的人才。\n",
      "\n",
      "不过，要让这些效果完全显现可能还要等一段时间，毕竟 Meta 开源系列模型的发布时间要晚于 ChatGPT。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在上个月发布的一个视频中，扎克伯格表示，他们正在全力训练 Llama 3，而且未来会坚持走开源路线。为了训练模型，他们将购买 35 万块英伟达 H100 GPU，而且自研芯片也提上了日程。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "为了算力，Meta 也在造芯\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "据路透社报道，Meta 正在研发一款面向数据中心的定制芯片「Artemis」。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这算是 Meta 内部 人工智能 芯片项目的一个积极转折，在此之前，Meta 高管曾在 2022 年的公司财务紧缩时期决定停止对自研芯片的投入。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "但在去年，Meta 先是公布了自研的第一代训练与推理加速器 MTIA。Artemis 是 Meta 内部芯片生产线的第二代产品，有助于减少 Meta 对英伟达芯片的依赖，同时控制与运行生成式 AI 工作负载相关的成本。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "研究机构 SemiAnalysis 创始人 Dylan Patel 表示，按照 Meta 的运营规模，成功部署自己的芯片有可能每年节省数亿美元的能源成本和数十亿美元的芯片采购成本。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "运行 人工智能 应用所需的芯片、基础设施和能源已成为科技公司投资的巨大洼地，在一定程度上抵消了围绕这波技术热潮所带来的收益。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "一位 Meta 发言人证实了在 2024 年投产更新芯片的计划，并表示它将与该公司正在购买的数十万个现成的 GPU 协同工作。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "发言人的声明中如此表示：「我们认为，我们内部开发的加速器与市面上的 GPU 有很强的互补性，可以在 Meta 特定的工作负载上提供性能和效率的最佳组合。」\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "或许大家都还记得，小扎前不久发布过一段短视频，曾表示公司计划在 2024 年底前从英伟达采购大约 35 万台旗舰 H100 处理器，加上其他供应商，Meta 将总共拥有相当于 60 万个 H100 的计算能力。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "过去几年，生成式 AI 的兴起某种程度上得益于英伟达先进的 GPU。反过来，由于供不应求，H100 变得备受追捧且极其昂贵，使英伟达首次跻身万亿美元市值公司的行列。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "这种态势还促使微软、Meta、OpenAI、亚马逊和谷歌等科技巨头开始开发自己的 AI 处理器。与此同时，英伟达、 AMD 和 英特尔 等芯片制造商也陷入了军备竞赛，以发布更新、更高效、更强大的芯片。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "随着生成式 AI 服务需求的持续增长，芯片显然将成为科技公司的下一个重点战场。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "在开源 AI 大模型上的突飞猛进，加上算力等方面的积极动作，Meta 在科技圈的形象正在重塑。当然，在 2024 年的竞争中，Meta 也将迎来更加严酷的考验。你看好这家公司吗？\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "参考链接：\n",
      "\n",
      "https://www.reuters.com/technology/meta-deploy-in-house-custom-chips-this-year-power-ai-drive-memo-2024-02-01/\n",
      "\n",
      "https://www.businessinsider.com/metas-profits-first-ever-dividend-2024-2 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-02-6\n",
      "title= 比肩GPT-4，商汤日日新大幅升级4.0，多模态能力领先一步\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= 大模型的未来时刻，已经来了？\n",
      "\n",
      "速度太快了。\n",
      "\n",
      "商汤一下子把多模态大模型的发展进度条，快进到了落地阶段。\n",
      "\n",
      "商汤的大模型体系「日日新 SenseNova」今天刚刚发布了 4.0 版，不论语言能力还是文生图能力都有全面升级，还自带低门槛的落地工具。\n",
      "\n",
      "新一代 SenseNova 不仅在大 语言模型 、文生图模型等方面进行了重大升级，部分垂直领域能力超越 GPT-4，还发布了全新多模态大模型，并面向 数据分析 、医疗等场景提供了全新版本，让大模型通用能力适配到了更多领域。\n",
      "\n",
      "API 申请网址：https://platform.sensenova.cn/\n",
      "\n",
      "与此同时，商汤还推出了日日新・商量大 语言模型 Function call & Assistants API 版本，除了对话能力外，还支持多种内置工具，包括图片生成 (文生图)、智能识图 (图生文)、 数据分析 （代码解释器）、在线检索。\n",
      "\n",
      "这是全球首个支持了文生图、图生文，并可支持不同模态工具调用的工具，跑在了 OpenAI 的前面。\n",
      "\n",
      "这一系列发布，从技术进步到落地「两翼齐飞」，可谓把通用大模型技术卷上了天。看来在技术竞争中，国内科技公司逐渐有了反超的趋势。\n",
      "\n",
      "最高支持 128k 长窗口\n",
      "\n",
      "商量 SenseChat 测试全方位比肩 GPT-4\n",
      "\n",
      "自 ChatGPT 出现以来，大模型成为了 AI 赛道的主力军。商汤的大模型体系正在「大模型 + 大装置」的战略布局下快速迭代。\n",
      "\n",
      "去年 4 月，商汤公布了「日日新 SenseNova」大模型体系，一上来就在 自然语言处理 、文生图创作、数字人生成、3D 场景和物体生成，自动化数据标注、自定义模型训练等多个领域全面发力。\n",
      "\n",
      "与此同时，商汤还直接提供图片生成、自然语言对话、 视觉推理 和标注服务的 API 接口。\n",
      "\n",
      "此后，该大模型体系持续推陈出新，在基础能力、API 服务、模型应用等多个方面不断进步，给用户和开发者们带来了越来越好用的技术。\n",
      "\n",
      "如今，近 10 个月过去了，商汤新一代「日日新 SenseNova 4.0」在 2024 年的新春之际与大家见面了， 不仅对已有多个大模型进行全方位升级，还有一些「新面孔」。\n",
      "\n",
      "升级之后，日日新在长文本理解、综合推理（包括数字推理）、代码生成、多模态交互等整体表现上「更上一层楼」，不仅全面超越了 GPT-3.5，并且大部分接近甚至超越了 GPT-4 系列模型。\n",
      "\n",
      "用下面一组核心数据说话，SenseNova 4.0 的：\n",
      "\n",
      "推理能力：达到 GPT-4 Turbo 的 99%；\n",
      "\n",
      "代码能力：在 HumanEval 代码生成 基准 测试上 准确率 达到 75.6，超越 GPT-4（74.4）；\n",
      "\n",
      "多模态能力：在 MMBench 多模态大 语言模型 综合评估 基准 上的整体性能超越了 GPT-4V（84.4 vs 74.4）；\n",
      "\n",
      "数据分析 能力：正确率（85.71%）超越 GPT-4（84.62%）；\n",
      "\n",
      "在部分垂直领域能力超越 GPT-4 Turbo。\n",
      "\n",
      "而日日新全维度、无死角的能力飙升，首要归功于商量大 语言模型 SenseChat 的重大升级。\n",
      "\n",
      "此次发布的商量大 语言模型 -通用版本（SenseChat V4） 在整体能力比肩 GPT-4，并相较于 GPT-3.5 实现显著超越。如下两图为 SenseChat V4 与GPT-3.5、GPT-4 在整体、考试、语言、知识、推理、数理、编程等数据集上的性能比较数据。\n",
      "\n",
      "至于为何能有如此明显的性能提升，SenseChat 4.0 在以下多个方面获得了加强。\n",
      "\n",
      "首先是更全面的知识覆盖，新增了包括业务通用数据、数学能力数据、K12 考试数据、文学期刊数据等在内约 600B tokens 的中英文预训练语料，这样理解多领域内容更加得心应手。同时，模型质量也通过数据清洗和增强得到进一步提高。\n",
      "\n",
      "其次推理能力变得更加可靠。从初始 1.0 版本以来，前后四次超强预训练的积累让模型在阅读理解、综合推理、代码能力等多项任务上实现了 5%-10% 的定向性提升。\n",
      "\n",
      "最后也是此次 4.0 版本升级的重点 —— 更强的长文本理解分析能力，更新了 3 种不同上下文窗口的全新模型，即 SenseChat-4K、SenseChat-32k 和 SenseChat-128k，不仅使得模型理解上下文的能力迎来史诗级加强，还提升了模型的适应能力，拓宽了应用范围，为用户提供根据需求自由选择模型的机会。\n",
      "\n",
      "在与 GPT-3.5、GPT-4 的多任务较量中，我们直观地看到了 SenseChat 不同上下文窗口版本的真正实力。\n",
      "\n",
      "其中，SenseChat-4K 虽然支持最少的 4k tokens（约 4000 中文字）的输入和输出，但仍然在写作总结、知识问答、闲聊娱乐、专业技能、安全测试等主客观题和安全性能上超越了 GPT-4。另外，新增的引文功能还可以返回在 线搜索 的知识来源。\n",
      "\n",
      "SenseChat-32k 则能够处理 32k tokens（约 3 万中文字）的长文本总结，总能力平均得分达到了同等上下文窗口 GPT-4-32k 能力的 90% 以上水平，中文理解能力则超越了后者。\n",
      "\n",
      "铺开来讲，SenseChat-32k 在平均考试能力和理解能力、以及 HellaSwag、C3、LAMBADA、CHID 等推理和理解类测试集中超越 GPT-4-32k；在 LongBench 长文本理解测试 基准 以及 tpo、multidocqa、scientificqa、PassageRetrieval-zh 等长文本测试集上均超越了 GPT-4–32k。\n",
      "\n",
      "对于支持最长 128k tokens（约 12 万以上中文字）长文本的 SenseChat-128k，它的中文理解能力也超过了 GPT-4 的水平。\n",
      "\n",
      "下表 1 和 2 分别为 SenseChat 三个版本模型与 GPT 系列在长文本理解和推理等测试集上的平均得分比较。\n",
      "\n",
      "表 1：Normalbench v1-4 万题对比结果。\n",
      "\n",
      "表 2：长文本 Leval 和 Longbench 测试集对比结果。\n",
      "\n",
      "看起来，SenseChat V4 不仅在主客观题方面达到了 GPT-4 的水平，更在长文本理解和推理能力上实现了全面超越。\n",
      "\n",
      "作为商汤「日日新 SenseNova」大模型体系的通用基础模型，SenseChat V4 的大幅度升级使得人们在使用模型处理多样化语言任务时更高效、更准确，让国产大模型拥有不输于 GPT-4 的使用体验。\n",
      "\n",
      "对于更多人来说，未来在商量 SenseChat 大 语言模型 的基础上开展学术研究、技术创新、商业应用也有了更多机会。\n",
      "\n",
      "填补行业空缺，打造专用大模型\n",
      "\n",
      "首家开放支持多模态的 Assistants API\n",
      "\n",
      "基础模型之外，商汤也希望能通过高效融合垂直领域知识，帮助人们构建各类专业大模型，降低大模型的下游应用成本和门槛。\n",
      "\n",
      "多模态是 人工智能 大模型重要的技术演进方向，新一代「日日新 SenseNova」推出了拥有 300 亿 参数 的日日新·商量多模态大模型（SenseChat-Vision V4），其图文 感知 能力处于全球领先水平，在权威评测 基准 测试集 MME Benchmark 上综合得分排名首位。\n",
      "\n",
      "目前，该模型可以支持智能驾驶、智能车舱、电力行业等多个实际场景的应用。\n",
      "\n",
      "与常规的 OCR 能力不同，它不仅可以理解图中的文字和物体，并且可以根据 逻辑 进行推理，实现了一定程度的认知能力。\n",
      "\n",
      "在办公与 数据分析 领域，商汤推出了日日新·商量语言大模型- 数据分析 版本（SenseChat-DataAnalysisCode V4），它可以通过自然语言输入，结合商汤大模型的 意图识别 、 逻辑 理解与代码解释器的能力，自动将数据转化为有意义的分析和可视化结果。\n",
      "\n",
      "目前，该工具已经支持 xls、xlsx、csv、txt、json 等格式的文件和表格处理。就实际效果而言，办公小浣熊在 1000 + 测试集精度上略胜于 GPT-4。\n",
      "\n",
      "体验入口：https://raccoon.sensetime.com/office\n",
      "\n",
      "在医疗健康领域，大 语言模型 的医疗版本也有全新升级，日日新·商量语言大模型-医疗版本“大医”（SenseChat-Medical V4）在本次更新后可以有效实现专业医学问答及复杂医学任务推理，并支持更多模态医学文件的智能解读和交互问答。据介绍，“大医”在两项行业权威评测 —— 2023 年职业药剂师考试大模型评测和中文医疗大 语言模型 开放评测平台 MedBench 中，均实现综合评分排名第二，性能接近 GPT-4。\n",
      "\n",
      "商汤自研的日日新-秒画文生图大模型（SenseMirage V4）较此前版本， 参数 量提升至百亿量级，通过 Mixture of text experts、Spatial-aware CFG 等算法优化，语义理解能力与图像质感细节表现显著增强，可达成电影级海报生成水平。同时结合 Adversarial Distillation 算法，秒画 SenseMirage-Turbo V4 也对外发布，相较于基础版本，可达到 10 倍推理加速效果。\n",
      "\n",
      "秒画一键生成电影海报级的精美图像\n",
      "\n",
      "再进一步，商汤还把调用不同模态的能力，做到了一个端口上，这就是全球首个支持调用不同模态的 Assistants API。\n",
      "\n",
      "去年 11 月，OpenAI 在其首届开发者大会上推出专门构建的 AI 工具 ——Assistants API，通过代码解释器、检索和函数调用等新功能帮助开发者构建高质量的 AI 应用。不过，至今这个工具也没有支持构建视觉相关的多模态应用。\n",
      "\n",
      "商汤提出的 Assistants API 填补了这一空缺。作为一个基于商量大 语言模型 构建的、具有状态的多轮对话接口，它不仅首次支持了文生图、图生文的不同模态工具调用，还内置 数据分析 、搜索引擎工具。\n",
      "\n",
      "如果把大模型看作是大脑，Assistants API 相当于给 AI 增加了眼睛和手，能够自主理解人类下达的任务，并做出正确 规划 ，使用合适的资源和工具。Assistants API 提供了一个桥梁，将先进的大模型与各类应用服务工具连接起来，支持图文结合的多模态交互和代码执行结果的直观呈现，可以帮助人们快速解决复杂的问题。\n",
      "\n",
      "目前，商汤的大模型体系已经在全面落地。在全行业层面上，自发布以来已经拥有了超过 3000 家企业用户，累积调用量已达近 9000 万次，服务的行业包含互联网娱乐、游戏、文娱、教育、医疗健康、金融、编程等方面。\n",
      "\n",
      "结语\n",
      "\n",
      "还记得去年的「百模大战」吗？现在，科技领域的大模型军备竞赛形势已经有了改变，竞争不再是单纯的模型技术，而变成了拼体系 —— 除了模型技术的升级改进，各家厂商正在整合与调优基础底座，开放的趋势也在催生出逐渐繁荣的生态。\n",
      "\n",
      "如今，战火已经燃烧到了多模态技术的落地上。能够睁开眼睛看世界的大模型，为我们带来了更多的想象力。\n",
      "\n",
      "而为了在千行百业中用好它们，真正实现「重做所有产品」，一套完整的体系势必能让我们事半功倍。\n",
      "\n",
      "在这一方面，商汤已经做到了更好。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-02-6#comment\n",
      "title= 比肩GPT-4，商汤日日新大幅升级4.0，多模态能力领先一步\n",
      "author= []\n",
      "publish_date= 2024-02-02 00:00:00\n",
      "text= 大模型的未来时刻，已经来了？\n",
      "\n",
      "速度太快了。\n",
      "\n",
      "商汤一下子把多模态大模型的发展进度条，快进到了落地阶段。\n",
      "\n",
      "商汤的大模型体系「日日新 SenseNova」今天刚刚发布了 4.0 版，不论语言能力还是文生图能力都有全面升级，还自带低门槛的落地工具。\n",
      "\n",
      "新一代 SenseNova 不仅在大 语言模型 、文生图模型等方面进行了重大升级，部分垂直领域能力超越 GPT-4，还发布了全新多模态大模型，并面向 数据分析 、医疗等场景提供了全新版本，让大模型通用能力适配到了更多领域。\n",
      "\n",
      "API 申请网址：https://platform.sensenova.cn/\n",
      "\n",
      "与此同时，商汤还推出了日日新・商量大 语言模型 Function call & Assistants API 版本，除了对话能力外，还支持多种内置工具，包括图片生成 (文生图)、智能识图 (图生文)、 数据分析 （代码解释器）、在线检索。\n",
      "\n",
      "这是全球首个支持了文生图、图生文，并可支持不同模态工具调用的工具，跑在了 OpenAI 的前面。\n",
      "\n",
      "这一系列发布，从技术进步到落地「两翼齐飞」，可谓把通用大模型技术卷上了天。看来在技术竞争中，国内科技公司逐渐有了反超的趋势。\n",
      "\n",
      "最高支持 128k 长窗口\n",
      "\n",
      "商量 SenseChat 测试全方位比肩 GPT-4\n",
      "\n",
      "自 ChatGPT 出现以来，大模型成为了 AI 赛道的主力军。商汤的大模型体系正在「大模型 + 大装置」的战略布局下快速迭代。\n",
      "\n",
      "去年 4 月，商汤公布了「日日新 SenseNova」大模型体系，一上来就在 自然语言处理 、文生图创作、数字人生成、3D 场景和物体生成，自动化数据标注、自定义模型训练等多个领域全面发力。\n",
      "\n",
      "与此同时，商汤还直接提供图片生成、自然语言对话、 视觉推理 和标注服务的 API 接口。\n",
      "\n",
      "此后，该大模型体系持续推陈出新，在基础能力、API 服务、模型应用等多个方面不断进步，给用户和开发者们带来了越来越好用的技术。\n",
      "\n",
      "如今，近 10 个月过去了，商汤新一代「日日新 SenseNova 4.0」在 2024 年的新春之际与大家见面了， 不仅对已有多个大模型进行全方位升级，还有一些「新面孔」。\n",
      "\n",
      "升级之后，日日新在长文本理解、综合推理（包括数字推理）、代码生成、多模态交互等整体表现上「更上一层楼」，不仅全面超越了 GPT-3.5，并且大部分接近甚至超越了 GPT-4 系列模型。\n",
      "\n",
      "用下面一组核心数据说话，SenseNova 4.0 的：\n",
      "\n",
      "推理能力：达到 GPT-4 Turbo 的 99%；\n",
      "\n",
      "代码能力：在 HumanEval 代码生成 基准 测试上 准确率 达到 75.6，超越 GPT-4（74.4）；\n",
      "\n",
      "多模态能力：在 MMBench 多模态大 语言模型 综合评估 基准 上的整体性能超越了 GPT-4V（84.4 vs 74.4）；\n",
      "\n",
      "数据分析 能力：正确率（85.71%）超越 GPT-4（84.62%）；\n",
      "\n",
      "在部分垂直领域能力超越 GPT-4 Turbo。\n",
      "\n",
      "而日日新全维度、无死角的能力飙升，首要归功于商量大 语言模型 SenseChat 的重大升级。\n",
      "\n",
      "此次发布的商量大 语言模型 -通用版本（SenseChat V4） 在整体能力比肩 GPT-4，并相较于 GPT-3.5 实现显著超越。如下两图为 SenseChat V4 与GPT-3.5、GPT-4 在整体、考试、语言、知识、推理、数理、编程等数据集上的性能比较数据。\n",
      "\n",
      "至于为何能有如此明显的性能提升，SenseChat 4.0 在以下多个方面获得了加强。\n",
      "\n",
      "首先是更全面的知识覆盖，新增了包括业务通用数据、数学能力数据、K12 考试数据、文学期刊数据等在内约 600B tokens 的中英文预训练语料，这样理解多领域内容更加得心应手。同时，模型质量也通过数据清洗和增强得到进一步提高。\n",
      "\n",
      "其次推理能力变得更加可靠。从初始 1.0 版本以来，前后四次超强预训练的积累让模型在阅读理解、综合推理、代码能力等多项任务上实现了 5%-10% 的定向性提升。\n",
      "\n",
      "最后也是此次 4.0 版本升级的重点 —— 更强的长文本理解分析能力，更新了 3 种不同上下文窗口的全新模型，即 SenseChat-4K、SenseChat-32k 和 SenseChat-128k，不仅使得模型理解上下文的能力迎来史诗级加强，还提升了模型的适应能力，拓宽了应用范围，为用户提供根据需求自由选择模型的机会。\n",
      "\n",
      "在与 GPT-3.5、GPT-4 的多任务较量中，我们直观地看到了 SenseChat 不同上下文窗口版本的真正实力。\n",
      "\n",
      "其中，SenseChat-4K 虽然支持最少的 4k tokens（约 4000 中文字）的输入和输出，但仍然在写作总结、知识问答、闲聊娱乐、专业技能、安全测试等主客观题和安全性能上超越了 GPT-4。另外，新增的引文功能还可以返回在 线搜索 的知识来源。\n",
      "\n",
      "SenseChat-32k 则能够处理 32k tokens（约 3 万中文字）的长文本总结，总能力平均得分达到了同等上下文窗口 GPT-4-32k 能力的 90% 以上水平，中文理解能力则超越了后者。\n",
      "\n",
      "铺开来讲，SenseChat-32k 在平均考试能力和理解能力、以及 HellaSwag、C3、LAMBADA、CHID 等推理和理解类测试集中超越 GPT-4-32k；在 LongBench 长文本理解测试 基准 以及 tpo、multidocqa、scientificqa、PassageRetrieval-zh 等长文本测试集上均超越了 GPT-4–32k。\n",
      "\n",
      "对于支持最长 128k tokens（约 12 万以上中文字）长文本的 SenseChat-128k，它的中文理解能力也超过了 GPT-4 的水平。\n",
      "\n",
      "下表 1 和 2 分别为 SenseChat 三个版本模型与 GPT 系列在长文本理解和推理等测试集上的平均得分比较。\n",
      "\n",
      "表 1：Normalbench v1-4 万题对比结果。\n",
      "\n",
      "表 2：长文本 Leval 和 Longbench 测试集对比结果。\n",
      "\n",
      "看起来，SenseChat V4 不仅在主客观题方面达到了 GPT-4 的水平，更在长文本理解和推理能力上实现了全面超越。\n",
      "\n",
      "作为商汤「日日新 SenseNova」大模型体系的通用基础模型，SenseChat V4 的大幅度升级使得人们在使用模型处理多样化语言任务时更高效、更准确，让国产大模型拥有不输于 GPT-4 的使用体验。\n",
      "\n",
      "对于更多人来说，未来在商量 SenseChat 大 语言模型 的基础上开展学术研究、技术创新、商业应用也有了更多机会。\n",
      "\n",
      "填补行业空缺，打造专用大模型\n",
      "\n",
      "首家开放支持多模态的 Assistants API\n",
      "\n",
      "基础模型之外，商汤也希望能通过高效融合垂直领域知识，帮助人们构建各类专业大模型，降低大模型的下游应用成本和门槛。\n",
      "\n",
      "多模态是 人工智能 大模型重要的技术演进方向，新一代「日日新 SenseNova」推出了拥有 300 亿 参数 的日日新·商量多模态大模型（SenseChat-Vision V4），其图文 感知 能力处于全球领先水平，在权威评测 基准 测试集 MME Benchmark 上综合得分排名首位。\n",
      "\n",
      "目前，该模型可以支持智能驾驶、智能车舱、电力行业等多个实际场景的应用。\n",
      "\n",
      "与常规的 OCR 能力不同，它不仅可以理解图中的文字和物体，并且可以根据 逻辑 进行推理，实现了一定程度的认知能力。\n",
      "\n",
      "在办公与 数据分析 领域，商汤推出了日日新·商量语言大模型- 数据分析 版本（SenseChat-DataAnalysisCode V4），它可以通过自然语言输入，结合商汤大模型的 意图识别 、 逻辑 理解与代码解释器的能力，自动将数据转化为有意义的分析和可视化结果。\n",
      "\n",
      "目前，该工具已经支持 xls、xlsx、csv、txt、json 等格式的文件和表格处理。就实际效果而言，办公小浣熊在 1000 + 测试集精度上略胜于 GPT-4。\n",
      "\n",
      "体验入口：https://raccoon.sensetime.com/office\n",
      "\n",
      "在医疗健康领域，大 语言模型 的医疗版本也有全新升级，日日新·商量语言大模型-医疗版本“大医”（SenseChat-Medical V4）在本次更新后可以有效实现专业医学问答及复杂医学任务推理，并支持更多模态医学文件的智能解读和交互问答。据介绍，“大医”在两项行业权威评测 —— 2023 年职业药剂师考试大模型评测和中文医疗大 语言模型 开放评测平台 MedBench 中，均实现综合评分排名第二，性能接近 GPT-4。\n",
      "\n",
      "商汤自研的日日新-秒画文生图大模型（SenseMirage V4）较此前版本， 参数 量提升至百亿量级，通过 Mixture of text experts、Spatial-aware CFG 等算法优化，语义理解能力与图像质感细节表现显著增强，可达成电影级海报生成水平。同时结合 Adversarial Distillation 算法，秒画 SenseMirage-Turbo V4 也对外发布，相较于基础版本，可达到 10 倍推理加速效果。\n",
      "\n",
      "秒画一键生成电影海报级的精美图像\n",
      "\n",
      "再进一步，商汤还把调用不同模态的能力，做到了一个端口上，这就是全球首个支持调用不同模态的 Assistants API。\n",
      "\n",
      "去年 11 月，OpenAI 在其首届开发者大会上推出专门构建的 AI 工具 ——Assistants API，通过代码解释器、检索和函数调用等新功能帮助开发者构建高质量的 AI 应用。不过，至今这个工具也没有支持构建视觉相关的多模态应用。\n",
      "\n",
      "商汤提出的 Assistants API 填补了这一空缺。作为一个基于商量大 语言模型 构建的、具有状态的多轮对话接口，它不仅首次支持了文生图、图生文的不同模态工具调用，还内置 数据分析 、搜索引擎工具。\n",
      "\n",
      "如果把大模型看作是大脑，Assistants API 相当于给 AI 增加了眼睛和手，能够自主理解人类下达的任务，并做出正确 规划 ，使用合适的资源和工具。Assistants API 提供了一个桥梁，将先进的大模型与各类应用服务工具连接起来，支持图文结合的多模态交互和代码执行结果的直观呈现，可以帮助人们快速解决复杂的问题。\n",
      "\n",
      "目前，商汤的大模型体系已经在全面落地。在全行业层面上，自发布以来已经拥有了超过 3000 家企业用户，累积调用量已达近 9000 万次，服务的行业包含互联网娱乐、游戏、文娱、教育、医疗健康、金融、编程等方面。\n",
      "\n",
      "结语\n",
      "\n",
      "还记得去年的「百模大战」吗？现在，科技领域的大模型军备竞赛形势已经有了改变，竞争不再是单纯的模型技术，而变成了拼体系 —— 除了模型技术的升级改进，各家厂商正在整合与调优基础底座，开放的趋势也在催生出逐渐繁荣的生态。\n",
      "\n",
      "如今，战火已经燃烧到了多模态技术的落地上。能够睁开眼睛看世界的大模型，为我们带来了更多的想象力。\n",
      "\n",
      "而为了在千行百业中用好它们，真正实现「重做所有产品」，一套完整的体系势必能让我们事半功倍。\n",
      "\n",
      "在这一方面，商汤已经做到了更好。 \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-01-6\n",
      "title= 赶超Gemini Pro，提升推理、OCR能力的LLaVA-1.6太强了\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 去年 4 月，威斯康星大学麦迪逊分校、微软研究院和哥伦比亚大学研究者共同发布了 LLaVA（Large Language and Vision Assistant）。尽管 LLaVA 是用一个小的多模态指令数据集训练的，却在一些样本上展示了与 GPT-4 非常相似的推理结果。10 月，LLaVA-1.5 重磅发布，通过对原始 LLaVA 的简单修改，在 11 个 基准 上刷新了 SOTA。\n",
      "\n",
      "现在，研究团队宣布推出 LLaVA-1.6，主要改进了模型在推理、OCR 和世界知识方面的性能。LLaVA-1.6 甚至在多项 基准 测试中超越了 Gemini Pro。\n",
      "\n",
      "demo 地址：https://llava.hliu.cc/\n",
      "\n",
      "项目地址：https://github.com/haotian-liu/LLaVA\n",
      "\n",
      "与 LLaVA-1.5 相比，LLaVA-1.6 有如下几个改进：\n",
      "\n",
      "将输入图像分辨率提升 4 倍，支持三种宽高比，最高可达 672x672、336x1344、1344x336 分辨率。这使得 LLaVA-1.6 能够掌握更多的视觉细节。\n",
      "\n",
      "通过改进的视觉指令调整数据混合，LLaVA-1.6 获得了更好的 视觉推理 和 OCR 能力。\n",
      "\n",
      "更好的视觉对话，更多场景，覆盖不同应用。LLaVA-1.6 掌握了更多世界知识，具备更好的 逻辑 推理能力。\n",
      "\n",
      "使用 SGLang 进行高效部署和推理。\n",
      "\n",
      "图源：https://twitter.com/imhaotian/status/1752621754273472927\n",
      "\n",
      "LLaVA-1.6 保持了 LLaVA-1.5 的极简设计和数据效率，它复用了 LLaVA-1.5 的预训练连接器，并且仍然使用不到 1M 的视觉指令调优样本。最大的 34B 模型使用 32 个 A100 在大约 1 天内完成了训练。LLaVA-1.6 使用 130 万个数据样本，计算 / 训练数据成本约为其他方法的 100-1000 分之一。\n",
      "\n",
      "与 CogVLM 或 Yi-VL 等开源 LMM 相比，LLaVA-1.6 实现了 SOTA 性能。与商用产品相比，LLaVA-1.6 在选定的 基准 测试中可以媲美 Gemini Pro，并且优于 Qwen-VL-Plus。\n",
      "\n",
      "值得一提的是，LLaVA-1.6 展现出强大的零样本（zero-shot）中文能力，它在多模态 基准 MMBench-CN 上取得了 SOTA 性能。\n",
      "\n",
      "方法改进\n",
      "\n",
      "动态高分辨率\n",
      "\n",
      "研究团队以高分辨率设计 LLaVA-1.6 模型，旨在保持其数据效率。当提供高分辨率图像和保留细节的表征时，模型 感知 图像中复杂细节的能力会显著提高。它减少了面对低分辨率图像时的模型幻觉，即猜测想象的视觉内容。\n",
      "\n",
      "数据混合\n",
      "\n",
      "高质量的用户指令数据。该研究对高质量视觉指令遵循数据的定义取决于两个主要标准：首先，任务指令的多样性，确保充分代表现实场景中可能遇到的广泛用户意图，特别是在模型部署阶段。其次，响应的优先级至关重要，旨在征求有利的用户反馈。\n",
      "\n",
      "因此，该研究考虑了两个数据源：\n",
      "\n",
      "现有的 GPT-V 数据 （LAION-GPT-V 和 ShareGPT-4V）；\n",
      "\n",
      "为了进一步促进更多场景下更好的视觉对话，研究团队收集了一个涵盖不同应用的小型 15K 视觉指令调优数据集，仔细过滤了可能存在隐私问题或可能有害的样本，并使用 GPT-4V 生成响应。\n",
      "\n",
      "多模态文档 / 图表数据。(1) 从训练数据中删除 TextCap，因为研究团队意识到 TextCap 使用与 TextVQA 相同的训练图像集。这使得研究团队能够在评估 TextVQA 时更好地了解模型的零样本 OCR 能力。为了保持并进一步提高模型的 OCR 能力，该研究用 DocVQA 和 SynDog-EN 替换了 TextCap。(2) 借助 Qwen-VL-7B-Chat，该研究进一步添加了 ChartQA、DVQA 和 AI2D，以更好地理解图和图表。\n",
      "\n",
      "研究团队还表示除了 Vicuna-1.5（7B 和 13B），还考虑采用更多 LLM 方案，包括 Mistral-7B 和 Nous-Hermes-2-Yi-34B，以使 LLaVA 能够支持更广泛的用户和更多的场景。\n",
      "\n",
      "参考链接：https://llava-vl.github.io/blog/2024-01-30-llava-1-6/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-01-6#comment\n",
      "title= 赶超Gemini Pro，提升推理、OCR能力的LLaVA-1.6太强了\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 去年 4 月，威斯康星大学麦迪逊分校、微软研究院和哥伦比亚大学研究者共同发布了 LLaVA（Large Language and Vision Assistant）。尽管 LLaVA 是用一个小的多模态指令数据集训练的，却在一些样本上展示了与 GPT-4 非常相似的推理结果。10 月，LLaVA-1.5 重磅发布，通过对原始 LLaVA 的简单修改，在 11 个 基准 上刷新了 SOTA。\n",
      "\n",
      "现在，研究团队宣布推出 LLaVA-1.6，主要改进了模型在推理、OCR 和世界知识方面的性能。LLaVA-1.6 甚至在多项 基准 测试中超越了 Gemini Pro。\n",
      "\n",
      "demo 地址：https://llava.hliu.cc/\n",
      "\n",
      "项目地址：https://github.com/haotian-liu/LLaVA\n",
      "\n",
      "与 LLaVA-1.5 相比，LLaVA-1.6 有如下几个改进：\n",
      "\n",
      "将输入图像分辨率提升 4 倍，支持三种宽高比，最高可达 672x672、336x1344、1344x336 分辨率。这使得 LLaVA-1.6 能够掌握更多的视觉细节。\n",
      "\n",
      "通过改进的视觉指令调整数据混合，LLaVA-1.6 获得了更好的 视觉推理 和 OCR 能力。\n",
      "\n",
      "更好的视觉对话，更多场景，覆盖不同应用。LLaVA-1.6 掌握了更多世界知识，具备更好的 逻辑 推理能力。\n",
      "\n",
      "使用 SGLang 进行高效部署和推理。\n",
      "\n",
      "图源：https://twitter.com/imhaotian/status/1752621754273472927\n",
      "\n",
      "LLaVA-1.6 保持了 LLaVA-1.5 的极简设计和数据效率，它复用了 LLaVA-1.5 的预训练连接器，并且仍然使用不到 1M 的视觉指令调优样本。最大的 34B 模型使用 32 个 A100 在大约 1 天内完成了训练。LLaVA-1.6 使用 130 万个数据样本，计算 / 训练数据成本约为其他方法的 100-1000 分之一。\n",
      "\n",
      "与 CogVLM 或 Yi-VL 等开源 LMM 相比，LLaVA-1.6 实现了 SOTA 性能。与商用产品相比，LLaVA-1.6 在选定的 基准 测试中可以媲美 Gemini Pro，并且优于 Qwen-VL-Plus。\n",
      "\n",
      "值得一提的是，LLaVA-1.6 展现出强大的零样本（zero-shot）中文能力，它在多模态 基准 MMBench-CN 上取得了 SOTA 性能。\n",
      "\n",
      "方法改进\n",
      "\n",
      "动态高分辨率\n",
      "\n",
      "研究团队以高分辨率设计 LLaVA-1.6 模型，旨在保持其数据效率。当提供高分辨率图像和保留细节的表征时，模型 感知 图像中复杂细节的能力会显著提高。它减少了面对低分辨率图像时的模型幻觉，即猜测想象的视觉内容。\n",
      "\n",
      "数据混合\n",
      "\n",
      "高质量的用户指令数据。该研究对高质量视觉指令遵循数据的定义取决于两个主要标准：首先，任务指令的多样性，确保充分代表现实场景中可能遇到的广泛用户意图，特别是在模型部署阶段。其次，响应的优先级至关重要，旨在征求有利的用户反馈。\n",
      "\n",
      "因此，该研究考虑了两个数据源：\n",
      "\n",
      "现有的 GPT-V 数据 （LAION-GPT-V 和 ShareGPT-4V）；\n",
      "\n",
      "为了进一步促进更多场景下更好的视觉对话，研究团队收集了一个涵盖不同应用的小型 15K 视觉指令调优数据集，仔细过滤了可能存在隐私问题或可能有害的样本，并使用 GPT-4V 生成响应。\n",
      "\n",
      "多模态文档 / 图表数据。(1) 从训练数据中删除 TextCap，因为研究团队意识到 TextCap 使用与 TextVQA 相同的训练图像集。这使得研究团队能够在评估 TextVQA 时更好地了解模型的零样本 OCR 能力。为了保持并进一步提高模型的 OCR 能力，该研究用 DocVQA 和 SynDog-EN 替换了 TextCap。(2) 借助 Qwen-VL-7B-Chat，该研究进一步添加了 ChartQA、DVQA 和 AI2D，以更好地理解图和图表。\n",
      "\n",
      "研究团队还表示除了 Vicuna-1.5（7B 和 13B），还考虑采用更多 LLM 方案，包括 Mistral-7B 和 Nous-Hermes-2-Yi-34B，以使 LLaVA 能够支持更广泛的用户和更多的场景。\n",
      "\n",
      "参考链接：https://llava-vl.github.io/blog/2024-01-30-llava-1-6/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-01-11\n",
      "title= 蚂蚁集团NextEvo全面开源AI Infra技术，可实现大模型训练“自动驾驶”\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 近日， 蚂蚁集团 AI创新研发部门NextEvo全面开源AI Infra技术，可帮助大模型千卡训练有效时间占比超过95%，能实现训练时“自动驾驶”，这推动了AI研发效率。\n",
      "\n",
      "图： 蚂蚁集团 的自动化分布式 深度学习 系统DLRover现已全面开源\n",
      "\n",
      "该技术框架名为DLRover，目标在于大规模分布式训练的智能化。目前很多企业的训练作业都是跑在混合部署的集群中，运行环境复杂多变，不管多么“崎岖的地形”，DLRover都可以“轻松行驶”。\n",
      "\n",
      "2023 年大模型技术的发展，带来了工程实践的爆发，如何管理数据，提高训练和推理效率，最大化利用现有算力，成了关键一环。\n",
      "\n",
      "完成一个千亿 参数 级别的大模型，如GPT-3，用一张卡训练一次要耗时32年，那么训练时的算力利用尤为重要。方法之一是把能用的算力用得更好，比如进一步压榨已购买GPU的性能；二是把以前利用不了的算力用起来，比如CPU、内存等，这就需要通过异构计算平台来解决。\n",
      "\n",
      "最新集成进DLRover的是Flash Checkpoint（FCP）方案。模型训练时，一般要打Checkpoint（检查点），以便中断时能恢复到最近状态，目前常规的做法，存在着耗时长、高频打点易降低训练可用时间、低频打点恢复时丢失过多等缺点。新方案FCP应用在千卡千亿 参数 模型训练后，Checkpoint 导致的训练浪费时间降低约5倍，其中持久化时间降低约70倍，有效训练时间从90%提升至95%。\n",
      "\n",
      "同时集成进去的，还有三项新的 优化器 （Optimizer）技术。 优化器 作为 机器学习 的核心组件，用于更新 神经网络 参数 以最小化 损失函数 。其中，蚂蚁的AGD（Auto-switchable optimizer with Gradient Difference of adjacent steps） 优化器 ，在大模型预训练任务中，相比传统的AdamW技术加速 1.5 倍，AGD已在蚂蚁内部多个场景使用并取得显著效果，相关论文已被 NeurIPS '23收录。\n",
      "\n",
      "图：在大模型预训练任务中，AGD相比AdamW可以加速1.5倍\n",
      "\n",
      "作为自动化分布式 深度学习 系统，DLRover的“自动驾驶”功能模块还包括：Atorch，一种PyTorch分布式训练扩展库，在千亿 参数 模型千卡级别规模下，训练的算力利用率可达60%，帮助开发者进一步压榨硬件算力。\n",
      "\n",
      "DLRover以 “ML for System” 的理念来提升分布式训练的智能度，旨在通过一个系统，让开发者完全摆脱资源配置的束缚，专注于模型训练本身。在没有任何资源配置输入的情况下，DLRover 仍然可以为每个训练作业提供最佳资源配置。\n",
      "\n",
      "据了解， 蚂蚁集团 在 人工智能 领域持续进行技术投入，最近， 蚂蚁集团 在内部成立了AI创新研发部门NextEvo，承担了蚂蚁AI的所有核心技术研发，包含百灵大模型的所有研发工作，涉及AI算法、AI工程、NLP、AIGC等核心技术，并在布局多模态大模型、数字人等领域的技术研发和产品创新。\n",
      "\n",
      "同时， 蚂蚁集团 还加速开源节奏，填补了国内相关技术空白，推动 人工智能 行业快速发展。\n",
      "\n",
      "DLRover开源地址：https://github.com/intelligent-machine-learning/dlrover \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-02-01-11#comment\n",
      "title= 蚂蚁集团NextEvo全面开源AI Infra技术，可实现大模型训练“自动驾驶”\n",
      "author= []\n",
      "publish_date= 2024-02-01 00:00:00\n",
      "text= 近日， 蚂蚁集团 AI创新研发部门NextEvo全面开源AI Infra技术，可帮助大模型千卡训练有效时间占比超过95%，能实现训练时“自动驾驶”，这推动了AI研发效率。\n",
      "\n",
      "图： 蚂蚁集团 的自动化分布式 深度学习 系统DLRover现已全面开源\n",
      "\n",
      "该技术框架名为DLRover，目标在于大规模分布式训练的智能化。目前很多企业的训练作业都是跑在混合部署的集群中，运行环境复杂多变，不管多么“崎岖的地形”，DLRover都可以“轻松行驶”。\n",
      "\n",
      "2023 年大模型技术的发展，带来了工程实践的爆发，如何管理数据，提高训练和推理效率，最大化利用现有算力，成了关键一环。\n",
      "\n",
      "完成一个千亿 参数 级别的大模型，如GPT-3，用一张卡训练一次要耗时32年，那么训练时的算力利用尤为重要。方法之一是把能用的算力用得更好，比如进一步压榨已购买GPU的性能；二是把以前利用不了的算力用起来，比如CPU、内存等，这就需要通过异构计算平台来解决。\n",
      "\n",
      "最新集成进DLRover的是Flash Checkpoint（FCP）方案。模型训练时，一般要打Checkpoint（检查点），以便中断时能恢复到最近状态，目前常规的做法，存在着耗时长、高频打点易降低训练可用时间、低频打点恢复时丢失过多等缺点。新方案FCP应用在千卡千亿 参数 模型训练后，Checkpoint 导致的训练浪费时间降低约5倍，其中持久化时间降低约70倍，有效训练时间从90%提升至95%。\n",
      "\n",
      "同时集成进去的，还有三项新的 优化器 （Optimizer）技术。 优化器 作为 机器学习 的核心组件，用于更新 神经网络 参数 以最小化 损失函数 。其中，蚂蚁的AGD（Auto-switchable optimizer with Gradient Difference of adjacent steps） 优化器 ，在大模型预训练任务中，相比传统的AdamW技术加速 1.5 倍，AGD已在蚂蚁内部多个场景使用并取得显著效果，相关论文已被 NeurIPS '23收录。\n",
      "\n",
      "图：在大模型预训练任务中，AGD相比AdamW可以加速1.5倍\n",
      "\n",
      "作为自动化分布式 深度学习 系统，DLRover的“自动驾驶”功能模块还包括：Atorch，一种PyTorch分布式训练扩展库，在千亿 参数 模型千卡级别规模下，训练的算力利用率可达60%，帮助开发者进一步压榨硬件算力。\n",
      "\n",
      "DLRover以 “ML for System” 的理念来提升分布式训练的智能度，旨在通过一个系统，让开发者完全摆脱资源配置的束缚，专注于模型训练本身。在没有任何资源配置输入的情况下，DLRover 仍然可以为每个训练作业提供最佳资源配置。\n",
      "\n",
      "据了解， 蚂蚁集团 在 人工智能 领域持续进行技术投入，最近， 蚂蚁集团 在内部成立了AI创新研发部门NextEvo，承担了蚂蚁AI的所有核心技术研发，包含百灵大模型的所有研发工作，涉及AI算法、AI工程、NLP、AIGC等核心技术，并在布局多模态大模型、数字人等领域的技术研发和产品创新。\n",
      "\n",
      "同时， 蚂蚁集团 还加速开源节奏，填补了国内相关技术空白，推动 人工智能 行业快速发展。\n",
      "\n",
      "DLRover开源地址：https://github.com/intelligent-machine-learning/dlrover \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-01-31-6\n",
      "title= Mistral-Medium意外泄露？冲上榜单的这个神秘模型让AI社区讨论爆了\n",
      "author= []\n",
      "publish_date= 2024-01-31 00:00:00\n",
      "text= 「我现在 100% 确信 Miqu 与 Perplexity Labs 上的 Mistral-Medium 是同一个模型。」\n",
      "\n",
      "近日，一则关于「Mistral-Medium 模型泄露」的消息引起了大家的关注。\n",
      "\n",
      "泄露传闻与一个名为「Miqu」的新模型有关，在评估 语言模型 情商的 基准 EQ-Bench（EQ-Bench 与 MMLU 的相关性约为 0.97、与 Arena Elo 的相关性约为 0.94）上， Miqu 直接吊打了除 GPT-4 之外的所有大模型，而且它的得分与 Mistral-Medium 非常接近：\n",
      "\n",
      "图源：https://x.com/N8Programs/status/1752441060133892503?s=20\n",
      "\n",
      "开源地址：https://huggingface.co/miqudev/miqu-1-70b\n",
      "\n",
      "这么强大的模型，项目的发布者却是一位神秘人士：\n",
      "\n",
      "有人问「who made you」， Miqu 直接自报家门：「I was created by the Mistral Al team.」\n",
      "\n",
      "有人分别向两个模型发送了同一道测试问题，收到的回答都是用俄语表达的。测试者加深了怀疑：「它似乎知道标准谜题，但如果是恶作剧者，根本不可能将其调整为同样用俄语回答。」\n",
      "\n",
      "在翻译过程中，表述也近乎相同。\n",
      "\n",
      "Miqu 到底来自何方？它真的是 Mistral-Medium 吗？\n",
      "\n",
      "在持续两天的热议中，多位开发者针对两个模型做了对比，对比的结果指向以下几种可能性：\n",
      "\n",
      "1、Miqu 就是 Mistral-Medium；\n",
      "\n",
      "2、Miqu 确实是来自 MistralAI 的一个模型，但是是一些早期的 MoE 实验版本或其他版本；\n",
      "\n",
      "3、Miqu 是 Llama2 的微调版本。\n",
      "\n",
      "在前面，我们介绍了支持第一种可能性的开发者给出的理由。随着事件的发酵，更多开发者投入了解密一般的行动中，对两个模型进行了更深入的测试。一位 reddit 网友熬夜肝出的测试表明，Miqu 更像是 MistralAI 模型的早期版本。\n",
      "\n",
      "这位开发者将模型应用于四个专业的德语在线数据保护培训 / 考试中。测试数据、问题及所有指令都是用德语进行的，而字符卡是英语的。这可以测试翻译能力和跨语言理解能力。\n",
      "\n",
      "具体测试方法如下：\n",
      "\n",
      "在提供信息之前，用德语指示模型：「我将给你一些信息，请注意这些信息，但回答时只需用『OK』来确认你已理解，不要多说其他的。」这是为了测试模型对指令的理解和执行能力。\n",
      "\n",
      "在提供话题的所有信息后，向模型提出考题。这是一个选择题（A/B/C），其中第一个问题和最后一个问题相同，但选项顺序和字母（X/Y/Z）被更改。每次测试包含 4-6 个考题，总共 18 个多项选择题。\n",
      "\n",
      "根据模型给出的正确答案数量来进行排名，首先考虑的是在提供了课程信息后的答案，其次是在没有提前提供信息的情况下盲目回答的答案，以应对平局情况。所有测试都是独立的单元，每次测试之间会清除上下文，各个会话之间不保留任何记忆或状态。\n",
      "\n",
      "详细测试报告如下：\n",
      "\n",
      "miqudev/miqu-1-70b GGUF Q5_K_M，32K 上下文， Mistral 格式：只对 4+4+4+5=17/18 道选择题给出了正确答案。没有先前的信息，只回答问题，给出正确答案：4+3+1+5=13/18。没有按照说明用 \"OK\" 确认数据输入。\n",
      "\n",
      "在测试过程中，开发者发现 Miqu 与 Mixtral 有许多相似之处：出色的德语拼写和语法双语；在回复中添加翻译；在回复中添加注释和评论。\n",
      "\n",
      "不过，在这位开发者的测试中，Miqu 与 Mixtral-8x7B-Instruct-v0.1（4-bit）相比表现要差一些，仍优于 Mistral Small 和 Medium。但它并不比 Mixtral 8x7B Instruct 好得多。这位开发者猜测，Miqu 可能是泄露的 MistralAI 模型，是一个较旧的，可能是概念验证模型。\n",
      "\n",
      "这是我们目前看到的支持第二种说法的最详细的测试。\n",
      "\n",
      "不过，也有开发者认为，Miqu 和 MistralAI 没有关系，反而更像 Llama 70B，因为其架构与 Llama 70B「完全相同」，「不是专家混合模型」。\n",
      "\n",
      "同样地，也有人测试之后发现，Miqu 的确更像 Llama：\n",
      "\n",
      "但从得分差距来看，Miqu 和 Llama 70B 显然又不是同一个模型。\n",
      "\n",
      "所以，有人总结，要么 Miqu 是 Llama 微调版本，要么是 Mistral-Medium 的早期版本：\n",
      "\n",
      "前者为真的话，Miqu 可能是在 Mistral-Medium 数据集上微调的 Llama 70B：\n",
      "\n",
      "假如后者为真，Miqu 只是 Mistral API 的蒸馏，这或许将是「美国伪造登月」级别的闹剧：\n",
      "\n",
      "最后一个问题，泄露者是谁？\n",
      "\n",
      "根据很多 X 平台用户提供的线索，这次疑似泄露的模型最初是发在一个名叫 4chan 的网站上的。这个网站是一个完全匿名的实时消息论坛，用户不需要注册就能就可以发表图文言论。\n",
      "\n",
      "当然，这些结论均属主观想法。对于所有的 AI 研究者来说，这波剧情需要一个「真相」来终结。\n",
      "\n",
      "参考链接：https://www.reddit.com/r/LocalLLaMA/comments/1af4fbg/llm_comparisontest_miqu170b/ \n",
      "\n",
      "summary= \n",
      "新闻页面url: https://jiqizhixin.com/articles/2024-01-31-6#comment\n",
      "title= Mistral-Medium意外泄露？冲上榜单的这个神秘模型让AI社区讨论爆了\n",
      "author= []\n",
      "publish_date= 2024-01-31 00:00:00\n",
      "text= 「我现在 100% 确信 Miqu 与 Perplexity Labs 上的 Mistral-Medium 是同一个模型。」\n",
      "\n",
      "近日，一则关于「Mistral-Medium 模型泄露」的消息引起了大家的关注。\n",
      "\n",
      "泄露传闻与一个名为「Miqu」的新模型有关，在评估 语言模型 情商的 基准 EQ-Bench（EQ-Bench 与 MMLU 的相关性约为 0.97、与 Arena Elo 的相关性约为 0.94）上， Miqu 直接吊打了除 GPT-4 之外的所有大模型，而且它的得分与 Mistral-Medium 非常接近：\n",
      "\n",
      "图源：https://x.com/N8Programs/status/1752441060133892503?s=20\n",
      "\n",
      "开源地址：https://huggingface.co/miqudev/miqu-1-70b\n",
      "\n",
      "这么强大的模型，项目的发布者却是一位神秘人士：\n",
      "\n",
      "有人问「who made you」， Miqu 直接自报家门：「I was created by the Mistral Al team.」\n",
      "\n",
      "有人分别向两个模型发送了同一道测试问题，收到的回答都是用俄语表达的。测试者加深了怀疑：「它似乎知道标准谜题，但如果是恶作剧者，根本不可能将其调整为同样用俄语回答。」\n",
      "\n",
      "在翻译过程中，表述也近乎相同。\n",
      "\n",
      "Miqu 到底来自何方？它真的是 Mistral-Medium 吗？\n",
      "\n",
      "在持续两天的热议中，多位开发者针对两个模型做了对比，对比的结果指向以下几种可能性：\n",
      "\n",
      "1、Miqu 就是 Mistral-Medium；\n",
      "\n",
      "2、Miqu 确实是来自 MistralAI 的一个模型，但是是一些早期的 MoE 实验版本或其他版本；\n",
      "\n",
      "3、Miqu 是 Llama2 的微调版本。\n",
      "\n",
      "在前面，我们介绍了支持第一种可能性的开发者给出的理由。随着事件的发酵，更多开发者投入了解密一般的行动中，对两个模型进行了更深入的测试。一位 reddit 网友熬夜肝出的测试表明，Miqu 更像是 MistralAI 模型的早期版本。\n",
      "\n",
      "这位开发者将模型应用于四个专业的德语在线数据保护培训 / 考试中。测试数据、问题及所有指令都是用德语进行的，而字符卡是英语的。这可以测试翻译能力和跨语言理解能力。\n",
      "\n",
      "具体测试方法如下：\n",
      "\n",
      "在提供信息之前，用德语指示模型：「我将给你一些信息，请注意这些信息，但回答时只需用『OK』来确认你已理解，不要多说其他的。」这是为了测试模型对指令的理解和执行能力。\n",
      "\n",
      "在提供话题的所有信息后，向模型提出考题。这是一个选择题（A/B/C），其中第一个问题和最后一个问题相同，但选项顺序和字母（X/Y/Z）被更改。每次测试包含 4-6 个考题，总共 18 个多项选择题。\n",
      "\n",
      "根据模型给出的正确答案数量来进行排名，首先考虑的是在提供了课程信息后的答案，其次是在没有提前提供信息的情况下盲目回答的答案，以应对平局情况。所有测试都是独立的单元，每次测试之间会清除上下文，各个会话之间不保留任何记忆或状态。\n",
      "\n",
      "详细测试报告如下：\n",
      "\n",
      "miqudev/miqu-1-70b GGUF Q5_K_M，32K 上下文， Mistral 格式：只对 4+4+4+5=17/18 道选择题给出了正确答案。没有先前的信息，只回答问题，给出正确答案：4+3+1+5=13/18。没有按照说明用 \"OK\" 确认数据输入。\n",
      "\n",
      "在测试过程中，开发者发现 Miqu 与 Mixtral 有许多相似之处：出色的德语拼写和语法双语；在回复中添加翻译；在回复中添加注释和评论。\n",
      "\n",
      "不过，在这位开发者的测试中，Miqu 与 Mixtral-8x7B-Instruct-v0.1（4-bit）相比表现要差一些，仍优于 Mistral Small 和 Medium。但它并不比 Mixtral 8x7B Instruct 好得多。这位开发者猜测，Miqu 可能是泄露的 MistralAI 模型，是一个较旧的，可能是概念验证模型。\n",
      "\n",
      "这是我们目前看到的支持第二种说法的最详细的测试。\n",
      "\n",
      "不过，也有开发者认为，Miqu 和 MistralAI 没有关系，反而更像 Llama 70B，因为其架构与 Llama 70B「完全相同」，「不是专家混合模型」。\n",
      "\n",
      "同样地，也有人测试之后发现，Miqu 的确更像 Llama：\n",
      "\n",
      "但从得分差距来看，Miqu 和 Llama 70B 显然又不是同一个模型。\n",
      "\n",
      "所以，有人总结，要么 Miqu 是 Llama 微调版本，要么是 Mistral-Medium 的早期版本：\n",
      "\n",
      "前者为真的话，Miqu 可能是在 Mistral-Medium 数据集上微调的 Llama 70B：\n",
      "\n",
      "假如后者为真，Miqu 只是 Mistral API 的蒸馏，这或许将是「美国伪造登月」级别的闹剧：\n",
      "\n",
      "最后一个问题，泄露者是谁？\n",
      "\n",
      "根据很多 X 平台用户提供的线索，这次疑似泄露的模型最初是发在一个名叫 4chan 的网站上的。这个网站是一个完全匿名的实时消息论坛，用户不需要注册就能就可以发表图文言论。\n",
      "\n",
      "当然，这些结论均属主观想法。对于所有的 AI 研究者来说，这波剧情需要一个「真相」来终结。\n",
      "\n",
      "参考链接：https://www.reddit.com/r/LocalLLaMA/comments/1af4fbg/llm_comparisontest_miqu170b/ \n",
      "\n",
      "summary= \n",
      "一共获取87篇文章\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "\n",
    "def spider_newspaper_url(url):\n",
    "    \"\"\"\n",
    "    默认情况下，newspaper缓存所有以前提取的文章，并删除它已经提取的任何文章。\n",
    "    使用memoize_articles参数选择退出此功能。\n",
    "    \"\"\"\n",
    "    web_paper = newspaper.build(url, language=\"zh\", memoize_articles=False)\n",
    "    print(\"提取新闻页面的url！！！\")\n",
    "    for article in web_paper.articles:\n",
    "    # 获取新闻网页的url\n",
    "        print(\"新闻页面url:\", article.url)\n",
    "# 调用spider_newspaper_information函数获取新闻网页数据\n",
    "        spider_newspaper_information(article.url)\n",
    "\n",
    "    print(\"一共获取%s篇文章\" % web_paper.size())  # 文章的数目\n",
    "\n",
    "# 获取文章的信息\n",
    "def spider_newspaper_information(url):\n",
    "    # 建立链接和下载文章\n",
    "    article = Article(url, language='zh')  # Chinese\n",
    "    article.download()\n",
    "    article.parse()\n",
    "\n",
    "# 获取文章的信息\n",
    "    print(\"title=\", article.title)  # 获取文章标题\n",
    "    print(\"author=\", article.authors)  # 获取文章作者\n",
    "    print(\"publish_date=\", article.publish_date)  # 获取文章日期\n",
    "    # print(\"top_iamge=\", article.top_image)  # 获取文章顶部图片地址\n",
    "    # print(\"movies=\", article.movies)  # 获取文章视频链接\n",
    "    print(\"text=\", article.text, \"\\n\")  # 获取文章正文\n",
    "    print(\"summary=\", article.summary)  # 获取文章摘要\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    web_lists = [\"https://www.jiqizhixin.com/columns/Thushujupai\"]\n",
    "    for web_list in web_lists:\n",
    "        spider_newspaper_url(web_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e2748b-2168-43fb-8385-4790df4de1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
